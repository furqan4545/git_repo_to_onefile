{"nyu_transform.py": "\nimport torch\nimport numpy as np\nfrom PIL import Image, ImageOps\nimport collections\ntry:\n    import accimage\nexcept ImportError:\n    accimage = None\nimport random\nimport scipy.ndimage as ndimage\n\nimport pdb\n\n\ndef _is_pil_image(img):\n    if accimage is not None:\n        return isinstance(img, (Image.Image, accimage.Image))\n    else:\n        return isinstance(img, Image.Image)\n\n\ndef _is_numpy_image(img):\n    return isinstance(img, np.ndarray) and (img.ndim in {2, 3})\n\n\nclass RandomRotate(object):\n    \"\"\"Random rotation of the image from -angle to angle (in degrees)\n    This is useful for dataAugmentation, especially for geometric problems such as FlowEstimation\n    angle: max angle of the rotation\n    interpolation order: Default: 2 (bilinear)\n    reshape: Default: false. If set to true, image size will be set to keep every pixel in the image.\n    diff_angle: Default: 0. Must stay less than 10 degrees, or linear approximation of flowmap will be off.\n    \"\"\"\n\n    def __init__(self, angle, diff_angle=0, order=2, reshape=False):\n        self.angle = angle\n        self.reshape = reshape\n        self.order = order\n\n    def __call__(self, sample):\n        #import ipdb;ipdb.set_trace()\n        image, depth, Semantic = sample['image'], sample['depth'], sample['Semantic']\n\n        applied_angle = random.uniform(-self.angle, self.angle)\n        angle1 = applied_angle\n        angle1_rad = angle1 * np.pi / 180\n\n        image = ndimage.interpolation.rotate(\n            image, angle1, reshape=self.reshape, order=self.order)\n        depth = ndimage.interpolation.rotate(\n            depth, angle1, reshape=self.reshape, order=self.order)\n            \n        \n        object_num1 = len(set(np.array(Semantic).reshape(-1)))\n        \n        # Semantic = ndimage.interpolation.rotate(\n            # Semantic, angle1, reshape=self.reshape, order=1)\n        Semantic = Semantic.rotate(angle1, resample=Image.NEAREST, expand=True)\n\n        object_num2 = len(set(np.array(Semantic).reshape(-1)))\n        \n        if not object_num1 == object_num2:\n            print('class_num: '+str(object_num1)+' '+str(object_num2))\n            \n        image = Image.fromarray(image)\n        depth = Image.fromarray(depth)\n        # Semantic = Image.fromarray(Semantic)\n\n        return {'image': image, 'depth': depth, 'Semantic': Semantic}\n\nclass RandomHorizontalFlip(object):\n\n    def __call__(self, sample):\n        #import ipdb;ipdb.set_trace()\n        image, depth, Semantic = sample['image'], sample['depth'], sample['Semantic']\n\n        if not _is_pil_image(image):\n            raise TypeError(\n                'img should be PIL Image. Got {}'.format(type(img)))\n        if not _is_pil_image(depth):\n            raise TypeError(\n                'img should be PIL Image. Got {}'.format(type(depth)))\n\n        if random.random() < 0.5:\n            image = image.transpose(Image.FLIP_LEFT_RIGHT)\n            depth = depth.transpose(Image.FLIP_LEFT_RIGHT)\n            Semantic = Semantic.transpose(Image.FLIP_LEFT_RIGHT)\n\n        return {'image': image, 'depth': depth, 'Semantic': Semantic}\n\n\nclass Scale(object):\n    \"\"\" Rescales the inputs and target arrays to the given 'size'.\n    'size' will be the size of the smaller edge.\n    For example, if height > width, then image will be\n    rescaled to (size * height / width, size)\n    size: size of the smaller edge\n    interpolation order: Default: 2 (bilinear)\n    \"\"\"\n\n    def __init__(self, size, rate):\n        self.size = size\n        self.rate = rate\n\n    def __call__(self, sample):\n        image, depth, Semantic = sample['image'], sample['depth'], sample['Semantic']\n        #import ipdb;ipdb.set_trace()\n        if not self.size == -1:\n            image = self.changeScale(image, self.size)\n        # depth = self.changeScale(depth, self.size,Image.NEAREST)\n        # Semantic = self.changeScale(Semantic, self.size,Image.NEAREST)\n        W, H = image.size\n        w, h = (int(W*self.rate), int(H*self.rate))\n        \n        depth = depth.resize((w, h), Image.NEAREST)\n        Semantic = Semantic.resize((w, h), Image.NEAREST)\n \n        return {'image': image, 'depth': depth, 'Semantic': Semantic}\n\n    def changeScale(self, img, size, interpolation=Image.BILINEAR):\n        #import ipdb;ipdb.set_trace()\n        if not _is_pil_image(img):\n            raise TypeError(\n                'img should be PIL Image. Got {}'.format(type(img)))\n        if not (isinstance(size, int) or (isinstance(size, collections.Iterable) and len(size) == 2)):\n            raise TypeError('Got inappropriate size arg: {}'.format(size))\n\n        if isinstance(size, int):\n            w, h = img.size\n            if (w <= h and w == size) or (h <= w and h == size):\n                return img\n            if w < h:\n                ow = size\n                oh = int(size * h / w)\n                return img.resize((ow, oh), interpolation)\n            else:\n                oh = size\n                ow = int(size * w / h)\n                return img.resize((ow, oh), interpolation)\n        else:\n            return img.resize(size[::-1], interpolation)\n\n\nclass CenterCrop(object):\n    def __init__(self, size_image, size_depth, size_Semantic):\n        self.size_image = size_image\n        self.size_depth = size_depth\n        self.size_Semantic = size_Semantic\n\n    def __call__(self, sample):\n        image, depth, Semantic = sample['image'], sample['depth'], sample['Semantic']\n    \n        image = self.centerCrop(image, self.size_image)\n        depth = self.centerCrop(depth, self.size_image)\n        Semantic = self.centerCrop(Semantic, self.size_image)\n\n        ow, oh = self.size_depth\n        depth = depth.resize((ow, oh), Image.NEAREST)\n        \n        ow, oh = self.size_Semantic\n        Semantic = Semantic.resize((ow, oh), Image.NEAREST)\n\n        return {'image': image, 'depth': depth, 'Semantic': Semantic}\n\n    def centerCrop(self, image, size):\n        #import ipdb;ipdb.set_trace()\n        w1, h1 = image.size\n\n        tw, th = size\n\n        if w1 == tw and h1 == th:\n            return image\n\n        x1 = int(round((w1 - tw) / 2.))\n        y1 = int(round((h1 - th) / 2.))\n\n        image = image.crop((x1, y1, tw + x1, th + y1))\n\n        return image\n\n\nclass ToTensor(object):\n    \"\"\"Convert a ``PIL.Image`` or ``numpy.ndarray`` to tensor.\n    Converts a PIL.Image or numpy.ndarray (H x W x C) in the range\n    [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n    \"\"\"\n    def __init__(self, dataroot):\n        if 'CAD120' in dataroot:\n            self.rate = 10\n        else:\n            self.rate = 1000\n        print('depth_rate: '+str(self.rate))\n    def __call__(self, sample):\n        image, depth, Semantic = sample['image'], sample['depth'], sample['Semantic']\n        \"\"\"\n        Args:\n            pic (PIL.Image or numpy.ndarray): Image to be converted to tensor.\n        Returns:\n            Tensor: Converted image.\n        \"\"\"\n        # ground truth depth of training samples is stored in 8-bit while test samples are saved in 16 bit\n        image = self.to_tensor(image) #\u53d8\u62100~1\u4e4b\u95f4\n        #import ipdb;ipdb.set_trace()\n        depth = self.to_tensor(depth).float()/self.rate #\u53d8\u62100~10\u4e4b\u95f4\n        Semantic = self.to_tensor(Semantic).float() #\u50a8\u5b58\u7c7b\u522bid\n        return {'image': image, 'depth': depth, 'Semantic': Semantic}\n\n    def to_tensor(self, pic):\n        #import ipdb;ipdb.set_trace()\n        if not(_is_pil_image(pic) or _is_numpy_image(pic)):\n            raise TypeError(\n                'pic should be PIL Image or ndarray. Got {}'.format(type(pic)))\n\n        if accimage is not None and isinstance(pic, accimage.Image):\n            nppic = np.zeros(\n                [pic.channels, pic.height, pic.width], dtype=np.float32)\n            pic.copyto(nppic)\n            return torch.from_numpy(nppic)\n        \n        # handle PIL Image\n        if pic.mode == 'I' :\n            img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n        elif pic.mode == 'I;16':\n            img = torch.from_numpy(np.array(pic, np.int16, copy=False))\n        elif pic.mode == 'L' :\n            img = torch.from_numpy(np.array(pic, np.int8, copy=False))\n        else:\n            img = torch.ByteTensor(\n                torch.ByteStorage.from_buffer(pic.tobytes()))\n        # PIL image mode: 1, L, P, I, F, RGB, YCbCr, RGBA, CMYK\n        if pic.mode == 'YCbCr':\n            nchannel = 3\n        elif pic.mode == 'I;16':\n            nchannel = 1\n        else:\n            nchannel = len(pic.mode)\n            \n        img = img.view(pic.size[1], pic.size[0], nchannel)\n        \n        # put it from HWC to CHW format\n        # yikes, this transpose takes 80% of the loading time/CPU\n        img = img.transpose(0, 1).transpose(0, 2).contiguous()\n        if isinstance(img, torch.ByteTensor):\n            return img.float().div(255)\n        else:\n            return img\n\n\nclass Lighting(object):\n\n    def __init__(self, alphastd, eigval, eigvec):\n        self.alphastd = alphastd\n        self.eigval = eigval\n        self.eigvec = eigvec\n\n    def __call__(self, sample):\n        image, depth, Semantic = sample['image'], sample['depth'], sample['Semantic']\n        if self.alphastd == 0:\n            return image\n\n        alpha = image.new().resize_(3).normal_(0, self.alphastd)\n        rgb = self.eigvec.type_as(image).clone()\\\n            .mul(alpha.view(1, 3).expand(3, 3))\\\n            .mul(self.eigval.view(1, 3).expand(3, 3))\\\n            .sum(1).squeeze()\n\n        image = image.add(rgb.view(3, 1, 1).expand_as(image))\n\n        return {'image': image, 'depth': depth, 'Semantic': Semantic}\n\n\nclass Grayscale(object):\n\n    def __call__(self, img):\n        gs = img.clone()\n        gs[0].mul_(0.299).add_(0.587, gs[1]).add_(0.114, gs[2])\n        gs[1].copy_(gs[0])\n        gs[2].copy_(gs[0])\n        return gs\n\n\nclass Saturation(object):\n\n    def __init__(self, var):\n        self.var = var\n\n    def __call__(self, img):\n        gs = Grayscale()(img)\n        alpha = random.uniform(-self.var, self.var)\n        return img.lerp(gs, alpha)\n\n\nclass Brightness(object):\n\n    def __init__(self, var):\n        self.var = var\n\n    def __call__(self, img):\n        gs = img.new().resize_as_(img).zero_()\n        alpha = random.uniform(-self.var, self.var)\n\n        return img.lerp(gs, alpha)\n\n\nclass Contrast(object):\n\n    def __init__(self, var):\n        self.var = var\n\n    def __call__(self, img):\n        gs = Grayscale()(img)\n        gs.fill_(gs.mean())\n        alpha = random.uniform(-self.var, self.var)\n        return img.lerp(gs, alpha)\n\n\nclass RandomOrder(object):\n    \"\"\" Composes several transforms together in random order.\n    \"\"\"\n\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, sample):\n        image, depth, Semantic = sample['image'], sample['depth'], sample['Semantic']\n\n        if self.transforms is None:\n            return {'image': image, 'depth': depth, 'Semantic': Semantic}\n        order = torch.randperm(len(self.transforms))\n        for i in order:\n            image = self.transforms[i](image)\n\n        return {'image': image, 'depth': depth, 'Semantic': Semantic}\n\n\nclass ColorJitter(RandomOrder):\n\n    def __init__(self, brightness=0.4, contrast=0.4, saturation=0.4):\n        self.transforms = []\n        if brightness != 0:\n            self.transforms.append(Brightness(brightness))\n        if contrast != 0:\n            self.transforms.append(Contrast(contrast))\n        if saturation != 0:\n            self.transforms.append(Saturation(saturation))\n\n\nclass Normalize(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, sample):\n        \"\"\"\n        Args:\n            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n        Returns:\n            Tensor: Normalized image.\n        \"\"\"\n        image, depth, Semantic = sample['image'], sample['depth'], sample['Semantic']\n\n        image = self.normalize(image, self.mean, self.std)\n\n        return {'image': image, 'depth': depth, 'Semantic': Semantic}\n\n    def normalize(self, tensor, mean, std):\n        \"\"\"Normalize a tensor image with mean and standard deviation.\n        See ``Normalize`` for more details.\n        Args:\n            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n            mean (sequence): Sequence of means for R, G, B channels respecitvely.\n            std (sequence): Sequence of standard deviations for R, G, B channels\n                respecitvely.\n        Returns:\n            Tensor: Normalized image.\n        \"\"\"\n\n        # TODO: make efficient\n        for t, m, s in zip(tensor, mean, std):\n            t.sub_(m).div_(s)\n        return tensor\n\n", "environment.yml": "name: liteseg\nchannels:\n  - anaconda\n  - pytorch\n  - conda-forge\n  - defaults\ndependencies:\n  - ca-certificates=2018.03.07\n  - certifi=2018.10.15\n  - mkl_fft=1.0.6\n  - mkl_random=1.0.1\n  - numpy=1.15.4\n  - numpy-base=1.15.4\n  - openssl=1.0.2p\n  - scikit-learn=0.20.0\n  - scipy=1.1.0\n  - cycler=0.10.0\n  - kiwisolver=1.0.1\n  - matplotlib=3.0.2\n  - matplotlib-base=3.0.2\n  - pyqt=5.6.0\n  - sip=4.18.1\n  - alabaster=0.7.12\n  - asn1crypto=0.24.0\n  - astroid=2.0.4\n  - babel=2.6.0\n  - backcall=0.1.0\n  - blas=1.0\n  - bleach=3.0.2\n  - bzip2=1.0.6\n  - cairo=1.14.12\n  - cffi=1.11.5\n  - chardet=3.0.4\n  - cloudpickle=0.6.1\n  - cryptography=2.3.1\n  - dbus=1.13.2\n  - decorator=4.3.0\n  - docutils=0.14\n  - entrypoints=0.2.3\n  - expat=2.2.6\n  - ffmpeg=4.0\n  - fontconfig=2.13.0\n  - freetype=2.9.1\n  - glib=2.56.2\n  - gmp=6.1.2\n  - graphite2=1.3.12\n  - gst-plugins-base=1.14.0\n  - gstreamer=1.14.0\n  - harfbuzz=1.8.8\n  - hdf5=1.10.2\n  - icu=58.2\n  - idna=2.7\n  - imagesize=1.1.0\n  - intel-openmp=2019.0\n  - ipykernel=5.1.0\n  - ipython=7.1.1\n  - ipython_genutils=0.2.0\n  - isort=4.3.4\n  - jasper=2.0.14\n  - jedi=0.13.1\n  - jeepney=0.4\n  - jinja2=2.10\n  - jpeg=9b\n  - jsonschema=2.6.0\n  - jupyter_client=5.2.3\n  - jupyter_core=4.4.0\n  - keyring=16.0.2\n  - lazy-object-proxy=1.3.1\n  - libedit=3.1.20170329\n  - libffi=3.2.1\n  - libopencv=3.4.2\n  - libopus=1.3\n  - libpng=1.6.35\n  - libsodium=1.0.16\n  - libtiff=4.0.9\n  - libuuid=1.0.3\n  - libvpx=1.7.0\n  - libxcb=1.13\n  - libxml2=2.9.8\n  - markupsafe=1.1.0\n  - mccabe=0.6.1\n  - mistune=0.8.4\n  - mkl=2019.0\n  - nbconvert=5.3.1\n  - nbformat=4.4.0\n  - ncurses=6.1\n  - ninja=1.8.2\n  - numpydoc=0.8.0\n  - olefile=0.46\n  - opencv=3.4.2\n  - packaging=18.0\n  - pandoc=2.2.3.2\n  - pandocfilters=1.4.2\n  - parso=0.3.1\n  - pcre=8.42\n  - pexpect=4.6.0\n  - pickleshare=0.7.5\n  - pillow=5.3.0\n  - pip=18.1\n  - pixman=0.34.0\n  - prompt_toolkit=2.0.7\n  - psutil=5.4.8\n  - ptyprocess=0.6.0\n  - py-opencv=3.4.2\n  - pycodestyle=2.4.0\n  - pycparser=2.19\n  - pyflakes=2.0.0\n  - pygments=2.2.0\n  - pylint=2.1.1\n  - pyopenssl=18.0.0\n  - pyparsing=2.3.0\n  - pysocks=1.6.8\n  - python=3.7.0\n  - python-dateutil=2.7.5\n  - pytz=2018.7\n  - pyzmq=17.1.2\n  - qt=5.6.3\n  - qtawesome=0.5.3\n  - qtconsole=4.4.2\n  - qtpy=1.5.2\n  - readline=7.0\n  - requests=2.20.1\n  - rope=0.11.0\n  - setuptools=40.6.2\n  - six=1.11.0\n  - snowballstemmer=1.2.1\n  - sphinx=1.8.2\n  - sphinxcontrib=1.0\n  - sphinxcontrib-websupport=1.1.0\n  - spyder=3.3.1\n  - spyder-kernels=0.2.6\n  - sqlite=3.25.3\n  - testpath=0.4.2\n  - tk=8.6.8\n  - tornado=5.1.1\n  - traitlets=4.3.2\n  - urllib3=1.23\n  - wcwidth=0.1.7\n  - webencodings=0.5.1\n  - wheel=0.32.2\n  - wrapt=1.10.11\n  - xz=5.2.4\n  - zeromq=4.2.5\n  - zlib=1.2.11\n  - pytorch=0.4.1\n  - torchvision=0.2.1\n  - pip:\n    - addict==2.2.0\n    - imutils==0.5.1\n    - munch==2.3.2\n    - pandas==0.23.4\n    - protobuf==3.6.1\n    - pyyaml==3.13\n    - tensorboardx==1.4\n    - thop==0.0.22\n    - torch==0.4.1.post2\n    - torchfile==0.1.0\n    - torchsummary==1.5.1\n    - tqdm==4.31.1\n    - visdom==0.1.8.5\n    - websocket-client==0.54.0\n    - libgfortran-ng==7.3.0\n    - libglu==9.0.0\n    - libstdcxx-ng==8.2.0\n    - freeglut==3.0.0\n    - secretstorage==3.1.0\n    - libgcc-ng==8.2.0\n\n\nprefix: /home/emaraic/anaconda3/envs/pytorch\n\n", "util.py": "\nimport torch\nimport math\nimport numpy as np\n\ndef lg10(x):\n    return torch.div(torch.log(x), math.log(10))\n\ndef maxOfTwo(x, y):\n    z = x.clone()\n    maskYLarger = torch.lt(x, y)  #\u5224\u65adx\u662f\u5426\u5c0f\u4e8ey\uff0c\u5c0f\u4e8e\u4e3a1\u4e0d\u5c0f\u4e8e\u4e3a0\n    z[maskYLarger.detach()] = y[maskYLarger.detach()]\n    return z  #z\u4e2d\u4e0ey\u4e0d\u540c\u7684\u503c\u4e3a\u8d1f\u503c\u53ca\u5927\u4e8ey\u7684\n\ndef nValid(x):\n    return torch.sum(torch.eq(x, x).float())\n\ndef nNanElement(x):\n    return torch.sum(torch.ne(x, x).float())\n\ndef getNanMask(x):\n    return torch.ne(x, x)\n\ndef setNanToZero(input, target):\n    #import ipdb;ipdb.set_trace()\n    nanMask = getNanMask(target)  #\u5224\u65adtarget\u4e2d\u662f\u5426\u6709nan\uff0cnan\u6240\u5728\u7684\u4f4d\u7f6e\u4e3a1\uff0c\u975enan\u7684\u4f4d\u7f6e\u4e3a0\uff0c\u5927\u5c0f\u4e0etarget\u76f8\u540c\n    nValidElement = nValid(target)  #target\u4e2d\u975enan\u5373\u6709\u6548\u7684\u50cf\u7d20\u6570\u76ee\n    \n    zeroMask = torch.eq(target, 0)\n    nValidElement = nValidElement - torch.sum(zeroMask)\n    \n\n    _input = input.clone()\n    _target = target.clone()\n\n    nanMask = (nanMask + zeroMask)>0\n    #\u9884\u6d4b\u548cgt\u4e2d\u65e0\u6548\u70b9\u7684\u6df1\u5ea6\u5747\u7f6e\u4e3a0\n    _input[nanMask] = 0\n    _target[nanMask] = 0\n\n    return _input, _target, nanMask, nValidElement\n\n\ndef evaluateError(output, target):\n    #import ipdb;ipdb.set_trace()\n    errors = {'MSE': 0, 'RMSE': 0, 'ABS_REL': 0, 'LG10': 0,\n              'MAE': 0,  'DELTA1': 0, 'DELTA2': 0, 'DELTA3': 0}\n    \n    #\u7ecf\u8fc7\u6709\u6548\u70b9\u7b5b\u9009\u540e\u7684\u9884\u6d4b\u3001gt\u3001\u65e0\u6548\u70b9\u4e3a1\u6709\u6548\u70b9\u4e3a0\u7684\u6807\u8bc6\u77e9\u9635\u3001\u6709\u6548\u70b9\u50cf\u7d20\u6570\u76ee\n    _output, _target, nanMask, nValidElement = setNanToZero(output, target)\n\n    if (nValidElement.data.cpu().numpy() > 0):\n        diffMatrix = torch.abs(_output - _target)\n\n        errors['MSE'] = torch.sum(torch.pow(diffMatrix, 2)) / nValidElement  #\u6574\u4e2abatch\u7684\u5e73\u5747\uff0c\u6709\u6548\u70b9\u9884\u6d4b\u548cgt\u7684\u5dee\u7684\u5e73\u65b9\u6c42\u548c\u53d6\u5e73\u5747\n\n        errors['MAE'] = torch.sum(diffMatrix) / nValidElement  #\u8bef\u5dee\u7edd\u5bf9\u503c\u6c42\u548c\u53d6\u5e73\u5747\n\n        realMatrix = torch.div(diffMatrix, _target)  #\u76f8\u5bf9\u8bef\u5dee\uff0c\u8bef\u5dee\u9664\u4ee5gt\uff0c\u76f8\u540c\u8bef\u5dee\u503c\u5728\u6df1\u5ea6\u4e0d\u540c\u7684\u70b9\u8d21\u732e\u4e0d\u540c\uff08\u8fd1\u7684\u70b9\u66f4\u5927\uff09\n        realMatrix[nanMask] = 0  #\u65e0\u6548\u70b9\u7684\u76f8\u5bf9\u8bef\u5dee\u7f6e\u4e3a0\n        errors['ABS_REL'] = torch.sum(realMatrix) / nValidElement  #\u76f8\u5bf9\u8bef\u5dee\u6c42\u548c\u53d6\u5e73\u5747\n\n        LG10Matrix = torch.abs(lg10(_output) - lg10(_target))  #\u9884\u6d4b\u548cgt\u5148\u53d6log10\u518d\u505a\u5dee\uff0c\u53d6\u7edd\u5bf9\u503c\uff0c\u76f8\u540c\u8bef\u5dee\u503c\u5728\u6df1\u5ea6\u4e0d\u540c\u7684\u70b9\u8d21\u732e\u4e0d\u540c\uff08\u8fd1\u7684\u70b9\u66f4\u5927\uff09\n        LG10Matrix[nanMask] = 0\n        errors['LG10'] = torch.sum(LG10Matrix) / nValidElement\n\n        yOverZ = torch.div(_output, _target)\n        zOverY = torch.div(_target, _output)\n\n        maxRatio = maxOfTwo(yOverZ, zOverY)  #\u9884\u6d4b\u503c\u4e3a\u6b63\u4e14\u5c0f\u4e8egt\u7684\u70b9\u8d4b\u503c\u4e3azOverY\n\n        errors['DELTA1'] = torch.sum(\n            torch.le(maxRatio, 1.25).float()) / nValidElement\n        errors['DELTA2'] = torch.sum(\n            torch.le(maxRatio, math.pow(1.25, 2)).float()) / nValidElement\n        errors['DELTA3'] = torch.sum(\n            torch.le(maxRatio, math.pow(1.25, 3)).float()) / nValidElement\n\n        errors['MSE'] = float(errors['MSE'].data.cpu().numpy())\n        errors['ABS_REL'] = float(errors['ABS_REL'].data.cpu().numpy())\n        errors['LG10'] = float(errors['LG10'].data.cpu().numpy())\n        errors['MAE'] = float(errors['MAE'].data.cpu().numpy())\n        errors['DELTA1'] = float(errors['DELTA1'].data.cpu().numpy())\n        errors['DELTA2'] = float(errors['DELTA2'].data.cpu().numpy())\n        errors['DELTA3'] = float(errors['DELTA3'].data.cpu().numpy())\n\n    return errors\n\n\ndef addErrors(errorSum, errors, batchSize):\n    #import ipdb;ipdb.set_trace()\n    errorSum['MSE']=errorSum['MSE'] + errors['MSE'] * batchSize\n    errorSum['ABS_REL']=errorSum['ABS_REL'] + errors['ABS_REL'] * batchSize\n    errorSum['LG10']=errorSum['LG10'] + errors['LG10'] * batchSize\n    errorSum['MAE']=errorSum['MAE'] + errors['MAE'] * batchSize\n\n    errorSum['DELTA1']=errorSum['DELTA1'] + errors['DELTA1'] * batchSize\n    errorSum['DELTA2']=errorSum['DELTA2'] + errors['DELTA2'] * batchSize\n    errorSum['DELTA3']=errorSum['DELTA3'] + errors['DELTA3'] * batchSize\n\n    return errorSum\n\n\ndef averageErrors(errorSum, N):\n    #import ipdb;ipdb.set_trace()\n    averageError={'MSE': 0, 'RMSE': 0, 'ABS_REL': 0, 'LG10': 0,\n                    'MAE': 0,  'DELTA1': 0, 'DELTA2': 0, 'DELTA3': 0}\n\n    averageError['MSE'] = errorSum['MSE'] / N\n    averageError['ABS_REL'] = errorSum['ABS_REL'] / N\n    averageError['LG10'] = errorSum['LG10'] / N\n    averageError['MAE'] = errorSum['MAE'] / N\n\n    averageError['DELTA1'] = errorSum['DELTA1'] / N\n    averageError['DELTA2'] = errorSum['DELTA2'] / N\n    averageError['DELTA3'] = errorSum['DELTA3'] / N\n\n    return averageError\n\n\n\n\n\n\t\n", "loaddata_ori.py": "import pandas as pd\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\nfrom PIL import Image\nimport random\nfrom nyu_transform import *\nimport os\n\n\nclass depthDataset(Dataset):\n    \"\"\"Face Landmarks dataset.\"\"\"\n\n    def __init__(self, dataroot):\n    \n        image_file = 'RGB'\n        depth_file = 'Depth'\n        Semantic_file = 'Semantic_label'\n    \n        img_list = os.listdir(os.path.join(dataroot, image_file))\n        self.image_names = [os.path.join(dataroot, image_file, i) for i in img_list]\n        self.depth_names = [os.path.join(dataroot, depth_file, i) for i in img_list]\n        self.Semantic_names = [os.path.join(dataroot, Semantic_file, i) for i in img_list]\n        \n        \n        __imagenet_pca = {\n            'eigval': torch.Tensor([0.2175, 0.0188, 0.0045]),\n            'eigvec': torch.Tensor([\n                [-0.5675,  0.7192,  0.4009],\n                [-0.5808, -0.0045, -0.8140],\n                [-0.5836, -0.6948,  0.4203],\n            ])\n        }\n        __imagenet_stats = {'mean': [0.485, 0.456, 0.406],\n                            'std': [0.229, 0.224, 0.225]}\n        \n        \n        \n        if 'Train' in dataroot:\n            '''\n            self.transform=transforms.Compose([\n                                            Scale(240),\n                                            RandomHorizontalFlip(),\n                                            # RandomRotate(5),\n                                            # CenterCrop([304, 228], [152, 114], [152, 114]),\n                                            # CenterCrop([304, 224], [152, 112], [152, 112]),\n                                            # CenterCrop([320, 240], [160, 120], [160, 120]),\n                                            ToTensor(dataroot),\n                                            # Lighting(0.1, __imagenet_pca[\n                                                # 'eigval'], __imagenet_pca['eigvec']),\n                                            # ColorJitter(\n                                                # brightness=0.4,\n                                                # contrast=0.4,\n                                                # saturation=0.4,\n                                            # ),\n                                            # Normalize(__imagenet_stats['mean'],\n                                                        # __imagenet_stats['std'])\n                                            ])\n        '''\n            if 'CAD' in dataroot:\n                self.transform=transforms.Compose([\n                                                Scale(240, 0.5),\n                                                RandomHorizontalFlip(),\n                                                ToTensor(dataroot),\n                                                ])\n            else:\n                self.transform=transforms.Compose([\n                                                Scale(-1, 0.5),\n                                                RandomHorizontalFlip(),\n                                                ToTensor(dataroot),\n                                                ])\n            \n                                            \n        else:\n            '''\n            self.transform=transforms.Compose([\n                                            Scale(240),\n                                            # CenterCrop([304, 224], [152, 112], [152, 112]),\n                                            # CenterCrop([320, 240], [160, 120], [160, 120]),\n                                            ToTensor(dataroot),\n                                            # Normalize(__imagenet_stats['mean'],\n                                                        # __imagenet_stats['std'])\n                                            ])\n            '''\n            if 'CAD' in dataroot:\n                self.transform=transforms.Compose([\n                                                Scale(240, 0.5),\n                                                ToTensor(dataroot),\n                                                ])\n            else:\n                self.transform=transforms.Compose([\n                                                Scale(-1, 0.5),\n                                                ToTensor(dataroot),\n                                                ])\n                                                \n        \n        self.map_ids = np.array([0,1,-1,2,-1,-1,-1,-1,-1,-1,-1,3,-1,-1,4,5,6,7,-1,-1,-1,-1,-1,-1,-1,8,9,10,\n                                11,12,13,14,-1,15,16,17,18,19,-1,20,21,22,23,-1,24,25,26,27,28,29,30,\n                                31,-1,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,\n                                51,52,53,54,55,-1,56,57,58,-1])\n        \n    def Semantic_process(self, Semantic):\n        img = np.array(Semantic) #(240,320) uint8\n        # print('-------------before----------------')\n        # print(img.max())\n        # print(img.min())\n        img_shape = img.shape\n        img = img.reshape((-1))\n        img_new = np.uint8(self.map_ids[img])\n        img_new = img_new.reshape(img_shape)\n        # print('-------------after----------------')\n        # print(img_new.max())\n        # print(img_new.min())\n        img_new = Image.fromarray(img_new)\n        return img_new\n    \n    def __getitem__(self, idx):\n        image_name = self.image_names[idx]\n        depth_name = self.depth_names[idx]\n        Semantic_name = self.Semantic_names[idx]\n\n        image = Image.open(image_name)\n        image = image.convert('RGB')\n        depth = Image.open(depth_name)\n        Semantic = Image.open(Semantic_name)\n        \n        Semantic = self.Semantic_process(Semantic)\n\n        sample = {'image': image, 'depth': depth, 'Semantic': Semantic}\n\n        if self.transform:\n            sample = self.transform(sample)\n        \n\n        return sample\n\n    def __len__(self):\n        return len(self.image_names)\n\n        \n        \ndef getTrainingData(dataroot, batch_size=64, is_refine=False):\n    \n    transformed_training = depthDataset(dataroot=os.path.join(dataroot, 'Train'))\n\n    dataloader_training = DataLoader(transformed_training, batch_size,drop_last=True,\n                                     shuffle=True, num_workers=32, pin_memory=False)\n\n    return dataloader_training\n\n\ndef getTestingData(dataroot, batch_size=64):\n\n    transformed_testing = depthDataset(dataroot=os.path.join(dataroot, 'Test'))\n\n    dataloader_testing = DataLoader(transformed_testing, batch_size,\n                                    shuffle=False, num_workers=0, pin_memory=False)\n\n    return dataloader_testing\n\n", "iouEval.py": "import torch\n\nclass iouEval:\n\n    def __init__(self, nClasses, ignoreIndex=255):\n        self.nClasses = nClasses\n        self.ignoreIndex = ignoreIndex if nClasses>ignoreIndex else -1 #if ignoreIndex is larger than nClasses, consider no ignoreIndex\n        self.reset()\n\n    def reset (self):\n        classes = self.nClasses if self.ignoreIndex==-1 else self.nClasses-1\n        self.tp = torch.zeros(classes).double()\n        self.fp = torch.zeros(classes).double()\n        self.fn = torch.zeros(classes).double()        \n\n    def addBatch(self, x, y):   #x=preds, y=targets\n        #sizes should be \"batch_size x nClasses x H x W\"\n        \n        #print (\"X is cuda: \", x.is_cuda)\n        #print (\"Y is cuda: \", y.is_cuda)\n\n        if (x.is_cuda or y.is_cuda):\n            x = x.cuda()\n            y = y.cuda()\n\n        #if size is \"batch_size x 1 x H x W\" scatter to onehot\n        if (x.size(1) == 1):\n            x_onehot = torch.zeros(x.size(0), self.nClasses, x.size(2), x.size(3))  \n            if x.is_cuda:\n                x_onehot = x_onehot.cuda()\n            x_onehot.scatter_(1, x, 1).float()\n        else:\n            x_onehot = x.float()\n\n        if (y.size(1) == 1):\n            y_onehot = torch.zeros(y.size(0), self.nClasses, y.size(2), y.size(3))\n            if y.is_cuda:\n                y_onehot = y_onehot.cuda()\n            y_onehot.scatter_(1, y, 1).float()\n        else:\n            y_onehot = y.float()\n\n        if (self.ignoreIndex != -1): \n            ignores = y_onehot[:,self.ignoreIndex].unsqueeze(1)\n            x_onehot = x_onehot[:, :self.ignoreIndex]\n            y_onehot = y_onehot[:, :self.ignoreIndex]\n        else:\n            ignores=0\n\n        #print(type(x_onehot))\n        #print(type(y_onehot))\n        #print(x_onehot.size())\n        #print(y_onehot.size())\n\n        tpmult = x_onehot * y_onehot    #times prediction and gt coincide is 1\n        tp = torch.sum(torch.sum(torch.sum(tpmult, dim=0, keepdim=True), dim=2, keepdim=True), dim=3, keepdim=True).squeeze()\n        fpmult = x_onehot * (1-y_onehot-ignores) #times prediction says its that class and gt says its not (subtracting cases when its ignore label!)\n        fp = torch.sum(torch.sum(torch.sum(fpmult, dim=0, keepdim=True), dim=2, keepdim=True), dim=3, keepdim=True).squeeze()\n        fnmult = (1-x_onehot) * (y_onehot) #times prediction says its not that class and gt says it is\n        fn = torch.sum(torch.sum(torch.sum(fnmult, dim=0, keepdim=True), dim=2, keepdim=True), dim=3, keepdim=True).squeeze() \n\n        self.tp += tp.double().cpu()\n        self.fp += fp.double().cpu()\n        self.fn += fn.double().cpu()\n\n    def getIoU(self):\n        num = self.tp\n        den = self.tp + self.fp + self.fn + 1e-15\n        iou = num / den\n        return torch.mean(iou), iou     #returns \"iou mean\", \"iou per class\"\n\n# Class for colors\nclass colors:\n    RED       = '\\033[31;1m'\n    GREEN     = '\\033[32;1m'\n    YELLOW    = '\\033[33;1m'\n    BLUE      = '\\033[34;1m'\n    MAGENTA   = '\\033[35;1m'\n    CYAN      = '\\033[36;1m'\n    BOLD      = '\\033[1m'\n    UNDERLINE = '\\033[4m'\n    ENDC      = '\\033[0m'\n\n# Colored value output if colorized flag is activated.\ndef getColorEntry(val):\n    if not isinstance(val, float):\n        return colors.ENDC\n    if (val < .20):\n        return colors.RED\n    elif (val < .40):\n        return colors.YELLOW\n    elif (val < .60):\n        return colors.BLUE\n    elif (val < .80):\n        return colors.CYAN\n    else:\n        return colors.GREEN\n\n\n", "test.py": "import argparse\n\nimport time\nimport torch\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport loaddata_ori as loaddata\nimport loaddata_fastdepth\nimport os\nimport numpy as np\nimport cv2\nfrom models.pdesnet import PDESNet\nfrom iouEval import iouEval, getColorEntry\nimport util\nimport matplotlib.pyplot as plt\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n\n\ndef load_model_my(model, checkpoint):\n    model_dict = model.state_dict()\n    # import ipdb; ipdb.set_trace()\n    pretrained_dict = torch.load(checkpoint)['state_dict']\n    \n    # 1. filter out unnecessary keys\n    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n    # 2. overwrite entries in the existing state dict\n    model_dict.update(pretrained_dict)\n    # 3. load the new state dict\n    model.load_state_dict(model_dict)\n    \n    return model\n\n    \ndef main(checkpoint, backbone_network, n_classes, save_file, dataroot):\n    #import ipdb;ipdb.set_trace()\n    \n    model = PDESNet.build(backbone_network,n_classes,is_train=False)\n    # if 'yangmei' in backbone_network:\n        # model = model.cuda()\n    # else:\n        # model = torch.nn.DataParallel(model).cuda()\n    model = model.cuda()\n    try:\n        model.load_state_dict(torch.load(checkpoint)['state_dict'])#(torch.load(checkpoint)['state_dict'])\n    except:\n        model.load_state_dict(torch.load(checkpoint))\n    print('Loading checkpoint ' + checkpoint)\n    # else:\n        # model.apply(weights_init)\n    \n    cudnn.benchmark = True\n    \n    if backbone_network.lower() == 'fastdepth':\n        # train_loader = loaddata_fastdepth.getTrainingData(batch_size)\n        test_loader = loaddata_fastdepth.getTestingData(1)\n    else:\n        # train_loader = loaddata.getTrainingData(dataroot,batch_size)\n        test_loader = loaddata.getTestingData(dataroot, 1)\n\n    \n    test(test_loader, model, n_classes, save_file)\n        \n\n \ndef test(test_loader, model, n_classes, save_file):\n    \n    model.eval()\n    iouEvalVal = iouEval(n_classes)\n    \n    totalNumber = 0\n\n    errorSum = {'MSE': 0, 'RMSE': 0, 'ABS_REL': 0, 'LG10': 0,\n                'MAE': 0,  'DELTA1': 0, 'DELTA2': 0, 'DELTA3': 0}\n    img_num = len(test_loader)\n    for i, sample_batched in enumerate(test_loader):\n        \n        \n        image, depth, Semantic = sample_batched['image'], sample_batched['depth'], sample_batched['Semantic']\n        \n        depth = depth.cuda()\n        image = image.cuda()\n        Semantic = Semantic.cuda().long()\n        image = torch.autograd.Variable(image, volatile=True)\n        depth = torch.autograd.Variable(depth, volatile=True)\n        Semantic = torch.autograd.Variable(Semantic, volatile=True)\n\n        output = model(image)\n        # import ipdb; ipdb.set_trace()\n        pre_depth = output['depth']\n        pre_class_mask = output['class_mask']\n        \n        save_result(i, image, depth, Semantic, pre_depth, pre_class_mask, n_classes, save_file)\n        \n        errorSum, totalNumber = eval_depth(pre_depth, depth, errorSum, totalNumber)\n        # import ipdb; ipdb.set_trace()\n        iouEvalVal.addBatch(pre_class_mask.max(1)[1].unsqueeze(1).data, Semantic.data)\n        \n        if i % 100 == 0:\n            print(str(i)+' / '+str(img_num))\n            # show_eval_result(errorSum, totalNumber, iouEvalVal)\n        # if i > 10:\n            # break\n    \n    show_eval_result(errorSum, totalNumber, iouEvalVal)\n    \n    \n\ndef show_eval_result(errorSum, totalNumber, iouEvalVal):\n    #\u53ea\u663e\u793a\u4e24\u4e2a\u6307\u6807\n    # averageError = util.averageErrors(errorSum, totalNumber)\n    # averageError['RMSE'] = np.sqrt(averageError['MSE'])\n    # line_depth = 'RMSE: ' + str(averageError['RMSE']) + '   Detal1: ' + str(averageError['DELTA1'])\n    # print(line_depth)\n    #\u663e\u793a\u5168\u90e8\u6307\u6807\n    averageError = util.averageErrors(errorSum, totalNumber)\n    averageError['RMSE'] = np.sqrt(averageError['MSE'])\n    averageError_keys = averageError.keys()\n    for key in averageError_keys:\n        line_depth = key+': ' + str(averageError[key])\n        print(line_depth)\n    \n    iouVal, iou_classes = iouEvalVal.getIoU()\n    iouStr = getColorEntry(iouVal)+'{:0.2f}'.format(iouVal*100) + '\\033[0m'\n    iouPeople = getColorEntry(iou_classes[1])+'{:0.2f}'.format(iou_classes[1]*100) + '\\033[0m'\n    line_mask = \"EPOCH IoU on VAL set: \"+str(iouStr)+\"% \"+ \"   People IoU: \"+ str(iouPeople)+\"%\"\n    print (line_mask)\n    \n    \n    \ndef eval_depth(pre_depth, depth, errorSum, totalNumber):\n    # output = torch.nn.functional.upsample(pre_depth, size=[depth.size(2),depth.size(3)], mode='bilinear')\n\n    #depth_edge = edge_detection(depth)\n    #output_edge = edge_detection(output)\n    \n    pre_depth[pre_depth<0] = 0\n    pre_depth[pre_depth>10] = 10\n    \n    batchSize = depth.size(0)\n    totalNumber = totalNumber + batchSize\n    errors = util.evaluateError(pre_depth, depth)\n    errorSum = util.addErrors(errorSum, errors, batchSize)\n    return errorSum, totalNumber\n            \ndef normalize_img(img, n_classes, img_name):\n\n    img_shape = img.shape\n    if img_shape[1] == 3:\n        img = np.array(img.squeeze().cpu()).transpose(1,2,0)[:,:,(2,1,0)] #(h,w,3) BGR\n    elif img_shape[1] == n_classes:\n        img = np.array(img.max(1)[1].squeeze().cpu())\n    else:\n        if 'pre' in img_name:\n            img = img.squeeze().cpu().detach().numpy()\n        else:\n            img = np.array(img.squeeze().cpu())\n            \n    if 'mask' in img_name:\n        # img[img>1] = 0\n        img[img>1] = 1\n    if 'depth' in img_name:\n        img[img<0] = 0\n        img[img>10] = 10\n        img = np.uint8(img/10*255)\n    else:\n        img = np.uint8(((img-img.min())/(img.max()-img.min()))*255)\n        \n    if not 'image' in img_name:\n        img = cv2.resize(img, (img.shape[1]*2, img.shape[0]*2))\n    \n    return img\n \ndef save_result(i, image, depth, Semantic, pre_depth, pre_class_mask, n_classes, save_file):\n    pre_depth[depth==0] = 0\n    imgs = [image, depth, Semantic, pre_depth, pre_class_mask]\n    img_names = ['image', 'depth_gt', 'mask_gt', 'depth_pre', 'mask_pre']\n    for img_i, img in enumerate(imgs):\n        img_name = img_names[img_i]\n        img_numpy = normalize_img(img, n_classes, img_name)\n        img_name = os.path.join(save_file, str(i)+'_'+img_name+'.jpg')\n        if 'depth' in img_name:\n            plt.imsave(img_name, img_numpy,cmap='rainbow')\n            # cv2.imwrite(img_name, img_numpy)\n        else:\n            cv2.imwrite(img_name, img_numpy)\n\n\nif __name__ == '__main__':\n        \n    # data_mode = 'data_CAD120'\n    # data_mode = 'data_CAD60'\n    data_mode = 'EPFL'\n    \n    n_classes = 58+1\n    # backbone_network = 'mobilenet'\n    # backbone_network = 'mobilenet_prune'\n    # backbone_network = 'fast_mobilenetv1_aspp'\n    backbone_network = 'fast_mobilenetv1_aspp_prune'\n    # backbone_network = 'ESPNet_Encoder_yangmei'\n    # backbone_network = 'fast_mobilenetv1_aspp_maskinput'\n    # backbone_network = 'fastdepth'\n    save_file = 'result'\n    if os.path.exists(save_file):\n        os.system('rm -r '+save_file)\n    os.system('mkdir -p '+save_file)\n    \n   \n    # checkpoint = 'model_out/fast_mobilenetv1_aspp/20200806-185758/checkpoint_mobilenet_99.pth.tar' #aspp_60\n    # checkpoint = 'model_out/fast_mobilenetv1_aspp/20200806-185446/checkpoint_mobilenet_99.pth.tar' #aspp_120\n    # checkpoint = 'model_out/fast_mobilenetv1_aspp/20200818-105502/checkpoint_mobilenet_299.pth.tar' #aspp_epfl\n    # checkpoint = 'model_out/fast_mobilenetv1_aspp_prune/20200814-170629/checkpoint_mobilenet_99.pth.tar' #prune_60\n    # checkpoint = 'model_out/fast_mobilenetv1_aspp_prune/20200817-145650/checkpoint_mobilenet_99.pth.tar' #prune_120\n    checkpoint = 'our_model/EPFL_checkpoint_mobilenet.pth.tar' #prune_epfl\n    # checkpoint = None\n    \n    dataroot = os.path.join('data', data_mode)\n    main(checkpoint, backbone_network, n_classes, save_file, dataroot)\n\n", "README.md": "# Real-Time Monocular Human Depth Estimation and Segmentation on Embedded Systems\n\nThis is the Pytorch implementation of our paper: Shan An, Fangru Zhou, Mei Yang, Haogang Zhu, Changhong Fu, and Konstantinos A. Tsintotas. Real-time Monocular Human Depth Estimation and Segmentation on Embedded Systems. Accepted by IROS 2021.\n\n**Abstract:** \n\nEstimating a scene's depth to achieve collision avoidance against moving pedestrians is a crucial and fundamental problem in the robotic field.\nThis paper proposes a novel, low complexity network architecture for fast and accurate human depth estimation and segmentation in indoor environments, aiming to applications for resource-constrained platforms (including battery-powered aerial, micro-aerial, and ground vehicles) with a monocular camera being the primary perception module. Following the encoder-decoder structure, the proposed framework consists of two branches, one for depth prediction and another for semantic segmentation. Moreover, network structure optimization is employed to improve its forward inference speed. Exhaustive experiments on three self-generated datasets prove our pipeline's capability to execute in real-time, achieving higher frame rates than contemporary state-of-the-art frameworks (114.6 frames per second on an NVIDIA Jetson Nano GPU with tensorRT) while maintaining comparable accuracy.\n\n**Videos:**\n\nBaidu Drive\uff1ahttps://pan.baidu.com/s/1AKUZuRf_u-PtBDQ7ocGbag \nCode\uff1adess \n\nGoogle Drive: https://drive.google.com/file/d/1eTjGA_7pJxVFl1RlJxMbC-SWVo9FtKAQ/view?usp=sharing\n\n**Paper:**\nhttps://arxiv.org/abs/2108.10506\n", "sobel.py": "import torch\nimport torch.nn as nn\nimport numpy as np\n\nclass Sobel(nn.Module):\n    def __init__(self):\n        super(Sobel, self).__init__()\n        self.edge_conv = nn.Conv2d(1, 2, kernel_size=3, stride=1, padding=1, bias=False)\n        edge_kx = np.array([[1, 0, -1], [2, 0, -2], [1, 0, -1]])\n        edge_ky = np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]])\n        edge_k = np.stack((edge_kx, edge_ky))\n\n        edge_k = torch.from_numpy(edge_k).float().view(2, 1, 3, 3)\n        self.edge_conv.weight = nn.Parameter(edge_k)\n        \n        for param in self.parameters():\n            param.requires_grad = False\n\n    def forward(self, x):\n        out = self.edge_conv(x) \n        out = out.contiguous().view(-1, 2, x.size(2), x.size(3))\n  \n        return out\n\n", "_config.yml": "theme: jekyll-theme-cayman\nwebmaster_verifications:\n  google: dgETCrZ9HJMcTFtG70Ya800RYhcYFKtOa9JWvtK9DPs\n", "train.py": "import argparse\n\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport loaddata_ori as loaddata\nimport loaddata_fastdepth\nimport util\nimport os\nimport numpy as np\nimport sobel\nfrom models.pdesnet import PDESNet\nfrom models.VNL_loss import VNL_Loss\nfrom datetime import datetime as dat\nimport warnings\nfrom iouEval import iouEval, getColorEntry\nimport cv2\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\nwarnings.filterwarnings(\"ignore\")\n\nparser = argparse.ArgumentParser(description='PyTorch DenseNet Training')\nparser.add_argument('--backbone_network', \n                help = 'name of backbone network',default='mobilenet_prune')#shufflenet, mobilenet, and darknet\n\nparser.add_argument('--momentum', default=0.9, type=float, help='momentum')\nparser.add_argument('--weight-decay', '--wd', default=1e-4, type=float,\n                    help='weight decay (default: 1e-4)')##\\4e-5\nparser.add_argument('--save_folder', default='weights/depth_normal/liteseg_759_SGD_poly_lr1e-4_refine_vnl/')\n\n\ndef load_model_my(model, checkpoint):\n    model_dict = model.state_dict()\n    # import ipdb; ipdb.set_trace()\n    pretrained_dict = torch.load(checkpoint)['state_dict']\n    \n    # 1. filter out unnecessary keys\n    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n    # 2. overwrite entries in the existing state dict\n    model_dict.update(pretrained_dict)\n    # 3. load the new state dict\n    model.load_state_dict(model_dict)\n    \n    return model\n\ndef weights_init(m):\n    # Initialize kernel weights with Gaussian distributions\n    if isinstance(m, nn.Conv2d):\n        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        m.weight.data.normal_(0, math.sqrt(2. / n))\n        if m.bias is not None:\n            m.bias.data.zero_()\n    elif isinstance(m, nn.ConvTranspose2d):\n        n = m.kernel_size[0] * m.kernel_size[1] * m.in_channels\n        m.weight.data.normal_(0, math.sqrt(2. / n))\n        if m.bias is not None:\n            m.bias.data.zero_()\n    elif isinstance(m, nn.BatchNorm2d):\n        m.weight.data.fill_(1)\n        m.bias.data.zero_()\n    \ndef main(ckpt_dir, checkpoint, lr, start_epoch, epochs, batch_size, backbone_network, n_classes, adjust_epoch, dataroot, opt):\n    #import ipdb;ipdb.set_trace()\n    global args\n    args = parser.parse_args()\n    model = PDESNet.build(backbone_network,n_classes,args,is_train=True)\n        \n    # model = torch.nn.DataParallel(model).cuda()\n    model = model.cuda()\n        \n    if checkpoint:\n        try:\n            model.load_state_dict(torch.load(checkpoint)['state_dict'])#(torch.load(checkpoint)['state_dict'])\n            print('Load all parameters')\n        except:\n            model = load_model_my(model, checkpoint)\n            print('Load some parameters')\n    # else:\n        # model.apply(weights_init)\n    \n    cudnn.benchmark = True\n    #optimizer = torch.optim.Adam(model.parameters(), args.lr, weight_decay=args.weight_decay)\n    ##\u539f\u59cbliteseg\u4f7f\u7528SGD\uff0c\u521d\u59cblr\u4e3a1e-7\n    if opt == 'Adam':\n        lr = 5e-4\n        optimizer = torch.optim.Adam(model.parameters(), lr, (0.9, 0.999), eps=1e-08, weight_decay=5e-4)\n    elif opt == 'SGD':\n        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=args.momentum, weight_decay=args.weight_decay)\n    \n    if backbone_network.lower() == 'fastdepth':\n        train_loader = loaddata_fastdepth.getTrainingData(batch_size)\n        test_loader = loaddata_fastdepth.getTestingData(1)\n    else:\n        train_loader = loaddata.getTrainingData(dataroot, batch_size)\n        test_loader = loaddata.getTestingData(dataroot, 1)\n\n    for epoch in range(start_epoch, epochs):\n        \n        gap = epochs//(adjust_epoch+1)\n        if epoch % gap == (gap-1) and opt == 'SGD':\n            # lr = adjust_learning_rate_poly(optimizer, epoch, epochs, lr)\n            # lr = adjust_learning_rate(optimizer, epoch, epochs, lr)\n            lr = lr*0.1\n            optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=args.momentum, weight_decay=args.weight_decay)\n        if opt == 'Adam' and epoch % 100 == 0 and epoch != 0:\n            lr = lr*0.5\n            optimizer = torch.optim.Adam(model.parameters(), lr, (0.9, 0.999), eps=1e-08, weight_decay=5e-4)\n        \n        train(train_loader, model, optimizer, epoch, ckpt_dir, lr)\n        test(test_loader, model, n_classes, epoch, ckpt_dir)\n        \n    save_checkpoint({'state_dict': model.state_dict()}, 'checkpoint.pth.tar')\n\nL1loss = nn.L1Loss()\ncriterion_mask = nn.CrossEntropyLoss()\n    \n    \ndef criterion_depth(pre_depth, gt_depth):\n    pre_depth = pre_depth.view(-1)[gt_depth.view(-1)!=0]\n    gt_depth = gt_depth.view(-1)[gt_depth.view(-1)!=0]\n    \n    loss_depth = F.smooth_l1_loss(pre_depth, gt_depth, size_average=True)\n    # loss_depth = L1loss(pre_depth, gt_depth)\n    return loss_depth\n    \ndef criterion_depth_weight(pre_depth, gt_depth, Semantic):\n    \n    masks = Semantic.cpu().float() #(b,h,w)\n    \n    kernel = np.ones((5,5),np.uint8)\n    batch_size = len(masks)\n    for id in range(batch_size):\n        mask = np.uint8(masks[id].numpy())\n        mask[mask>0] = 255\n        mask_erosion = cv2.erode(mask,kernel,iterations = 1) #\u8150\u8680\n        mask_dilation = cv2.dilate(mask,kernel,iterations = 1) #\u81a8\u80c0\n        mask_edge = mask_dilation-mask_erosion\n        mask_edge[mask_edge>0] = 5\n        mask_edge[mask_edge==0] = 1\n        # print(len(np.where(mask_edge>1)[0]))\n        mask_edge = torch.from_numpy(mask_edge).float().cuda()\n        masks[id] = mask_edge\n        \n    # print(masks.shape)\n    # print(gt_depth.shape)\n    masks = masks.view(-1)[gt_depth.view(-1)!=0]\n    pre_depth = pre_depth.view(-1)[gt_depth.view(-1)!=0]\n    gt_depth = gt_depth.view(-1)[gt_depth.view(-1)!=0]\n    \n    loss_depth = F.smooth_l1_loss(pre_depth, gt_depth, weight=masks, size_average=True)\n    return loss_depth\n\n    \ndef train(train_loader, model, optimizer, epoch, ckpt_dir, lr):\n    \n    # vnl_loss = VNL_Loss(input_size = np.array([114, 152]))\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    \n\n    model.train()\n\n    cos = nn.CosineSimilarity(dim=1, eps=0)\n    get_gradient = sobel.Sobel().cuda()\n\n    start_epoch = time.time()\n    end = time.time()\n    for i, sample_batched in enumerate(train_loader):\n        \n        image, depth, Semantic = sample_batched['image'], sample_batched['depth'], sample_batched['Semantic']\n        \n        '''\n        ## \u68c0\u67e5\u6570\u636e\n        image_debug = np.transpose(np.array(image[0]),(1,2,0))[:,:, (2, 1, 0)]\n        image_debug = (image_debug-image_debug.min())/(image_debug.max()-image_debug.min())\n        image_debug = np.uint8(image_debug*255)\n        \n        depth_debug = np.array(depth[0][0])\n        # print(depth_debug.shape)\n        depth_debug = (depth_debug-depth_debug.min())/(depth_debug.max()-depth_debug.min())\n        depth_debug = np.uint8(depth_debug*255)\n        import cv2\n        debug_file = 'debug_file'\n        if os.path.exists(debug_file):\n            os.system('rm -r '+debug_file)\n        os.system('mkdir -p '+debug_file)\n        image_debug_name = os.path.join(debug_file, str(i)+'_image.jpg')\n        depth_debug_name = os.path.join(debug_file, str(i)+'_depth.jpg')\n        cv2.imwrite(image_debug_name,  image_debug)\n        cv2.imwrite(depth_debug_name,  depth_debug)\n        \n        import ipdb;ipdb.set_trace()\n        '''\n\n        depth = depth.cuda()\n        image = image.cuda()\n        Semantic = Semantic.cuda().long().squeeze()\n        image = torch.autograd.Variable(image)\n        depth = torch.autograd.Variable(depth)\n        Semantic = torch.autograd.Variable(Semantic)\n\n        ones = torch.ones(depth.size(0), 1, depth.size(2),depth.size(3)).float().cuda()\n        ones = torch.autograd.Variable(ones)\n        optimizer.zero_grad()\n\n        output = model(image)\n        # import ipdb; ipdb.set_trace()\n        pre_depth = output['depth']\n        pre_class_mask = output['class_mask']\n\n        '''\n        depth_grad = get_gradient(depth)\n        output_grad = get_gradient(output)\n        depth_grad_dx = depth_grad[:, 0, :, :].contiguous().view_as(depth)\n        depth_grad_dy = depth_grad[:, 1, :, :].contiguous().view_as(depth)\n        output_grad_dx = output_grad[:, 0, :, :].contiguous().view_as(depth)\n        output_grad_dy = output_grad[:, 1, :, :].contiguous().view_as(depth)\n\n        depth_normal = torch.cat((-depth_grad_dx, -depth_grad_dy, ones), 1)\n        output_normal = torch.cat((-output_grad_dx, -output_grad_dy, ones), 1)\n\n        #depth_normal = F.normalize(depth_normal, p=2, dim=1)\n        #output_normal = F.normalize(output_normal, p=2, dim=1)\n\n        loss_depth = torch.log(torch.abs(output - depth) + 0.5).mean()\n        loss_dx = torch.log(torch.abs(output_grad_dx - depth_grad_dx) + 0.5).mean()\n        loss_dy = torch.log(torch.abs(output_grad_dy - depth_grad_dy) + 0.5).mean()\n        loss_normal = torch.abs(1 - cos(output_normal, depth_normal)).mean()\n        \n        loss_vnl = vnl_loss(depth/5, output/5)\n        \n\n        loss = loss_depth + loss_normal + (loss_dx + loss_dy) \n        '''\n        # import ipdb; ipdb.set_trace()\n        loss_depth = criterion_depth(pre_depth, depth)\n        # loss_depth = criterion_depth_weight(pre_depth, depth, Semantic)\n        # import ipdb; ipdb.set_trace()\n        # print(Semantic.max())\n        loss_mask = criterion_mask(pre_class_mask, Semantic)\n        loss = loss_depth + loss_mask\n        # loss = loss_mask\n        # import ipdb; ipdb.set_trace()\n        losses.update(loss.data, image.size(0))\n        loss.backward()\n        optimizer.step()\n\n        batch_time.update(time.time() - end)\n        end = time.time()\n   \n        '''\n        print('Epoch: [{0}][{1}/{2}]\\t'\n          'Depth: {3:.4f}\\t'\n          'Normal: {4:.4f}\\t'\n          'Dx: {5:.4f}\\t'\n          'Dy: {6:.4f}\\t'\n          #'VNL: {7:.4f}\\t'\n          'Loss: {loss.val:.4f} ({loss.avg:.4f})'\n          .format(epoch, i, len(train_loader), loss_depth.item(), loss_normal.item(), loss_dx.item(), loss_dy.item(), loss=losses))\n        '''\n        if i%500 == 0:\n            print('Epoch: [{0}][{1}/{2}]\\t'\n            'D_loss: {3:.4f}\\t'\n            'M_loss: {4:.4f}\\t'\n            'epoch_loss: {5:.4f}\\t'\n            'avg_loss: {6:.4f}\\t'\n            'lr: {7:.6f}'\n            .format(epoch, i, len(train_loader), loss_depth.item(), loss_mask.item(), loss.item(), losses.avg, lr))\n          \n    end_epoch = time.time()\n    print('Epoch %d last %4f minutes.' % (epoch, (end_epoch-start_epoch)/60))\n    filename = os.path.join(ckpt_dir, 'checkpoint_mobilenet_' + str(epoch) + '.pth.tar')\n    save_checkpoint({'state_dict': model.state_dict()}, filename)\n \ndef test(test_loader, model, n_classes, epoch, ckpt_dir):\n    file = os.path.join(ckpt_dir, 'val.txt')\n    model.eval()\n    iouEvalVal = iouEval(n_classes)\n\n    totalNumber = 0\n\n    Ae = 0\n    Pe = 0\n    Re = 0\n    Fe = 0\n\n    errorSum = {'MSE': 0, 'RMSE': 0, 'ABS_REL': 0, 'LG10': 0,\n                'MAE': 0,  'DELTA1': 0, 'DELTA2': 0, 'DELTA3': 0}\n\n    for i, sample_batched in enumerate(test_loader):\n        \n        image, depth, Semantic = sample_batched['image'], sample_batched['depth'], sample_batched['Semantic']\n        \n        depth = depth.cuda()\n        image = image.cuda()\n        Semantic = Semantic.cuda().long()\n        image = torch.autograd.Variable(image, volatile=True)\n        depth = torch.autograd.Variable(depth, volatile=True)\n        Semantic = torch.autograd.Variable(Semantic, volatile=True)\n\n        output = model(image)\n        # import ipdb; ipdb.set_trace()\n        pre_depth = output['depth']\n        pre_class_mask = output['class_mask']\n        \n        #depth_edge = edge_detection(depth)\n        #output_edge = edge_detection(output)\n\n        errorSum, totalNumber = eval_depth(pre_depth, depth, errorSum, totalNumber)\n        \n        iouEvalVal.addBatch(pre_class_mask.max(1)[1].unsqueeze(1).data, Semantic.data)\n    \n    show_eval_result(errorSum, totalNumber, iouEvalVal, epoch, file)\n    \n    \n\ndef show_eval_result(errorSum, totalNumber, iouEvalVal, epoch, file):\n\n    averageError = util.averageErrors(errorSum, totalNumber)\n    model_name = 'checkpoint_'+str(epoch)\n    print(model_name)\n    averageError['RMSE'] = np.sqrt(averageError['MSE'])\n    line_depth = 'RMSE: ' + str(averageError['RMSE']) + '   Detal1: ' + str(averageError['DELTA1'])\n    print(line_depth)\n    \n    iouVal, iou_classes = iouEvalVal.getIoU()\n    iouStr = getColorEntry(iouVal)+'{:0.2f}'.format(iouVal*100) + '\\033[0m'\n    iouPeople = getColorEntry(iou_classes[1])+'{:0.2f}'.format(iou_classes[1]*100) + '\\033[0m'\n    line_mask = \"EPOCH IoU on VAL set: \"+str(iouStr)+\"% \"+ \"   People IoU: \"+ str(iouPeople)+\"%\"\n    print (line_mask) \n    \n    \n    f = open(file,'a+')\n    f.write(model_name + '\\n')\n    f.write(line_depth + '\\n')\n    f.write(line_mask + '\\n')\n    f.close()\n    \ndef eval_depth(pre_depth, depth, errorSum, totalNumber):\n    # output = torch.nn.functional.upsample(pre_depth, size=[depth.size(2),depth.size(3)], mode='bilinear')\n\n    #depth_edge = edge_detection(depth)\n    #output_edge = edge_detection(output)\n    \n    pre_depth[pre_depth<0] = 0\n    pre_depth[pre_depth>10] = 10\n\n    batchSize = depth.size(0)\n    totalNumber = totalNumber + batchSize\n    errors = util.evaluateError(pre_depth, depth)\n    errorSum = util.addErrors(errorSum, errors, batchSize)\n    return errorSum, totalNumber\n    \ndef adjust_learning_rate(optimizer, epoch, epochs, lr, adjust_epoch):\n    # lr = lr * (0.1 ** (epoch // 5))\n    \n    # lr = lr * (0.1 ** (epoch // (epochs//(adjust_epoch+1))))\n    lr = lr * 0.1\n\n    # for param_group in optimizer.param_groups:\n        # param_group['lr'] = lr\n\n    return lr\n    \ndef adjust_learning_rate_poly(optimizer, epoch, epochs, lr):\n    ##\u539f\u59cbliteseg\u7684lr\u66f4\u65b0\n    lr_ = lr * ((1 - float(epoch) / epochs) ** 0.9)\n    print('(poly lr policy) learning rate: ', lr_)\n    \n    return lr_\n\nclass AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef save_checkpoint(state, filename):\n    torch.save(state, filename)\n\n\nif __name__ == '__main__':\n    #opt = 'Adam'  ##EPFL\n    opt = 'SGD'  ##CAD\n    data_mode = 'data_CAD60'\n    #data_mode = 'data_CAD120'\n    #data_mode = 'EPFL'\n    print('The dataset is : '+data_mode)\n    adjust_epoch = 4 #\u4e00\u5171\u8c03\u6574\u51e0\u6b21\u5b66\u4e60\u7387,opt = 'SGD'\u65f6\u624d\u4f1a\u7528\u5230\n    # adjust_epoch = 2 #\u4e00\u5171\u8c03\u6574\u51e0\u6b21\u5b66\u4e60\u7387\n    lr = 1e-2 #opt = 'SGD'\u65f6\u624d\u4f1a\u7528\u5230\n    # lr = 1e-4\n    start_epoch = 0\n    epochs = 300 ##CAD100,EPFL300\n    # epochs = 100\n    # epochs = 50\n    # batch_size = 16\n    batch_size = 64\n    n_classes = 58+1\n    # backbone_network = 'mobilenet'\n    # backbone_network = 'mobilenet_prune'\n    #backbone_network = 'fast_mobilenetv1_aspp'\n    backbone_network = 'fast_mobilenetv1_aspp_prune'\n    # backbone_network = 'fast_mobilenetv1_aspp_conv'\n    #backbone_network = 'fast_mobilenetv1_no_aspp'\n    # backbone_network = 'fast_mobilenetv1_aspp_maskinput'\n    # backbone_network = 'ESPNet_Encoder'\n    # backbone_network = 'fastdepth'\n    \n    date_name = dat.strftime(dat.now(), '%Y%m%d-%H%M%S')\n    ckpt_dir = 'model_out'\n    if not os.path.exists(ckpt_dir):\n        os.makedirs(ckpt_dir)\n    ckpt_dir = os.path.join(ckpt_dir, backbone_network)\n    if not os.path.exists(ckpt_dir):\n        os.makedirs(ckpt_dir)\n    ckpt_dir = os.path.join(ckpt_dir,date_name)\n    if not os.path.exists(ckpt_dir):\n        os.makedirs(ckpt_dir)\n    print('Models saved in: '+ckpt_dir)\n    # checkpoint = 'model_out/fast_mobilenetv1_aspp/20200806-185758/checkpoint_mobilenet_99.pth.tar'\n    # checkpoint = 'model_out/fast_mobilenetv1_aspp/20200807-180736/checkpoint_mobilenet_99.pth.tar'\n    checkpoint = None\n    dataroot = os.path.join('data', data_mode)\n    main(ckpt_dir, checkpoint, lr, start_epoch, epochs, batch_size, backbone_network, n_classes, adjust_epoch, dataroot, opt)\n\n", "loaddata_fastdepth.py": "import pandas as pd\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\nfrom PIL import Image\nimport random\nfrom nyu_transform import *\nimport os\n\n\nclass depthDataset(Dataset):\n    \"\"\"Face Landmarks dataset.\"\"\"\n\n    def __init__(self, csv_file, transform=None):\n        #import ipdb;ipdb.set_trace()\n        self.frame = pd.read_csv(csv_file, header=None)\n        self.transform = transform\n\n    def __getitem__(self, idx):\n        image_name = self.frame[0][idx]\n        depth_name = self.frame[1][idx]\n\n        image = Image.open(image_name)\n        depth = Image.open(depth_name)\n\n        sample = {'image': image, 'depth': depth}\n        '''\n        ## \u68c0\u67e5\u6570\u636e\n        image_debug = np.array(image)[:,:, (2, 1, 0)]\n        depth_debug = np.array(depth)\n        import cv2\n        debug_file = 'debug_file'\n        if not os.path.exists(debug_file):\n            os.system('mkdir -p '+debug_file)\n        image_debug_name = os.path.join(debug_file, image_name.split('/')[-1])\n        depth_debug_name = os.path.join(debug_file, depth_name.split('/')[-1])\n        cv2.imwrite(image_debug_name,  image_debug)\n        cv2.imwrite(depth_debug_name,  depth_debug)\n        '''\n        \n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample\n\n    def __len__(self):\n        return len(self.frame)\n\nclass depthDataset_refine(Dataset):\n    \"\"\"Face Landmarks dataset.\"\"\"\n\n    def __init__(self, dataroot, transform=None):\n        #import ipdb;ipdb.set_trace()\n        self.images = [os.path.join(dataroot, str(i)+'.jpg') for i in range(795)]\n        self.depths = [os.path.join(dataroot, str(i)+'.png') for i in range(795)]\n        self.transform = transform\n        \n        #image_name = self.images[0]\n        #depth_name = self.depths[0]\n\n        #image = Image.open(image_name)\n        #depth = Image.open(depth_name)\n\n        #sample = {'image': image, 'depth': depth}\n\n        #if self.transform:\n            #sample = self.transform(sample)\n            \n    def __getitem__(self, idx):\n        image_name = self.images[idx]\n        depth_name = self.depths[idx]\n\n        image = Image.open(image_name)\n        depth = Image.open(depth_name)\n\n        sample = {'image': image, 'depth': depth}\n\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample\n\n    def __len__(self):\n        return len(self.images)\n        \n        \ndef getTrainingData(batch_size=64, is_refine=False):\n    __imagenet_pca = {\n        'eigval': torch.Tensor([0.2175, 0.0188, 0.0045]),\n        'eigvec': torch.Tensor([\n            [-0.5675,  0.7192,  0.4009],\n            [-0.5808, -0.0045, -0.8140],\n            [-0.5836, -0.6948,  0.4203],\n        ])\n    }\n    __imagenet_stats = {'mean': [0.485, 0.456, 0.406],\n                        'std': [0.229, 0.224, 0.225]}\n    if is_refine:\n        transformed_training = depthDataset_refine(dataroot='./data/nyu2_refine_labeled',\n                                        transform=transforms.Compose([\n                                            Scale(240),\n                                            RandomHorizontalFlip(),\n                                            RandomRotate(5),\n                                            CenterCrop([304, 228], [304, 228]),\n                                            Scale((224,224)),\n                                            ToTensor(),\n                                            Lighting(0.1, __imagenet_pca[\n                                                'eigval'], __imagenet_pca['eigvec']),\n                                            ColorJitter(\n                                                brightness=0.4,\n                                                contrast=0.4,\n                                                saturation=0.4,\n                                            ),\n                                            Normalize(__imagenet_stats['mean'],\n                                                      __imagenet_stats['std'])\n                                        ]))\n    else:\n        transformed_training = depthDataset(csv_file='./data/nyu2_train.csv',\n                                        transform=transforms.Compose([\n                                            Scale(240),\n                                            RandomHorizontalFlip(),\n                                            RandomRotate(5),\n                                            CenterCrop([304, 228], [304, 228]),\n                                            Scale((224,224)),\n                                            ToTensor(),\n                                            Lighting(0.1, __imagenet_pca[\n                                                'eigval'], __imagenet_pca['eigvec']),\n                                            ColorJitter(\n                                                brightness=0.4,\n                                                contrast=0.4,\n                                                saturation=0.4,\n                                            ),\n                                            Normalize(__imagenet_stats['mean'],\n                                                      __imagenet_stats['std'])\n                                        ]))\n\n    dataloader_training = DataLoader(transformed_training, batch_size,\n                                     shuffle=True, num_workers=4, pin_memory=False)\n\n    return dataloader_training\n\n\ndef getTestingData(batch_size=64):\n\n    __imagenet_stats = {'mean': [0.485, 0.456, 0.406],\n                        'std': [0.229, 0.224, 0.225]}\n    # scale = random.uniform(1, 1.5)\n    transformed_testing = depthDataset(csv_file='./data/nyu2_test.csv',\n                                       transform=transforms.Compose([\n                                           Scale(240),\n                                           CenterCrop([304, 228], [304, 228]),\n                                           Scale((224,224)),\n                                           ToTensor(is_test=True),\n                                           Normalize(__imagenet_stats['mean'],\n                                                     __imagenet_stats['std'])\n                                       ]))\n\n    dataloader_testing = DataLoader(transformed_testing, batch_size,\n                                    shuffle=False, num_workers=0, pin_memory=False)\n\n    return dataloader_testing\n\n", "demo.py": "# -*- coding: utf-8 -*-\n\n\nimport socket\nimport timeit\nfrom datetime import datetime\nimport os\nimport glob\nfrom collections import OrderedDict\nimport numpy as np\nimport yaml\nfrom addict import Dict\nimport argparse\n\n# PyTorch includes\nimport torch\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport torch.optim as optim\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.utils import make_grid\n\n# Tensorboard include\nfrom tensorboardX import SummaryWriter\n\n# Custom includes\nfrom dataloaders import cityscapes\nfrom dataloaders import utils\nfrom dataloaders import augmentation as augment\nfrom models.pdesnet import PDESNet\nfrom models import fast_mobilenetv1_aspp_prune\nfrom models.fastdepth import MobileNetSkipAdd\nfrom utils import loss as losses\nfrom utils import iou_eval\nimport util\n\nimport cv2\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nColors = np.array([[0,0,0], [0,255,255],[114, 255,   0], [255,   0, 105], [255, 131,   0],\n                  [  0, 255, 202], [  0, 255, 149], [219,   0, 255], [255,   0,   0], [219, 255,   0],\n                  [  0,  43, 255], [255,  79,   0], [114,   0, 255], [255,   0, 211], [255, 237,   0],\n                  [255,  26,   0], [ 87, 255,   0], [  0,  70, 255], [246,   0, 255], [  0, 255,  70],\n                  [  0, 255,  96], [255,   0, 131], [  0, 255,  17], [193,   0, 255], [255, 211,   0],\n                  [ 61,   0, 255], [255,   0, 158], [  0, 202, 255], [255,   0,  26], [167, 255,   0],\n                  [255,   0, 237], [  8, 255,   0], [ 61, 255,   0], [255, 105,   0], [255, 184,   0],\n                  [ 35,   0, 255], [  0, 175, 255], [  0, 255, 228], [255,  52,   0], [255,   0,  79],\n                  [140,   0, 255], [  0, 255, 175], [167,   0, 255], [193, 255,   0], [  0, 255, 123],\n                  [246, 255,   0], [  0,  96, 255], [  0, 255, 255], [  8,   0, 255], [ 35, 255,   0],\n                  [  0, 255,  43], [140, 255,   0], [255,   0, 184], [ 87,   0, 255], [  0,  17, 255], \n                  [  0, 228, 255], [255,   0,  52], [  0, 123, 255], [  0, 149, 255]])\n                  \n                  \ndef map_semantic_id_to_color(label):\n    #import ipdb;ipdb.set_trace()\n    return Colors[label]\n\n\n\n\nap = argparse.ArgumentParser()\nap.add_argument('--backbone_network', required=False,\n                help = 'name of backbone network',default='mobilenet')#shufflenet, mobilenet, and darknet\nap.add_argument('--model_path_resume', required=False,\n                help = 'path to a model to resume from',default='our_model/EPFL_checkpoint_mobilenet.pth.tar')\nap.add_argument('--steps-plot', type=int, default=1000)\nap.add_argument('--epochs-vis', type=int, default=1)\n\nargs = ap.parse_args()\nbackbone_network=args.backbone_network\nmodel_path_resume=args.model_path_resume\n\nNUM_CLASSES = 59\n\ntestBatch = 1  # Testing batch size\nnValInterval = 1  # Run on test set every nTestInterval epochs\nsnapshot = 1  # Store a model every snapshot epochs\n\n\nsave_dir_root = os.path.join(os.path.dirname(os.path.abspath(__file__)))\n\n\n\nwith torch.cuda.device(0): \n\n    torch.manual_seed(2020)\n    torch.cuda.manual_seed(2020)\n    np.random.seed(2020)\n    torch.backends.cudnn.benchmark = True\n    \n    CONFIG=Dict(yaml.load(open(\"config/training.yaml\")))    \n    \n    net= fast_mobilenetv1_aspp_prune.RT()#MobileNetSkipAdd()\n    torch.cuda.set_device(device=CONFIG.GPU_ID)\n    ckpt = torch.load(model_path_resume, map_location=lambda storage, loc: storage)\n    #ckpt = torch.load(model_path_resume)\n    net.load_state_dict(ckpt['state_dict'])\n    #net.load_state_dict(ckpt)\n    #load_state_dict(net.state_dict(), ckpt)\n    net.cuda()\n    \n    composed_transforms = transforms.Compose([\n        augment.Scale((240, 320), 1),\n        #augment.RandomHorizontalFlip(),\n        augment.ToTensor()])\n\n    dataset_path='./demoImages'#CONFIG.DATASET\n    CAD60_val = cityscapes.Cityscapes_test(root=dataset_path, transform=composed_transforms)\n    #CAD60_val = cityscapes.Cityscapes(root=dataset_path, split='Test', transform=composed_transforms)\n        \n    valloader = DataLoader(CAD60_val, batch_size=testBatch, shuffle=False, num_workers=4)\n    \n    num_img_vl = len(valloader)\n    net.eval()\n    image_dir = './Show/demoImages'\n    depth_dir = './Show/demoImages_people_depth'\n    if not os.path.exists(image_dir):\n        os.makedirs(image_dir)\n    if not os.path.exists(depth_dir):\n        os.makedirs(depth_dir)\n    idx = 0\n    for ii, sample_batched in enumerate(valloader):\n        print(ii+1,'/', num_img_vl)\n        inputs = sample_batched\n        inputs = Variable(inputs, requires_grad=True)\n        inputs = inputs.cuda()\n        with torch.no_grad():\n            output = net.forward(inputs)\n        semantics, predepths = output['class_mask'], output['depth']\n        #semantics, predepths = output\n        semantics = F.interpolate(semantics, scale_factor=2, mode='bilinear', align_corners=True)\n        predepths = F.interpolate(predepths, scale_factor=2, mode='bilinear', align_corners=True)\n        #import ipdb;ipdb.set_trace()\n        #predepths[depths==0] = 0\n        batchSize = inputs.size(0)\n        for i in range(batchSize):\n            #import ipdb;ipdb.set_trace()\n            idx += 1\n            image = inputs[i]\n            #gtdepth = depths.unsqueeze(1)[i]\n            predsemantic = semantics.max(1)[1].unsqueeze(1).data[i][0].cpu().numpy()\n            predsemantic[predsemantic!=1] = 0\n            \n            #gtsemantic = labels.data[i][0].cpu().numpy().astype(np.int)\n            #gtsemantic[gtsemantic!=1] = 0\n            #semanticShowGT = map_semantic_id_to_color(gtsemantic)\n            semanticShowPRE = map_semantic_id_to_color(predsemantic)\n            preddepth = predepths.unsqueeze(1)[i]\n    \n            #plt.imsave(os.path.join(image_dir, str(idx)+'_GTdepth' + '.png'), gtdepth.cpu().detach().numpy().squeeze(), cmap='rainbow')\n            plt.imsave(os.path.join(image_dir, str(idx)+'_PREdepth' + '.png' ), preddepth.cpu().detach().numpy().squeeze(), cmap='rainbow')\n            cv2.imwrite(os.path.join(image_dir, str(idx)+'_PREsemantic' + '.png'), semanticShowPRE)\n            #cv2.imwrite(os.path.join(image_dir, str(idx)+'_GTsemantic' + '.png'), semanticShowGT)\n            cv2.imwrite(os.path.join(image_dir, str(idx)+'_RGB' + '.png'), image.permute(1,2,0).cpu().detach().numpy().squeeze()*255)\n            \n            #preddepth[gtdepth==0]=0\n            #plt.imsave(os.path.join(image_dir, str(idx)+'_PREdepth_set0' + '.png' ), preddepth.cpu().detach().numpy().squeeze(), cmap='rainbow')\n            \n            showdepthPRE = cv2.imread(os.path.join(image_dir, str(idx)+'_PREdepth' + '.png' ))\n            showdepthPRE[predsemantic!=1]=0\n            cv2.imwrite(os.path.join(depth_dir, str(idx)+'_PRE_semantic_depth' + '.png'), showdepthPRE)", "training.yaml": "DATASET: /export/diskpool/yangmei/EPFL\n\nBACKEND_NETWORK: MOBILENET\n\nUSING_GPU: TRUE\nGPU_ID: 2\n\n\n", "cityscapes.py": "#!/usr/bin/env python3\n\"\"\"\nCreated on Sat Oct 20 00:18:43 2018\n\n@author: Taha Emara  @email: taha@emaraic.com\n\"\"\"\n\nimport os\nimport numpy as np\nfrom PIL import Image\nfrom torch.utils import data\nimport torch\nfrom dataloaders.utils import listFiles\n\nclass Cityscapes(data.Dataset):\n\n    def __init__(self, root='path/to/datasets/cityscapes', split=\"Train\", transform=None):\n        \"\"\"\n        Cityscapes dataset folder has two folders, 'leftImg8bit' folder for images and 'gtFine_trainvaltest' \n        folder for annotated images with fine annotations 'labels'.\n        \"\"\"\n        self.root = root\n        self.split = split #train, validation, and test sets\n        self.transform = transform\n        self.files = {}\n        self.n_classes = 59\n\n        print(\"Using CAD dataset\")\n        self.images_path = os.path.join(self.root, self.split, 'RGB')\n        self.labels_path = os.path.join(self.root, self.split, 'Semantic_label')\n        self.depths_path = os.path.join(self.root, self.split, 'Depth')\n          \n            \n        #print(self.images_path)\n        self.files[split] = listFiles(rootdir=self.images_path, suffix='.png')#list of the pathes to images\n\n        self.void_classes = [0, 1, 2, 3, 4, 5, 6, 9, 10, 14, 15, 16, 18, 29, 30, -1] #not to train\n        self.valid_classes = [7, 8, 11, 12, 13, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 31, 32, 33]\n        self.class_names = ['road', 'sidewalk', 'building', 'wall', 'fence', \\\n                            'pole', 'traffic_light', 'traffic_sign', 'vegetation', 'terrain', \\\n                            'sky', 'person', 'rider', 'car', 'truck', 'bus', 'train', \\\n                            'motorcycle', 'bicycle']\n        \n        self.ignore_index = 255\n        self.class_map = dict(zip(self.valid_classes, range(self.n_classes)))\n        #print(self.class_map)\n        \n        if not self.files[split]:\n            raise Exception(\"No files for split=[%s] found in %s\" % (split, self.images.path))\n\n        print(\"Found %d %s images\" % (len(self.files[split]), split))\n        #import ipdb;ipdb.set_trace()\n        \n    \n    def __len__(self):\n        return len(self.files[self.split])\n    \n    def __getitem__(self, index):\n        image_path = self.files[self.split][index].rstrip()\n        #print(image_path)\n        label_path = os.path.join(self.labels_path,os.path.basename(image_path))\n        depth_path = os.path.join(self.depths_path,os.path.basename(image_path))\n                            \n        _img = Image.open(image_path).convert('RGB')\n        _tmp = np.array(Image.open(label_path))\n        _tmp = self.map(_tmp)\n\n        _target = Image.fromarray(_tmp)\n\n        _depth = Image.open(depth_path)\n        \n        sample = {'image': _img, 'label': _target, 'depth': _depth}\n\n        if self.transform:\n            sample = self.transform(sample)\n        return sample\n    \n    def encode_segmap(self, mask):\n        # Put all void classes to ignore_index\n        for _voidc in self.void_classes:\n            mask[mask == _voidc] = self.ignore_index\n        for _validc in self.valid_classes:\n            mask[mask == _validc] = self.class_map[_validc]\n        return mask\n    \n    def map(self, mask):\n        h,w = mask.shape\n        map_ids = np.array([0,1,-1,2,-1,-1,-1,-1,-1,-1,-1,3,-1,-1,4,5,6,7,-1,-1,-1,-1,-1,-1,-1,8,9,10,11,12,13,\n                       14,-1,15,16,17,18,19,-1,20,21,22,23,-1,24,25,26,27,28,29,30,31,-1,32,33,34,35,36,37,\n                       38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,-1,56,57,58,-1])\n        mask = map_ids[mask.reshape(-1)].reshape((h,w))\n        return mask.astype(np.uint8)\n\n\nclass Cityscapes_test(data.Dataset):\n\n    def __init__(self, root='path/to/datasets/cityscapes', transform=None):\n        \"\"\"\n        Cityscapes dataset folder has two folders, 'leftImg8bit' folder for images and 'gtFine_trainvaltest' \n        folder for annotated images with fine annotations 'labels'.\n        \"\"\"\n        self.root = root\n        self.transform = transform\n        self.files = {}\n        self.n_classes = 59\n\n        print(\"Using dataset\")\n          \n            \n        #print(self.images_path)\n        self.files = listFiles(rootdir=self.root, suffix='.png')#list of the pathes to images\n\n        \n        self.ignore_index = 255\n        \n    \n    def __len__(self):\n        return len(self.files)\n    \n    def __getitem__(self, index):\n        image_path = self.files[index].rstrip()\n\n        _img = Image.open(image_path).convert('RGB')\n        _img = _img.resize((320, 240), Image.BILINEAR)\n        _img = np.array(_img).astype(np.float32).transpose((2, 0, 1))\n        _img = torch.from_numpy(_img).float().div(255)\n        \n        return _img\n\n\n", "augmentation.py": "import torch\nimport math\nimport numbers\nimport random\nimport numpy as np\n\nfrom PIL import Image, ImageOps\n\nclass RandomCrop(object):\n    def __init__(self, size, padding=0):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size # h, w\n        self.padding = padding\n\n    def __call__(self, sample):\n        img, mask = sample['image'], sample['label']\n\n        if self.padding > 0:\n            img = ImageOps.expand(img, border=self.padding, fill=255)\n            mask = ImageOps.expand(mask, border=self.padding, fill=255)\n\n        assert img.size == mask.size\n        w, h = img.size\n        th, tw = self.size # target size\n        if w == tw and h == th:\n            return {'image': img,\n                    'label': mask}\n        if w < tw or h < th:\n            img = img.resize((tw, th), Image.BILINEAR)\n            mask = mask.resize((tw, th), Image.NEAREST)\n            return {'image': img,\n                    'label': mask}\n\n        x1 = random.randint(0, w - tw)\n        y1 = random.randint(0, h - th)\n        img = img.crop((x1, y1, x1 + tw, y1 + th))\n        mask = mask.crop((x1, y1, x1 + tw, y1 + th))\n\n        return {'image': img,\n                'label': mask}\n\n\nclass CenterCrop(object):\n    def __init__(self, size):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n\n    def __call__(self, sample):\n        img = sample['image']\n        mask = sample['label']\n        assert img.size == mask.size\n        w, h = img.size\n        th, tw = self.size\n        x1 = int(round((w - tw) / 2.))\n        y1 = int(round((h - th) / 2.))\n        img = img.crop((x1, y1, x1 + tw, y1 + th))\n        mask = mask.crop((x1, y1, x1 + tw, y1 + th))\n\n        return {'image': img,\n                'label': mask}\n\n\nclass RandomHorizontalFlip(object):\n    def __call__(self, sample):\n        img = sample['image']\n        mask = sample['label']\n        depth = sample['depth']\n        if random.random() < 0.5:\n            img = img.transpose(Image.FLIP_LEFT_RIGHT)\n            mask = mask.transpose(Image.FLIP_LEFT_RIGHT)\n            depth = depth.transpose(Image.FLIP_LEFT_RIGHT)\n\n        return {'image': img,\n                'label': mask,\n                'depth': depth}\n\n\nclass Normalize(object):\n    \"\"\"Normalize a tensor image with mean and standard deviation.\n    Args:\n        mean (tuple): means for each channel.\n        std (tuple): standard deviations for each channel.\n    \"\"\"\n    def __init__(self, mean=(0., 0., 0.), std=(1., 1., 1.)):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, sample):\n        img = np.array(sample['image']).astype(np.float32)\n        mask = np.array(sample['label']).astype(np.float32)\n        img /= 255.0\n        img -= self.mean\n        img /= self.std\n\n        return {'image': img,\n                'label': mask}\n\n\nclass Normalize_cityscapes(object):\n    \"\"\"Normalize a tensor image with mean and standard deviation.\n    Args:\n        mean (tuple): means for each channel.\n        std (tuple): standard deviations for each channel.\n    \"\"\"\n    def __init__(self, mean=(0., 0., 0.)):\n        self.mean = mean\n\n    def __call__(self, sample):\n        img = np.array(sample['image']).astype(np.float32)\n        mask = np.array(sample['label']).astype(np.float32)\n        img -= self.mean\n        img /= 255.0\n\n        return {'image': img,\n                'label': mask}\n\nclass ToTensor(object):\n    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n\n    def __call__(self, sample):\n        # swap color axis because\n        # numpy image: H x W x C\n        # torch image: C X H X W\n        img = np.array(sample['image']).astype(np.float32).transpose((2, 0, 1))\n        mask = np.expand_dims(np.array(sample['label']).astype(np.float32), -1).transpose((2, 0, 1))\n        depth = np.expand_dims(np.array(sample['depth']).astype(np.float32), -1).transpose((2, 0, 1))\n#        mask[mask == 255] = 19\n\n        img = torch.from_numpy(img).float().div(255)\n        mask = torch.from_numpy(mask).float()\n        depth = torch.from_numpy(depth).float().div(1000)\n#        print(\"MAsk from Custom transforsms\"+str(mask.size()))\n\n        return {'image': img,\n                'label': mask,\n                'depth': depth}\n\n\nclass FixedResize(object):\n    def __init__(self, size):\n        self.size = tuple(reversed(size))  # size: (h, w)\n\n    def __call__(self, sample):\n        img = sample['image']\n        mask = sample['label']\n\n        assert img.size == mask.size\n\n        img = img.resize(self.size, Image.BILINEAR)\n        mask = mask.resize(self.size, Image.NEAREST)\n\n        return {'image': img,\n                'label': mask}\n\n\nclass Scale(object):\n    def __init__(self, size, scaleIn):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n        self.scaleIn = scaleIn\n\n    def __call__(self, sample):\n        img = sample['image']\n        mask = sample['label']\n        depth = sample['depth']\n        assert img.size == mask.size == depth.size\n        w, h = img.size\n\n        if (w >= h and w == self.size[1]) or (h >= w and h == self.size[0]):\n            mask = mask.resize((math.ceil(w/self.scaleIn), math.ceil(h/self.scaleIn)), Image.NEAREST)\n            depth = depth.resize((math.ceil(w/self.scaleIn), math.ceil(h/self.scaleIn)), Image.NEAREST)\n            return {'image': img,\n                    'label': mask,\n                    'depth': depth}\n        oh, ow = self.size\n        img = img.resize((ow, oh), Image.BILINEAR)\n        mask = mask.resize((math.ceil(ow/self.scaleIn), math.ceil(oh/self.scaleIn)), Image.NEAREST)\n        depth = depth.resize((math.ceil(ow/self.scaleIn), math.ceil(oh/self.scaleIn)), Image.NEAREST)\n\n        return {'image': img,\n                'label': mask,\n                'depth': depth}\n\n\nclass RandomSizedCrop(object):\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, sample):\n        img = sample['image']\n        mask = sample['label']\n        assert img.size == mask.size\n        for attempt in range(10):\n            area = img.size[0] * img.size[1]\n            target_area = random.uniform(0.45, 1.0) * area\n            aspect_ratio = random.uniform(0.5, 2)\n\n            w = int(round(math.sqrt(target_area * aspect_ratio)))\n            h = int(round(math.sqrt(target_area / aspect_ratio)))\n\n            if random.random() < 0.5:\n                w, h = h, w\n\n            if w <= img.size[0] and h <= img.size[1]:\n                x1 = random.randint(0, img.size[0] - w)\n                y1 = random.randint(0, img.size[1] - h)\n\n                img = img.crop((x1, y1, x1 + w, y1 + h))\n                mask = mask.crop((x1, y1, x1 + w, y1 + h))\n                assert (img.size == (w, h))\n\n                img = img.resize((self.size, self.size), Image.BILINEAR)\n                mask = mask.resize((self.size, self.size), Image.NEAREST)\n\n                return {'image': img,\n                        'label': mask}\n\n        # Fallback\n        scale = Scale(self.size)\n        crop = CenterCrop(self.size)\n        sample = crop(scale(sample))\n        return sample\n\n\nclass RandomRotate(object):\n    def __init__(self, degree):\n        self.degree = degree\n\n    def __call__(self, sample):\n        img = sample['image']\n        mask = sample['label']\n        rotate_degree = random.random() * 2 * self.degree - self.degree\n        img = img.rotate(rotate_degree, Image.BILINEAR)\n        #mask = mask.rotate(rotate_degree, Image.NEAREST)\n        im2 = mask.convert('RGBA')\n        # rotated image\n        rot = im2.rotate(rotate_degree, Image.NEAREST)\n        # a white image same size as rotated image\n        fff = Image.new('RGBA', rot.size, (255,)*4)\n        # create a composite image using the alpha layer of rot as a mask\n        out = Image.composite(rot, fff, rot)\n        # save your work (converting back to mode='1' or whatever..)\n        out=out.convert(mask.mode)\n\n        return {'image': img,\n                'label': out}\n\n\nclass RandomSized(object):\n    def __init__(self, size):\n        self.size = size\n        self.scale = Scale(self.size)\n        self.crop = RandomCrop(self.size)\n\n    def __call__(self, sample):\n        img = sample['image']\n        mask = sample['label']\n        assert img.size == mask.size\n\n        w = int(random.uniform(0.8, 2.5) * img.size[0])\n        h = int(random.uniform(0.8, 2.5) * img.size[1])\n\n        img, mask = img.resize((w, h), Image.BILINEAR), mask.resize((w, h), Image.NEAREST)\n        sample = {'image': img, 'label': mask}\n\n        return self.crop(self.scale(sample))\nclass RandomScaleCrop(object):\n    def __init__(self, base_size, crop_size, fill=255):\n        self.base_size = base_size\n        self.crop_size = crop_size\n        self.fill = fill\n\n    def __call__(self, sample):\n        img = sample['image']\n        mask = sample['label']\n        # random scale (short edge)\n        short_size = random.randint(int(self.base_size * 0.5), int(self.base_size * 2.0))\n        w, h = img.size\n        if h > w:\n            ow = short_size\n            oh = int(1.0 * h * ow / w)\n        else:\n            oh = short_size\n            ow = int(1.0 * w * oh / h)\n        img = img.resize((ow, oh), Image.BILINEAR)\n        mask = mask.resize((ow, oh), Image.NEAREST)\n        # pad crop\n        if short_size < self.crop_size:\n            padh = self.crop_size - oh if oh < self.crop_size else 0\n            padw = self.crop_size - ow if ow < self.crop_size else 0\n            img = ImageOps.expand(img, border=(0, 0, padw, padh), fill=0)\n            mask = ImageOps.expand(mask, border=(0, 0, padw, padh), fill=self.fill)\n        # random crop crop_size\n        w, h = img.size\n        x1 = random.randint(0, w - self.crop_size)\n        y1 = random.randint(0, h - self.crop_size)\n        img = img.crop((x1, y1, x1 + self.crop_size, y1 + self.crop_size))\n        mask = mask.crop((x1, y1, x1 + self.crop_size, y1 + self.crop_size))\n\n        return {'image': img,\n                'label': mask}\n        \nclass RandomScale(object):\n    def __init__(self, limit):\n        self.limit = limit\n\n    def __call__(self, sample):\n        img = sample['image']\n        mask = sample['label']\n        assert img.size == mask.size\n\n        scale = random.uniform(self.limit[0], self.limit[1])\n        w = int(scale * img.size[0])\n        h = int(scale * img.size[1])\n\n        img, mask = img.resize((w, h), Image.BILINEAR), mask.resize((w, h), Image.NEAREST)\n\n        return {'image': img, 'label': mask}", "utils.py": "import os\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef listFiles(rootdir='.', suffix='png'):\n    \"\"\"Performs recursive glob with given suffix and rootdir\n        :param rootdir is the root directory\n        :param suffix is the suffix to be searched as PNG or JPG\n    \"\"\"\n    return [os.path.join(looproot, filename)\n        for looproot, _, filenames in os.walk(rootdir)\n        for filename in filenames if filename.endswith(suffix) and 'CAD120' in filename]\n\ncityscapes_valid_classes = [7, 8, 11, 12, 13, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 31, 32, 33]\nclass_map = dict(zip( range(19),cityscapes_valid_classes))\n\ndef convertTrainIdToClassId(img):\n         temp=np.copy(img)\n         for trainID in range(19):\n             #print(trainID,\" \" ,class_map[trainID])\n             temp[img==trainID]=class_map[trainID]\n             \n         return temp\n\n\ndef get_cityscapes_labels():\n    return np.array([\n         #[  0,   0,   0],\n        [128, 64, 128],\n        [244, 35, 232],\n        [70, 70, 70],\n        [102, 102, 156],\n        [190, 153, 153],\n        [153, 153, 153],\n        [250, 170, 30],\n        [220, 220, 0],\n        [107, 142, 35],\n        [152, 251, 152],\n        [0, 130, 180],\n        [220, 20, 60],\n        [255, 0, 0],\n        [0, 0, 142],\n        [0, 0, 70],\n        [0, 60, 100],\n        [0, 80, 100],\n        [0, 0, 230],\n        [119, 11, 32]\n        ])\n\n\n\ndef encode_segmap(mask):\n    \"\"\"Encode segmentation label images as pascal classes\n    Args:\n        mask (np.ndarray): raw segmentation label image of dimension\n          (M, N, 3), in which the Pascal classes are encoded as colours.\n    Returns:\n        (np.ndarray): class map with dimensions (M,N), where the value at\n        a given location is the integer denoting the class index.\n    \"\"\"\n    mask = mask.astype(int)\n    label_mask = np.zeros((mask.shape[0], mask.shape[1]), dtype=np.int16)\n    for ii, label in enumerate(get_cityscapes_labels()):#get_pascal_labels()\n        label_mask[np.where(np.all(mask == label, axis=-1))[:2]] = ii\n    label_mask = label_mask.astype(int)\n    return label_mask\n\n\ndef decode_seg_map_sequence(label_masks, dataset='pascal'):\n    rgb_masks = []\n    for label_mask in label_masks:\n        rgb_mask = decode_segmap(label_mask, dataset)\n        rgb_masks.append(rgb_mask)\n    rgb_masks = torch.from_numpy(np.array(rgb_masks).transpose([0, 3, 1, 2]))\n    return rgb_masks\n\ndef decode_segmap(label_mask, dataset, plot=False):\n    \"\"\"Decode segmentation class labels into a color image\n    Args:\n        label_mask (np.ndarray): an (M,N) array of integer values denoting\n          the class label at each spatial location.\n        plot (bool, optional): whether to show the resulting color image\n          in a figure.\n    Returns:\n        (np.ndarray, optional): the resulting decoded color image.\n    \"\"\"\n    if dataset == 'pascal':\n      print()\n    elif dataset == 'cityscapes':\n        n_classes = 19\n        label_colours = get_cityscapes_labels()\n    else:\n        raise NotImplementedError\n\n    r = label_mask.copy()\n    g = label_mask.copy()\n    b = label_mask.copy()\n    for ll in range(0, n_classes):\n        r[label_mask == ll] = label_colours[ll, 0]\n        g[label_mask == ll] = label_colours[ll, 1]\n        b[label_mask == ll] = label_colours[ll, 2]\n    \n    r[label_mask == 255] = 0\n    g[label_mask == 255] = 0\n    b[label_mask == 255] =0\n    \n    rgb = np.zeros((label_mask.shape[0], label_mask.shape[1], 3))\n   # rgb = np.zeros((label_mask.shape[1], label_mask.shape[2], 3))\n    #replace blue with red as opencv uses bgr\n    rgb[:, :, 0] = r /255.0     \n    rgb[:, :, 1] = g /255.0\n    rgb[:, :, 2] = b /255.0\n#    \n#    rgb = np.zeros((label_mask.shape[1], label_mask.shape[2], 3))\n#    #replace blue with red as opencv uses bgr\n#    rgb[:, :, 0] = b #/255.0     \n#    rgb[:, :, 1] = g #/255.0\n#    rgb[:, :, 2] = r #/255.0\n#    \n    if plot:\n        plt.imshow(rgb)\n        plt.show()\n    else:\n        return rgb\ndef decode_segmap_cv(label_mask, dataset, plot=False):\n    \"\"\"Decode segmentation class labels into a color image\n    Args:\n        label_mask (np.ndarray): an (M,N) array of integer values denoting\n          the class label at each spatial location.\n        plot (bool, optional): whether to show the resulting color image\n          in a figure.\n    Returns:\n        (np.ndarray, optional): the resulting decoded color image.\n    \"\"\"\n    if dataset == 'pascal':\n      print()\n    elif dataset == 'cityscapes':\n        n_classes = 19\n        label_colours = get_cityscapes_labels()\n    else:\n        raise NotImplementedError\n\n    r = label_mask.copy()\n    g = label_mask.copy()\n    b = label_mask.copy()\n    for ll in range(0, n_classes):\n        r[label_mask == ll] = label_colours[ll, 0]\n        g[label_mask == ll] = label_colours[ll, 1]\n        b[label_mask == ll] = label_colours[ll, 2]\n    \n    r[label_mask == 255] = 0\n    g[label_mask == 255] = 0\n    b[label_mask == 255] =0\n    \n#    rgb = np.zeros((label_mask.shape[0], label_mask.shape[1], 3))\n#   # rgb = np.zeros((label_mask.shape[1], label_mask.shape[2], 3))\n#    #replace blue with red as opencv uses bgr\n#    rgb[:, :, 0] = r /255.0     \n#    rgb[:, :, 1] = g /255.0\n#    rgb[:, :, 2] = b /255.0\n#    \n    rgb = np.zeros((label_mask.shape[1], label_mask.shape[2], 3))\n    #replace blue with red as opencv uses bgr\n    rgb[:, :, 0] = b #/255.0     \n    rgb[:, :, 1] = g #/255.0\n    rgb[:, :, 2] = r #/255.0\n#    \n    if plot:\n        plt.imshow(rgb)\n        plt.show()\n    else:\n        return rgb\ndef generate_param_report(logfile, param):\n    log_file = open(logfile, 'w')\n    for key, val in param.items():\n        log_file.write(key + ':' + str(val) + '\\n')\n    log_file.close()\n\n\ndef lr_poly(base_lr, iter_, max_iter=100, power=0.9):\n    return base_lr * ((1 - float(iter_) / max_iter) ** power)\n\n\n    \nfrom torchvision import transforms \n\nif __name__ == '__main__':\n    print()\n    ar=np.array([[0,7,10],[7,3,6]])\n    z=convertTrainIdToClassId(ar)\n#    img3= transforms.ToPILImage()(torch.from_numpy(ou).type(torch.FloatTensor))#.detach().cpu()\n#    img3.save(oupath)\n    print(z)\n    ", "loss.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Wed Oct 31 17:52:22 2018\n\n@author: Taha Emara  @email: taha@emaraic.com\n\"\"\"\nimport torch.nn as nn\n\ndef cross_entropy2d(logit, target, ignore_index=255, weight=None, reduct='mean'):\n    n, c, h, w = logit.size()\n    #print(\"Inside Cross entropy \",logit.size()  ,\"Target Size\",target.size())\n    # logit = logit.permute(0, 2, 3, 1)\n    target = target.squeeze(1)\n    if weight is None:\n        criterion = nn.CrossEntropyLoss(ignore_index=ignore_index,reduction=reduct)\n    else:\n        criterion = nn.CrossEntropyLoss(weight=weight, ignore_index=ignore_index,reduction=reduct)\n    loss = criterion(logit, target.long())\n\n    #batch_average=True\n    #if batch_average:\n    #    loss /= n\n\n    return loss\n", "flops_counter.py": "import torch.nn as nn\nimport torch\nimport numpy as np\n\ndef flops_to_string(flops):\n    if flops // 10**9 > 0:\n        return str(round(flops / 10.**9, 2)) + 'GMac'\n    elif flops // 10**6 > 0:\n        return str(round(flops / 10.**6, 2)) + 'MMac'\n    elif flops // 10**3 > 0:\n        return str(round(flops / 10.**3, 2)) + 'KMac'\n    return str(flops) + 'Mac'\n\ndef get_model_parameters_number(model, as_string=True):\n    params_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    if not as_string:\n        return params_num\n\n    if params_num // 10 ** 6 > 0:\n        return str(round(params_num / 10 ** 6, 2)) + 'M'\n    elif params_num // 10 ** 3:\n        return str(round(params_num / 10 ** 3, 2)) + 'k'\n\n    return str(params_num)\n\ndef add_flops_counting_methods(net_main_module):\n    # adding additional methods to the existing module object,\n    # this is done this way so that each function has access to self object\n    net_main_module.start_flops_count = start_flops_count.__get__(net_main_module)\n    net_main_module.stop_flops_count = stop_flops_count.__get__(net_main_module)\n    net_main_module.reset_flops_count = reset_flops_count.__get__(net_main_module)\n    net_main_module.compute_average_flops_cost = compute_average_flops_cost.__get__(net_main_module)\n\n    net_main_module.reset_flops_count()\n\n    # Adding variables necessary for masked flops computation\n    net_main_module.apply(add_flops_mask_variable_or_reset)\n\n    return net_main_module\n\n\ndef compute_average_flops_cost(self):\n    \"\"\"\n    A method that will be available after add_flops_counting_methods() is called\n    on a desired net object.\n\n    Returns current mean flops consumption per image.\n\n    \"\"\"\n\n    batches_count = self.__batch_counter__\n    flops_sum = 0\n    for module in self.modules():\n        if is_supported_instance(module):\n            flops_sum += module.__flops__\n\n    return flops_sum / batches_count\n\n\ndef start_flops_count(self):\n    \"\"\"\n    A method that will be available after add_flops_counting_methods() is called\n    on a desired net object.\n\n    Activates the computation of mean flops consumption per image.\n    Call it before you run the network.\n\n    \"\"\"\n    add_batch_counter_hook_function(self)\n    self.apply(add_flops_counter_hook_function)\n\n\ndef stop_flops_count(self):\n    \"\"\"\n    A method that will be available after add_flops_counting_methods() is called\n    on a desired net object.\n\n    Stops computing the mean flops consumption per image.\n    Call whenever you want to pause the computation.\n\n    \"\"\"\n    remove_batch_counter_hook_function(self)\n    self.apply(remove_flops_counter_hook_function)\n\n\ndef reset_flops_count(self):\n    \"\"\"\n    A method that will be available after add_flops_counting_methods() is called\n    on a desired net object.\n\n    Resets statistics computed so far.\n\n    \"\"\"\n    add_batch_counter_variables_or_reset(self)\n    self.apply(add_flops_counter_variable_or_reset)\n\n\ndef add_flops_mask(module, mask):\n    def add_flops_mask_func(module):\n        if isinstance(module, torch.nn.Conv2d):\n            module.__mask__ = mask\n    module.apply(add_flops_mask_func)\n\n\ndef remove_flops_mask(module):\n    module.apply(add_flops_mask_variable_or_reset)\n\n\n# ---- Internal functions\ndef is_supported_instance(module):\n    if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.ReLU) \\\n       or isinstance(module, torch.nn.PReLU) or isinstance(module, torch.nn.ELU) \\\n       or isinstance(module, torch.nn.LeakyReLU) or isinstance(module, torch.nn.ReLU6) \\\n       or isinstance(module, torch.nn.Linear) or isinstance(module, torch.nn.MaxPool2d) \\\n       or isinstance(module, torch.nn.AvgPool2d) or isinstance(module, torch.nn.BatchNorm2d) \\\n       or isinstance(module, torch.nn.Upsample):\n        return True\n\n    return False\n\n\ndef empty_flops_counter_hook(module, input, output):\n    module.__flops__ += 0\n\n\ndef upsample_flops_counter_hook(module, input, output):\n    output_size = output[0]\n    batch_size = output_size.shape[0]\n    output_elements_count = batch_size\n    for val in output_size.shape[1:]:\n        output_elements_count *= val\n    module.__flops__ += output_elements_count\n\n\ndef relu_flops_counter_hook(module, input, output):\n    input = input[0]\n    batch_size = input.shape[0]\n    active_elements_count = batch_size\n    for val in input.shape[1:]:\n        active_elements_count *= val\n\n    module.__flops__ += active_elements_count\n\n\ndef linear_flops_counter_hook(module, input, output):\n    input = input[0]\n    batch_size = input.shape[0]\n    module.__flops__ += batch_size * input.shape[1] * output.shape[1]\n\n\ndef pool_flops_counter_hook(module, input, output):\n    input = input[0]\n    module.__flops__ += np.prod(input.shape)\n\ndef bn_flops_counter_hook(module, input, output):\n    module.affine\n    input = input[0]\n\n    batch_flops = np.prod(input.shape)\n    if module.affine:\n        batch_flops *= 2\n    module.__flops__ += batch_flops\n\ndef conv_flops_counter_hook(conv_module, input, output):\n    # Can have multiple inputs, getting the first one\n    input = input[0]\n\n    batch_size = input.shape[0]\n    output_height, output_width = output.shape[2:]\n\n    kernel_height, kernel_width = conv_module.kernel_size\n    in_channels = conv_module.in_channels\n    out_channels = conv_module.out_channels\n    groups = conv_module.groups\n\n    filters_per_channel = out_channels // groups\n    conv_per_position_flops = (kernel_height * kernel_width+((kernel_height * kernel_width)-1))* in_channels * filters_per_channel\n\n    active_elements_count = batch_size * output_height * output_width\n\n    if conv_module.__mask__ is not None:\n        # (b, 1, h, w)\n        flops_mask = conv_module.__mask__.expand(batch_size, 1, output_height, output_width)\n        active_elements_count = flops_mask.sum()\n\n    overall_conv_flops = conv_per_position_flops * active_elements_count\n\n    bias_flops = 0\n\n    if conv_module.bias is not None:\n\n        bias_flops = out_channels * active_elements_count\n\n    overall_flops = overall_conv_flops + bias_flops\n\n    conv_module.__flops__ += overall_flops\n\n\ndef batch_counter_hook(module, input, output):\n    # Can have multiple inputs, getting the first one\n    input = input[0]\n    batch_size = input.shape[0]\n    module.__batch_counter__ += batch_size\n\n\ndef add_batch_counter_variables_or_reset(module):\n\n    module.__batch_counter__ = 0\n\n\ndef add_batch_counter_hook_function(module):\n    if hasattr(module, '__batch_counter_handle__'):\n        return\n\n    handle = module.register_forward_hook(batch_counter_hook)\n    module.__batch_counter_handle__ = handle\n\n\ndef remove_batch_counter_hook_function(module):\n    if hasattr(module, '__batch_counter_handle__'):\n        module.__batch_counter_handle__.remove()\n        del module.__batch_counter_handle__\n\n\ndef add_flops_counter_variable_or_reset(module):\n    if is_supported_instance(module):\n        module.__flops__ = 0\n\n\ndef add_flops_counter_hook_function(module):\n    if is_supported_instance(module):\n        if hasattr(module, '__flops_handle__'):\n            return\n\n        if isinstance(module, torch.nn.Conv2d):\n            handle = module.register_forward_hook(conv_flops_counter_hook)\n        elif isinstance(module, torch.nn.ReLU) or isinstance(module, torch.nn.PReLU) \\\n             or isinstance(module, torch.nn.ELU) or isinstance(module, torch.nn.LeakyReLU) \\\n             or isinstance(module, torch.nn.ReLU6):\n            handle = module.register_forward_hook(relu_flops_counter_hook)\n        elif isinstance(module, torch.nn.Linear):\n            handle = module.register_forward_hook(linear_flops_counter_hook)\n        elif isinstance(module, torch.nn.AvgPool2d) or isinstance(module, torch.nn.MaxPool2d):\n            handle = module.register_forward_hook(pool_flops_counter_hook)\n        elif isinstance(module, torch.nn.BatchNorm2d):\n            handle = module.register_forward_hook(bn_flops_counter_hook)\n        elif isinstance(module, torch.nn.Upsample):\n            handle = module.register_forward_hook(upsample_flops_counter_hook)\n        else:\n            handle = module.register_forward_hook(empty_flops_counter_hook)\n        module.__flops_handle__ = handle\n\n\ndef remove_flops_counter_hook_function(module):\n    if is_supported_instance(module):\n        if hasattr(module, '__flops_handle__'):\n            module.__flops_handle__.remove()\n            del module.__flops_handle__\n# --- Masked flops counting\n\n\n# Also being run in the initialization\ndef add_flops_mask_variable_or_reset(module):\n    if is_supported_instance(module):\n        module.__mask__ = None\n\n# --- Masked flops counting\n\n\n# Also being run in the initialization\ndef add_flops_mask_variable_or_reset(module):\n    if is_supported_instance(module):\n        module.__mask__ = None\ndef flops_to_string(flops):\n    if flops // 10**9 > 0:\n        return str(round(flops / 10.**9, 2)) + 'GMac'\n    elif flops // 10**6 > 0:\n        return str(round(flops / 10.**6, 2)) + 'MMac'\n    elif flops // 10**3 > 0:\n        return str(round(flops / 10.**3, 2)) + 'KMac'\n    return str(flops) + 'Mac'\n\ndef get_model_parameters_number(model, as_string=True):\n    params_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    if not as_string:\n        return params_num\n\n    if params_num // 10 ** 6 > 0:\n        return str(round(params_num / 10 ** 6, 2)) + 'M'\n    elif params_num // 10 ** 3:\n        return str(round(params_num / 10 ** 3, 2)) + 'k'\n\n    return str(params_num)\n", "iou_eval.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Wed Oct 31 18:28:07 2018\n\n@author: Taha Emara  @email: taha@emaraic.com\n\"\"\"\n\nimport torch \n\nclass Eval:\n\n    def __init__(self, nClasses, ignoreIndex=19):\n        self.nClasses = nClasses\n        self.ignoreIndex = ignoreIndex if nClasses>ignoreIndex else -1 #if ignoreIndex is larger than nClasses, consider no ignoreIndex\n        self.reset()\n\n    def reset (self):\n        classes = self.nClasses if self.ignoreIndex==-1 else self.nClasses-1\n        self.tp = torch.zeros(classes).double()\n        self.fp = torch.zeros(classes).double()\n        self.fn = torch.zeros(classes).double()        \n\n    def addBatch(self, x, y):   #x=preds, y=targets\n        #sizes should be \"batch_size x nClasses x H x W\"\n        \n        #print (\"X is cuda: \", x.is_cuda)\n        #print (\"Y is cuda: \", y.is_cuda)\n        x=x.type( torch.LongTensor)\n        y=y.type( torch.LongTensor)\n        if (x.is_cuda or y.is_cuda):\n            x = x.cuda()\n            y = y.cuda()\n\n        #if size is \"batch_size x 1 x H x W\" scatter to onehot\n        if (x.size(1) == 1):\n            x_onehot = torch.zeros(x.size(0), self.nClasses, x.size(2), x.size(3))  \n            if x.is_cuda:\n                x_onehot = x_onehot.cuda()\n            x_onehot.scatter_(1, x, 1).float()\n        else:\n            x_onehot = x.float()\n\n        if (y.size(1) == 1):\n            y_onehot = torch.zeros(y.size(0), self.nClasses, y.size(2), y.size(3))\n            if y.is_cuda:\n                y_onehot = y_onehot.cuda()\n            y_onehot.scatter_(1, y, 1).float()\n        else:\n            y_onehot = y.float()\n\n        if (self.ignoreIndex != -1): \n            ignores = y_onehot[:,self.ignoreIndex].unsqueeze(1)\n            x_onehot = x_onehot[:, :self.ignoreIndex]\n            y_onehot = y_onehot[:, :self.ignoreIndex]\n        else:\n            ignores=0\n\n        #print(type(x_onehot))\n        #print(type(y_onehot))\n        #print(x_onehot.size())\n        #print(y_onehot.size())\n\n        tpmult = x_onehot * y_onehot    #times prediction and gt coincide is 1\n        tp = torch.sum(torch.sum(torch.sum(tpmult, dim=0, keepdim=True), dim=2, keepdim=True), dim=3, keepdim=True).squeeze()\n        fpmult = x_onehot * (1-y_onehot-ignores) #times prediction says its that class and gt says its not (subtracting cases when its ignore label!)\n        fp = torch.sum(torch.sum(torch.sum(fpmult, dim=0, keepdim=True), dim=2, keepdim=True), dim=3, keepdim=True).squeeze()\n        fnmult = (1-x_onehot) * (y_onehot) #times prediction says its not that class and gt says it is\n        fn = torch.sum(torch.sum(torch.sum(fnmult, dim=0, keepdim=True), dim=2, keepdim=True), dim=3, keepdim=True).squeeze() \n\n        self.tp += tp.double().cpu()\n        self.fp += fp.double().cpu()\n        self.fn += fn.double().cpu()\n\n    def getIoU(self):\n        num = self.tp\n        den = self.tp + self.fp + self.fn + 1e-15\n        iou = num / den\n        return torch.mean(iou), iou #returns \"iou m\n\nclass colors:\n    RED       = '\\033[31;1m'\n    GREEN     = '\\033[32;1m'\n    YELLOW    = '\\033[33;1m'\n    BLUE      = '\\033[34;1m'\n    MAGENTA   = '\\033[35;1m'\n    CYAN      = '\\033[36;1m'\n    BOLD      = '\\033[1m'\n    UNDERLINE = '\\033[4m'\n    ENDC      = '\\033[0m'\n    \ndef getColorEntry(val):\n    if not isinstance(val, float):\n        return colors.ENDC\n    if (val < .20):\n        return colors.RED\n    elif (val < .40):\n        return colors.YELLOW\n    elif (val < .60):\n        return colors.BLUE\n    elif (val < .80):\n        return colors.CYAN\n    else:\n        return colors.GREEN", "Espnet_yangmei.py": "import torch\nimport torch.nn as nn\n\n__author__ = \"Sachin Mehta\"\n\nclass CBR(nn.Module):\n    '''\n    This class defines the convolution layer with batch normalization and PReLU activation\n    '''\n    def __init__(self, nIn, nOut, kSize, stride=1):\n        '''\n\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param kSize: kernel size\n        :param stride: stride rate for down-sampling. Default is 1\n        '''\n        super().__init__()\n        padding = int((kSize - 1)/2)\n        #self.conv = nn.Conv2d(nIn, nOut, kSize, stride=stride, padding=padding, bias=False)\n        self.conv = nn.Conv2d(nIn, nOut, (kSize, kSize), stride=stride, padding=(padding, padding), bias=False)\n        #self.conv1 = nn.Conv2d(nOut, nOut, (1, kSize), stride=1, padding=(0, padding), bias=False)\n        self.bn = nn.BatchNorm2d(nOut, eps=1e-03)\n        self.act = nn.PReLU(nOut)\n\n    def forward(self, input):\n        '''\n        :param input: input feature map\n        :return: transformed feature map\n        '''\n        output = self.conv(input)\n        #output = self.conv1(output)\n        output = self.bn(output)\n        output = self.act(output)\n        return output\n\n\nclass BR(nn.Module):\n    '''\n        This class groups the batch normalization and PReLU activation\n    '''\n    def __init__(self, nOut):\n        '''\n        :param nOut: output feature maps\n        '''\n        super().__init__()\n        self.bn = nn.BatchNorm2d(nOut, eps=1e-03)\n        self.act = nn.PReLU(nOut)\n\n    def forward(self, input):\n        '''\n        :param input: input feature map\n        :return: normalized and thresholded feature map\n        '''\n        output = self.bn(input)\n        output = self.act(output)\n        return output\n\nclass CB(nn.Module):\n    '''\n       This class groups the convolution and batch normalization\n    '''\n    def __init__(self, nIn, nOut, kSize, stride=1):\n        '''\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param kSize: kernel size\n        :param stride: optinal stide for down-sampling\n        '''\n        super().__init__()\n        padding = int((kSize - 1)/2)\n        self.conv = nn.Conv2d(nIn, nOut, (kSize, kSize), stride=stride, padding=(padding, padding), bias=False)\n        self.bn = nn.BatchNorm2d(nOut, eps=1e-03)\n\n    def forward(self, input):\n        '''\n\n        :param input: input feature map\n        :return: transformed feature map\n        '''\n        output = self.conv(input)\n        output = self.bn(output)\n        return output\n\nclass C(nn.Module):\n    '''\n    This class is for a convolutional layer.\n    '''\n    def __init__(self, nIn, nOut, kSize, stride=1):\n        '''\n\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param kSize: kernel size\n        :param stride: optional stride rate for down-sampling\n        '''\n        super().__init__()\n        padding = int((kSize - 1)/2)\n        self.conv = nn.Conv2d(nIn, nOut, (kSize, kSize), stride=stride, padding=(padding, padding), bias=False)\n\n    def forward(self, input):\n        '''\n        :param input: input feature map\n        :return: transformed feature map\n        '''\n        output = self.conv(input)\n        return output\n\nclass CDilated(nn.Module):\n    '''\n    This class defines the dilated convolution.\n    '''\n    def __init__(self, nIn, nOut, kSize, stride=1, d=1):\n        '''\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param kSize: kernel size\n        :param stride: optional stride rate for down-sampling\n        :param d: optional dilation rate\n        '''\n        super().__init__()\n        padding = int((kSize - 1)/2) * d\n        self.conv = nn.Conv2d(nIn, nOut, (kSize, kSize), stride=stride, padding=(padding, padding), bias=False, dilation=d)\n\n    def forward(self, input):\n        '''\n        :param input: input feature map\n        :return: transformed feature map\n        '''\n        output = self.conv(input)\n        return output\n\nclass DownSamplerB(nn.Module):\n    def __init__(self, nIn, nOut):\n        super().__init__()\n        n = int(nOut/5)\n        n1 = nOut - 4*n\n        self.c1 = C(nIn, n, 3, 2)\n        self.d1 = CDilated(n, n1, 3, 1, 1)\n        self.d2 = CDilated(n, n, 3, 1, 2)\n        self.d4 = CDilated(n, n, 3, 1, 4)\n        self.d8 = CDilated(n, n, 3, 1, 8)\n        self.d16 = CDilated(n, n, 3, 1, 16)\n        self.bn = nn.BatchNorm2d(nOut, eps=1e-3)\n        self.act = nn.PReLU(nOut)\n\n    def forward(self, input):\n        output1 = self.c1(input)\n        d1 = self.d1(output1)\n        d2 = self.d2(output1)\n        d4 = self.d4(output1)\n        d8 = self.d8(output1)\n        d16 = self.d16(output1)\n\n        add1 = d2\n        add2 = add1 + d4\n        add3 = add2 + d8\n        add4 = add3 + d16\n\n        combine = torch.cat([d1, add1, add2, add3, add4],1)\n        #combine_in_out = input + combine\n        output = self.bn(combine)\n        output = self.act(output)\n        return output\n\nclass DilatedParllelResidualBlockB(nn.Module):\n    '''\n    This class defines the ESP block, which is based on the following principle\n        Reduce ---> Split ---> Transform --> Merge\n    '''\n    def __init__(self, nIn, nOut, add=True):\n        '''\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param add: if true, add a residual connection through identity operation. You can use projection too as\n                in ResNet paper, but we avoid to use it if the dimensions are not the same because we do not want to\n                increase the module complexity\n        '''\n        super().__init__()\n        n = int(nOut/5)\n        n1 = nOut - 4*n\n        self.c1 = C(nIn, n, 1, 1)\n        self.d1 = CDilated(n, n1, 3, 1, 1) # dilation rate of 2^0\n        self.d2 = CDilated(n, n, 3, 1, 2) # dilation rate of 2^1\n        self.d4 = CDilated(n, n, 3, 1, 4) # dilation rate of 2^2\n        self.d8 = CDilated(n, n, 3, 1, 8) # dilation rate of 2^3\n        self.d16 = CDilated(n, n, 3, 1, 16) # dilation rate of 2^4\n        self.bn = BR(nOut)\n        self.add = add\n\n    def forward(self, input):\n        '''\n        :param input: input feature map\n        :return: transformed feature map\n        '''\n        # reduce\n        output1 = self.c1(input)\n        # split and transform\n        d1 = self.d1(output1)\n        d2 = self.d2(output1)\n        d4 = self.d4(output1)\n        d8 = self.d8(output1)\n        d16 = self.d16(output1)\n\n        # heirarchical fusion for de-gridding\n        add1 = d2\n        add2 = add1 + d4\n        add3 = add2 + d8\n        add4 = add3 + d16\n\n        #merge\n        combine = torch.cat([d1, add1, add2, add3, add4], 1)\n\n        # if residual version\n        if self.add:\n            combine = input + combine\n        output = self.bn(combine)\n        return output\n\nclass InputProjectionA(nn.Module):\n    '''\n    This class projects the input image to the same spatial dimensions as the feature map.\n    For example, if the input image is 512 x512 x3 and spatial dimensions of feature map size are 56x56xF, then\n    this class will generate an output of 56x56x3\n    '''\n    def __init__(self, samplingTimes):\n        '''\n        :param samplingTimes: The rate at which you want to down-sample the image\n        '''\n        super().__init__()\n        self.pool = nn.ModuleList()\n        for i in range(0, samplingTimes):\n            #pyramid-based approach for down-sampling\n            self.pool.append(nn.AvgPool2d(3, stride=2, padding=1))\n\n    def forward(self, input):\n        '''\n        :param input: Input RGB Image\n        :return: down-sampled image (pyramid-based approach)\n        '''\n        for pool in self.pool:\n            input = pool(input)\n        return input\n\n\nclass ESPNet_Encoder(nn.Module):\n    '''\n    This class defines the ESPNet-C network in the paper\n    '''\n    def __init__(self, classes=59, p=2, q=8):\n        '''\n        :param classes: number of classes in the dataset. Default is 20 for the cityscapes\n        :param p: depth multiplier\n        :param q: depth multiplier\n        '''\n        super().__init__()\n        self.level1 = CBR(3, 16, 3, 2)\n        self.sample1 = InputProjectionA(1)\n        self.sample2 = InputProjectionA(2)\n\n        self.b1 = BR(16 + 3)\n        self.level2_0 = DownSamplerB(16 +3, 64)\n\n        self.level2 = nn.ModuleList()\n        for i in range(0, p):\n            self.level2.append(DilatedParllelResidualBlockB(64 , 64))\n        self.b2 = BR(128 + 3)\n\n        self.level3_0 = DownSamplerB(128 + 3, 128)\n        self.level3 = nn.ModuleList()\n        for i in range(0, q):\n            self.level3.append(DilatedParllelResidualBlockB(128 , 128))\n        self.b3 = BR(256)\n\n        self.up = nn.Sequential(nn.ConvTranspose2d(256, 256, 2, stride=2, padding=0, output_padding=0, bias=False), BR(256))\n        #self.semantic = C(256, classes, 1, 1)\n        #self.depth = C(256, 1, 1, 1)\n        self.semantic = nn.ConvTranspose2d(256, classes, 2, stride=2, padding=0, output_padding=0, bias=False)\n        self.depth = nn.ConvTranspose2d(256, 1, 2, stride=2, padding=0, output_padding=0, bias=False)\n\n    def forward(self, input):\n        '''\n        :param input: Receives the input RGB image\n        :return: the transformed feature map with spatial dimensions 1/8th of the input image\n        '''\n        output0 = self.level1(input)\n        inp1 = self.sample1(input)\n        inp2 = self.sample2(input)\n\n        output0_cat = self.b1(torch.cat([output0, inp1], 1))\n        output1_0 = self.level2_0(output0_cat) # down-sampled\n        \n        for i, layer in enumerate(self.level2):\n            if i==0:\n                output1 = layer(output1_0)\n            else:\n                output1 = layer(output1)\n\n        output1_cat = self.b2(torch.cat([output1,  output1_0, inp2], 1))\n\n        output2_0 = self.level3_0(output1_cat) # down-sampled\n        for i, layer in enumerate(self.level3):\n            if i==0:\n                output2 = layer(output2_0)\n            else:\n                output2 = layer(output2)\n\n        output2_cat = self.b3(torch.cat([output2_0, output2], 1))\n\n        output2_cat = self.up(output2_cat)\n        semantic = self.semantic(output2_cat)\n        depth = self.depth(output2_cat)\n\n        return {'depth': depth, 'class_mask': semantic}\n        \nclass ESPNet(nn.Module):\n    '''\n    This class defines the ESPNet network\n    '''\n\n    def __init__(self, classes=20, p=2, q=3, encoderFile=None):\n        '''\n        :param classes: number of classes in the dataset. Default is 20 for the cityscapes\n        :param p: depth multiplier\n        :param q: depth multiplier\n        :param encoderFile: pretrained encoder weights. Recall that we first trained the ESPNet-C and then attached the\n                            RUM-based light weight decoder. See paper for more details.\n        '''\n        super().__init__()\n        self.encoder = ESPNet_Encoder(classes, p, q)\n        if encoderFile != None:\n            self.encoder.load_state_dict(torch.load(encoderFile))\n            print('Encoder loaded!')\n        # load the encoder modules\n        self.modules = []\n        for i, m in enumerate(self.encoder.children()):\n            self.modules.append(m)\n\n        # light-weight decoder\n        self.level3_C = C(128 + 3, classes, 1, 1)\n        self.br = nn.BatchNorm2d(classes, eps=1e-03)\n        self.br_depth = nn.BatchNorm2d(1, eps=1e-03)\n        self.conv = CBR(19 + classes, classes, 3, 1)\n\n        self.up_l3 = nn.Sequential(nn.ConvTranspose2d(classes, classes, 2, stride=2, padding=0, output_padding=0, bias=False))\n        self.up_l3_depth = nn.Sequential(nn.ConvTranspose2d(1, 1, 2, stride=2, padding=0, output_padding=0, bias=False))\n        self.combine_l2_l3 = nn.Sequential(BR(2*classes), DilatedParllelResidualBlockB(2*classes , classes, add=False))\n\n        self.up_l2 = nn.Sequential(nn.ConvTranspose2d(classes, classes, 2, stride=2, padding=0, output_padding=0, bias=False), BR(classes))\n\n        self.semantic = nn.ConvTranspose2d(classes, classes, 2, stride=2, padding=0, output_padding=0, bias=False)\n        self.depth = nn.ConvTranspose2d(classes, classes, 2, stride=2, padding=0, output_padding=0, bias=False)\n        \n    def forward(self, input):\n        '''\n        :param input: RGB image\n        :return: transformed feature map\n        '''\n        output0 = self.modules[0](input)\n        inp1 = self.modules[1](input)\n        inp2 = self.modules[2](input)\n\n        output0_cat = self.modules[3](torch.cat([output0, inp1], 1))\n        output1_0 = self.modules[4](output0_cat)  # down-sampled\n\n        for i, layer in enumerate(self.modules[5]):\n            if i == 0:\n                output1 = layer(output1_0)\n            else:\n                output1 = layer(output1)\n\n        output1_cat = self.modules[6](torch.cat([output1, output1_0, inp2], 1))\n\n        output2_0 = self.modules[7](output1_cat)  # down-sampled\n        for i, layer in enumerate(self.modules[8]):\n            if i == 0:\n                output2 = layer(output2_0)\n            else:\n                output2 = layer(output2)\n\n        output2_cat = self.modules[9](torch.cat([output2_0, output2], 1)) # concatenate for feature map width expansion\n\n        output2_c = self.up_l3(self.br(self.modules[10](output2_cat))) #RUM,1/8\u7684\u8bed\u4e49\u5206\u5272\u7ed3\u679c\n        output2_depth = self.up_l3_depth(self.br_depth(self.modules[11](output2_cat)))\n\n        output1_C = self.level3_C(output1_cat) # project to C-dimensional space\n        comb_l2_l3 = self.up_l2(self.combine_l2_l3(torch.cat([output1_C, output2_c], 1))) #RUM\n\n        concat_features = self.conv(torch.cat([comb_l2_l3, output0_cat], 1))\n\n        classifier = self.classifier(concat_features)\n        return classifier\n\n", "lednet.py": "import torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as F\nfrom torch.nn.functional import interpolate as interpolate\nimport math\n\n\ndef split(x):\n    c = int(x.size()[1])\n    c1 = round(c * 0.5)\n    x1 = x[:, :c1, :, :].contiguous()\n    x2 = x[:, c1:, :, :].contiguous()\n\n    return x1, x2\n\ndef channel_shuffle(x,groups):\n    batchsize, num_channels, height, width = x.data.size()\n    \n    channels_per_group = num_channels // groups\n    \n    # reshape\n    x = x.view(batchsize,groups,\n        channels_per_group,height,width)\n    \n    x = torch.transpose(x,1,2).contiguous()\n    \n    # flatten\n    x = x.view(batchsize,-1,height,width)\n    \n    return x\n    \n\nclass Conv2dBnRelu(nn.Module):\n    def __init__(self,in_ch,out_ch,kernel_size=3,stride=1,padding=0,dilation=1,bias=True):\n        super(Conv2dBnRelu,self).__init__()\n\t\t\n        self.conv = nn.Sequential(\n\t\tnn.Conv2d(in_ch,out_ch,kernel_size,stride,padding,dilation=dilation,bias=bias),\n\t\tnn.BatchNorm2d(out_ch, eps=1e-3),\n\t\tnn.ReLU(inplace=True)\n\t)\n\n    def forward(self, x):\n        return self.conv(x)\n\n\n# after Concat -> BN, you also can use Dropout like SS_nbt_module may be make a good result!\nclass DownsamplerBlock (nn.Module):\n    def __init__(self, in_channel, out_channel):\n        super(DownsamplerBlock,self).__init__()\n\n        self.conv = nn.Conv2d(in_channel, out_channel-in_channel, (3, 3), stride=2, padding=1, bias=True)\n        self.pool = nn.MaxPool2d(2, stride=2)\n        self.bn = nn.BatchNorm2d(out_channel, eps=1e-3)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, input):\n        x1 = self.pool(input)\n        x2 = self.conv(input)\n\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n\n        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n                        diffY // 2, diffY - diffY // 2])\n\n        output = torch.cat([x2, x1], 1)\n        output = self.bn(output)\n        output = self.relu(output)\n        return output\n\n\nclass SS_nbt_module(nn.Module):\n    def __init__(self, chann, dropprob, dilated):        \n        super().__init__()\n\n        oup_inc = chann//2\n        \n        # dw\n        self.conv3x1_1_l = nn.Conv2d(oup_inc, oup_inc, (3,1), stride=1, padding=(1,0), bias=True)\n\n        self.conv1x3_1_l = nn.Conv2d(oup_inc, oup_inc, (1,3), stride=1, padding=(0,1), bias=True)\n\n        self.bn1_l = nn.BatchNorm2d(oup_inc, eps=1e-03)\n\n        self.conv3x1_2_l = nn.Conv2d(oup_inc, oup_inc, (3,1), stride=1, padding=(1*dilated,0), bias=True, dilation = (dilated,1))\n\n        self.conv1x3_2_l = nn.Conv2d(oup_inc, oup_inc, (1,3), stride=1, padding=(0,1*dilated), bias=True, dilation = (1,dilated))\n\n        self.bn2_l = nn.BatchNorm2d(oup_inc, eps=1e-03)\n        \n        # dw\n        self.conv3x1_1_r = nn.Conv2d(oup_inc, oup_inc, (3,1), stride=1, padding=(1,0), bias=True)\n\n        self.conv1x3_1_r = nn.Conv2d(oup_inc, oup_inc, (1,3), stride=1, padding=(0,1), bias=True)\n\n        self.bn1_r = nn.BatchNorm2d(oup_inc, eps=1e-03)\n\n        self.conv3x1_2_r = nn.Conv2d(oup_inc, oup_inc, (3,1), stride=1, padding=(1*dilated,0), bias=True, dilation = (dilated,1))\n\n        self.conv1x3_2_r = nn.Conv2d(oup_inc, oup_inc, (1,3), stride=1, padding=(0,1*dilated), bias=True, dilation = (1,dilated))\n\n        self.bn2_r = nn.BatchNorm2d(oup_inc, eps=1e-03)       \n        \n        self.relu = nn.ReLU(inplace=True)\n        self.dropout = nn.Dropout2d(dropprob)       \n        \n    @staticmethod\n    def _concat(x,out):\n        return torch.cat((x,out),1)    \n    \n    def forward(self, input):\n\n        # x1 = input[:,:(input.shape[1]//2),:,:]\n        # x2 = input[:,(input.shape[1]//2):,:,:]\n        residual = input\n        x1, x2 = split(input) #\u6309\u901a\u9053\u5206\u6210\u4e24\u4efd\n\n        output1 = self.conv3x1_1_l(x1)\n        output1 = self.relu(output1)\n        output1 = self.conv1x3_1_l(output1)\n        output1 = self.bn1_l(output1)\n        output1 = self.relu(output1)\n\n        output1 = self.conv3x1_2_l(output1)\n        output1 = self.relu(output1)\n        output1 = self.conv1x3_2_l(output1)\n        output1 = self.bn2_l(output1)\n    \n    \n        output2 = self.conv1x3_1_r(x2)\n        output2 = self.relu(output2)\n        output2 = self.conv3x1_1_r(output2)\n        output2 = self.bn1_r(output2)\n        output2 = self.relu(output2)\n\n        output2 = self.conv1x3_2_r(output2)\n        output2 = self.relu(output2)\n        output2 = self.conv3x1_2_r(output2)\n        output2 = self.bn2_r(output2)\n\n        if (self.dropout.p != 0):\n            output1 = self.dropout(output1)\n            output2 = self.dropout(output2)\n\n        out = self._concat(output1,output2)\n        out = F.relu(residual + out, inplace=True)\n        return channel_shuffle(out,2)\n\n\n\nclass Encoder(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n\n        self.initial_block = DownsamplerBlock(3,32)\n\n        self.layers1 = nn.ModuleList()\n        self.layers2 = nn.ModuleList()\n\n        for x in range(0, 3):\n            self.layers1.append(SS_nbt_module(32, 0.03, 1))\n        \n\n        self.layers1.append(DownsamplerBlock(32,64))\n        \n\n        for x in range(0, 2):\n            self.layers1.append(SS_nbt_module(64, 0.03, 1))\n  \n        self.layers2.append(DownsamplerBlock(64,128))\n\n        for x in range(0, 1):    \n            self.layers2.append(SS_nbt_module(128, 0.3, 1))\n            self.layers2.append(SS_nbt_module(128, 0.3, 2))\n            self.layers2.append(SS_nbt_module(128, 0.3, 5))\n            self.layers2.append(SS_nbt_module(128, 0.3, 9))\n            \n        for x in range(0, 1):    \n            self.layers2.append(SS_nbt_module(128, 0.3, 2))\n            self.layers2.append(SS_nbt_module(128, 0.3, 5))\n            self.layers2.append(SS_nbt_module(128, 0.3, 9))\n            self.layers2.append(SS_nbt_module(128, 0.3, 17))\n                    \n\n        # Only in encoder mode:\n        self.output_conv = nn.Conv2d(128, num_classes, 1, stride=1, padding=0, bias=True)\n        self.depth_conv = nn.Conv2d(128, 1, 1, stride=1, padding=0, bias=True)\n\n    def forward(self, input, predict=False):\n        \n        output = self.initial_block(input)\n\n        for layer in self.layers1:\n            output = layer(output)\n        low_feature = output  #1/4*1/4*64\n        \n        for layer in self.layers2:\n            output = layer(output)\n\n        if predict:\n            semantic = self.output_conv(output)\n            depth = self.depth_conv(output)\n            #depth = F.interpolate(depth, size=(int(math.ceil(input.size()[-2]/4)),\n            #                    int(math.ceil(input.size()[-1]/4))), mode='bilinear', align_corners=True)\n            return semantic, depth\n\n        return low_feature, output\n\nclass Interpolate(nn.Module):\n    def __init__(self,size,mode):\n        super(Interpolate,self).__init__()\n        \n        self.interp = nn.functional.interpolate\n        self.size = size\n        self.mode = mode\n    def forward(self,x):\n        x = self.interp(x,size=self.size,mode=self.mode,align_corners=True)\n        return x\n        \n\nclass APN_Module(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(APN_Module, self).__init__()\n        # global pooling branch\n        self.branch1 = nn.Sequential(\n                nn.AdaptiveAvgPool2d(1),\n                Conv2dBnRelu(in_ch, out_ch, kernel_size=1, stride=1, padding=0)\n\t)\n        # midddle branch\n        self.mid = nn.Sequential(\n\t\tConv2dBnRelu(in_ch, out_ch, kernel_size=1, stride=1, padding=0)\n\t)\n        self.down1 = Conv2dBnRelu(in_ch, 1, kernel_size=7, stride=2, padding=3)\n\t\t\n        self.down2 = Conv2dBnRelu(1, 1, kernel_size=5, stride=2, padding=2)\n\t\t\n        self.down3 = nn.Sequential(\n\t\tConv2dBnRelu(1, 1, kernel_size=3, stride=2, padding=1),\n\t\tConv2dBnRelu(1, 1, kernel_size=3, stride=1, padding=1)\n\t)\n\t\t\n        self.conv2 = Conv2dBnRelu(1, 1, kernel_size=5, stride=1, padding=2)\n        self.conv1 = Conv2dBnRelu(1, 1, kernel_size=7, stride=1, padding=3)\n\t\n        self.depth = nn.Conv2d(1, out_ch, kernel_size=1, stride=1)\n    def forward(self, x):\n        \n        h = x.size()[2]\n        w = x.size()[3]\n        \n        #b1 = self.branch1(x)\n        # b1 = Interpolate(size=(h, w), mode=\"bilinear\")(b1)\n        #b1= interpolate(b1, size=(h, w), mode=\"bilinear\", align_corners=True)\n\t\n        mid = self.mid(x)\n\t\t\n        x1 = self.down1(x) #1/16,1\n        x2 = self.down2(x1) #1/32,1\n        x3 = self.down3(x2) #1/64,1\n        # x3 = Interpolate(size=(h // 4, w // 4), mode=\"bilinear\")(x3)\n        x3= interpolate(x3, size=x2.size()[2:], mode=\"bilinear\", align_corners=True)\t #1/32,1\n        x2 = self.conv2(x2) \n        x = x2 + x3\n        # x = Interpolate(size=(h // 2, w // 2), mode=\"bilinear\")(x)\n        x= interpolate(x, size=x1.size()[2:], mode=\"bilinear\", align_corners=True)\n       \t\t\n        x1 = self.conv1(x1)\n        x = x + x1\n        # x = Interpolate(size=(h, w), mode=\"bilinear\")(x)\n        x= interpolate(x, size=(h, w), mode=\"bilinear\", align_corners=True)  #1/8,1\n        \t\t\n        #x = torch.mul(x, mid)\n\n        x = x + mid\n        x = self.depth(x)\n       \n       \n        return x\n          \n\nclass ASPP(nn.Module):\n    \n    def __init__(self, inplanes, planes, rate):\n        super(ASPP, self).__init__()\n        self.rate=rate\n        if rate == 1:\n            kernel_size = 1\n            padding = 0\n        else:\n            kernel_size = 3\n            padding = rate\n            self.conv1 = nn.Conv2d(planes, planes, kernel_size=3, bias=False,padding=1)\n            #self.conv1 =SeparableConv2d(planes,planes,3,1,1)\n            self.bn1 = nn.BatchNorm2d(planes)\n            self.relu1 = nn.ReLU()\n   \n            #self.atrous_convolution = nn.Conv2d(inplanes, planes, kernel_size=kernel_size,\n            #                         stride=1, padding=padding, dilation=rate, bias=False)\n        self.atrous_convolution = SeparableConv2d(inplanes,planes,kernel_size,1,padding,rate)\n        self.bn = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU()\n        \n        \n        \n\n    def forward(self, x):\n        x = self.atrous_convolution(x)\n        x = self.bn(x)\n        #x = self.relu(x)\n        if self.rate!=1:\n            x=self.conv1(x)\n            x=self.bn1(x)\n            x=self.relu1(x)\n        return x\n\n\nclass SeparableConv2d(nn.Module):\n    def __init__(self,in_channels,out_channels,kernel_size=1,stride=1,padding=0,dilation=1,bias=False):\n        super(SeparableConv2d,self).__init__()\n\n        self.conv1 = nn.Conv2d(in_channels,in_channels,kernel_size,stride,padding,dilation,groups=in_channels,bias=bias)\n        self.pointwise = nn.Conv2d(in_channels,out_channels,1,1,0,1,1,bias=bias)\n\n    def forward(self,x):\n        x = self.conv1(x)\n        x = self.pointwise(x)\n        return x\n        \n        \n\nclass Decoder (nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n\n        self.apn = APN_Module(in_ch=128,out_ch=1)\n\n        self.conv = nn.Conv2d(128, 256, (3, 3), stride=2, padding=1, bias=True)\n        self.bn = nn.BatchNorm2d(256, eps=1e-3)\n        self.relu = nn.ReLU(inplace=True)\n\n        rates = [1, 3, 6, 9]\n        self.aspp1 = ASPP(256, 96, rate=rates[0])\n        self.aspp2 = ASPP(256, 96, rate=rates[1])\n        self.aspp3 = ASPP(256, 96, rate=rates[2])\n        self.aspp4 = ASPP(256, 96, rate=rates[3])\n\n        self.global_avg_pool = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n                                             nn.Conv2d(256, 96, 1, stride=1, bias=False),\n                                             nn.BatchNorm2d(96),\n                                             nn.ReLU())\n                                             \n        self.conv1 =SeparableConv2d(480+256,96,1)\n        self.bn1 = nn.BatchNorm2d(96)\n        \n        self.last_conv = nn.Sequential(#nn.Conv2d(24+96, 96, kernel_size=3, stride=1, padding=1, bias=False),\n                                       SeparableConv2d(64+96,96,3,1,1),\n                                       nn.BatchNorm2d(96),\n                                       nn.ReLU(),\n                                       #nn.Conv2d(96, 96, kernel_size=3, stride=1, padding=1, bias=False),\n                                       SeparableConv2d(96,96,3,1,1),\n                                       nn.BatchNorm2d(96),\n                                       nn.ReLU())\n        \n        self.semantic = nn.Conv2d(96, num_classes, kernel_size=1, stride=1)\n        # self.upsample = Interpolate(size=(512, 1024), mode=\"bilinear\")\n        # self.output_conv = nn.ConvTranspose2d(16, num_classes, kernel_size=4, stride=2, padding=1, output_padding=0, bias=True)\n        # self.output_conv = nn.ConvTranspose2d(16, num_classes, kernel_size=3, stride=2, padding=1, output_padding=1, bias=True)\n        # self.output_conv = nn.ConvTranspose2d(16, num_classes, kernel_size=2, stride=2, padding=0, output_padding=0, bias=True)\n  \n    def forward(self, low_feature, input):\n        \n        depth = self.apn(input)\n        depth = interpolate(depth, size=(212, 256), mode=\"bilinear\", align_corners=True)\n        \n        semantic = self.conv(input)\n        x1 = self.aspp1(semantic)\n        x2 = self.aspp2(semantic)\n        x3 = self.aspp3(semantic)\n        x4 = self.aspp4(semantic)\n        x5 = self.global_avg_pool(semantic)\n        x5 = F.interpolate(x5, size=x4.size()[2:], mode='bilinear', align_corners=True)\n        semantic = torch.cat((semantic,x1, x2, x3, x4, x5), dim=1)\n\n        semantic = self.conv1(semantic)\n        semantic = self.bn1(semantic)\n        semantic = self.relu(semantic)\n        semantic = F.interpolate(semantic, size=(low_feature.size()[-2],\n                                low_feature.size()[-1]), mode='bilinear', align_corners=True)\n                                \n        semantic = torch.cat((semantic, low_feature), dim=1)\n        semantic = self.last_conv(semantic)\n        semantic = self.semantic(semantic)\n        semantic = F.interpolate(semantic, size=(212, 256), mode='bilinear', align_corners=True)\n        return semantic, depth\n\n\n# LEDNet\nclass Net(nn.Module):\n    def __init__(self, num_classes, encoder=None):  \n        super().__init__()\n\n        if (encoder == None):\n            self.encoder = Encoder(num_classes)\n        else:\n            self.encoder = encoder\n        self.decoder = Decoder(num_classes)\n\n    def forward(self, input, only_encode=True):\n        #import ipdb;ipdb.set_trace()\n        if only_encode:\n            return self.encoder.forward(input, predict=True)  #semantic,depth\n        else:\n            low_feature, output = self.encoder(input)    \n            return self.decoder.forward(low_feature, output)\n\ndef flops(model,input_size):\n    input = torch.rand(1, 3, input_size, input_size).cuda()\n    flops, params = profile(model, inputs=(input, ))\n    from thop import clever_format\n    flops, params = clever_format([flops, params], \"%.3f\")\n    print(flops, params)\n    #import pdb;pdb.set_trace()\n\ndef FPS(model,input_size):\n    import time\n    model.eval()\n\n    for x in range(0,200):\n        input = torch.randn(1, 3, input_size, input_size).cuda()\n        with torch.no_grad():\n            # import ipdb;ipdb.set_trace()\n            output = model.forward(input)\n        \n    total=0\n    for x in range(0,200):\n        input = torch.randn(1, 3, input_size, input_size).cuda()\n        with torch.no_grad():\n            a = time.perf_counter()\n            output = model.forward(input)\n            torch.cuda.synchronize()\n            b = time.perf_counter()\n            total+=b-a\n    print('FPS:', str(200/total))\n    print('ms:', str(1000*total/200))\n    \nif __name__ == '__main__':\n\n    from thop import profile\n    model = Net(num_classes=59)\n    model.cuda()\n    input_size = 224\n    # flops(model,input_size)\n    FPS(model,input_size)\n            \n", "separableconv.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Sun Jan 13 11:05:01 2019\n\n@author: Taha Emara  @email: taha@emaraic.com\n\"\"\"\nimport torch.nn as nn\n\n\nclass SeparableConv2d(nn.Module):\n    def __init__(self,in_channels,out_channels,kernel_size=1,stride=1,padding=0,dilation=1,bias=False):\n        super(SeparableConv2d,self).__init__()\n\n        self.conv1 = nn.Conv2d(in_channels,in_channels,kernel_size,stride,padding,dilation,groups=in_channels,bias=bias)\n        self.pointwise = nn.Conv2d(in_channels,out_channels,1,1,0,1,1,bias=bias)\n\n    def forward(self,x):\n        x = self.conv1(x)\n        x = self.pointwise(x)\n        return x\n", "fast_mobilenetv1_aspp.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Sun Dec 16 11:20:32 2018\n\n@author: Taha Emara  @email: taha@emaraic.com\n\"\"\"\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\nimport math\nimport sys\nimport os\nsys.path.append(os.path.abspath('./'))\n# from models.backbone_networks import MobileNetV2_prune as MobileNetV2\nfrom models.backbone_networks import MobileNetV1\nfrom models import aspp\nfrom models.separableconv import SeparableConv2d \n\n\ndef weights_init(m):\n    # Initialize kernel weights with Gaussian distributions\n    if isinstance(m, nn.Conv2d):\n        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        m.weight.data.normal_(0, math.sqrt(2. / n))\n        if m.bias is not None:\n            m.bias.data.zero_()\n    elif isinstance(m, nn.ConvTranspose2d):\n        n = m.kernel_size[0] * m.kernel_size[1] * m.in_channels\n        m.weight.data.normal_(0, math.sqrt(2. / n))\n        if m.bias is not None:\n            m.bias.data.zero_()\n    elif isinstance(m, nn.BatchNorm2d):\n        m.weight.data.fill_(1)\n        m.bias.data.zero_()\n\n\n\n\nclass RT(nn.Module):\n    \n    def __init__(self, n_classes=19, pretrained=False):\n        \n        super(RT, self).__init__()\n        print(\"PDESNet-MobileNet...\")\n\n        # self.mobile_features=MobileNetV2.MobileNetV2()\n        # self.mobile_features=MobileNetV1.MobileNet_res()\n        self.mobile_features=MobileNetV1.MobileNet()\n        \n        if pretrained:\n            print('Loading pretrained model: model_best.pth.tar...')\n            pretrained_path = os.path.join('pretrained_model', 'model_best.pth.tar')\n            checkpoint = torch.load(pretrained_path)\n            state_dict = checkpoint['state_dict']\n\n            from collections import OrderedDict\n            new_state_dict = OrderedDict()\n            for k, v in state_dict.items():\n                name = k[7:] # remove `module.`\n                new_state_dict[name] = v\n            self.mobile_features.load_state_dict(new_state_dict)\n        else:\n            self.mobile_features.apply(weights_init)\n        \n        \n        rates = [1, 3, 6, 9]\n        # rates = [1, 5, 9]\n\n        x_c = 1024\n        out_c = 96\n        low_level_c = 128\n        self.aspp1 = aspp.ASPP(x_c, out_c, rate=rates[0], dw=True)\n        self.aspp2 = aspp.ASPP(x_c, out_c, rate=rates[1], dw=True)\n        self.aspp3 = aspp.ASPP(x_c, out_c, rate=rates[2], dw=True)\n        self.aspp4 = aspp.ASPP(x_c, out_c, rate=rates[3], dw=True)\n\n        self.relu_prune = nn.ReLU()\n        self.global_avg_pool_prune = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n                                             nn.Conv2d(x_c, out_c, 1, stride=1, bias=False),\n                                             nn.BatchNorm2d(out_c),\n                                             nn.ReLU())\n        #self.conv1 = nn.Conv2d(480+1280, 96, 1, bias=False)\n        self.conv1_prune =SeparableConv2d(x_c+out_c*5,out_c,1)\n        self.bn1_prune = nn.BatchNorm2d(out_c)\n\n        #adopt [1x1, 48] for channel reduction.\n        #self.conv2 = nn.Conv2d(24, 32, 1, bias=False)\n        #self.bn2 = nn.BatchNorm2d(32)\n    \n        self.last_conv_prune1 = nn.Sequential(\n                                       SeparableConv2d(low_level_c+out_c,out_c,3,1,1),\n                                       nn.BatchNorm2d(out_c),\n                                       nn.ReLU())\n                                       \n        self.last_conv_prune2 = nn.Sequential(\n                                       SeparableConv2d(out_c,out_c,3,1,1),\n                                       nn.BatchNorm2d(out_c),\n                                       nn.ReLU())\n        \n        self.deep_prune = nn.Conv2d(out_c, 1, kernel_size=1, stride=1)\n        self.class_prune = nn.Conv2d(out_c, n_classes, kernel_size=1, stride=1)\n        \n    def trunk(self, input, input_size):\n        x, low_level_features = self.mobile_features(input)\n        # x: (b,1024,input_h/32,input_h/32)\n        # low_level_features: (b, 128, input_h/4, input_w/4)\n        # import ipdb; ipdb.set_trace()\n        x1 = self.aspp1(x) #(b,96,input_h/32,input_h/32)\n        x2 = self.aspp2(x) #(b,96,input_h/32,input_h/32)\n        x3 = self.aspp3(x) #(b,96,input_h/32,input_h/32)\n        x4 = self.aspp4(x) #(b,96,input_h/32,input_h/32)\n        x5 = self.global_avg_pool_prune(x) #(b,96,1,1)\n        x5 = F.interpolate(x5, size=x4.size()[2:], mode='bilinear', align_corners=True) #(b,96,input_h/32,input_h/32)\n\n        x = torch.cat((x,x1, x2, x3, x4, x5), dim=1) #(b,1760,input_h/32,input_h/32)\n        # x = torch.cat((x,x1, x2, x3), dim=1) #(b,288,input_h/32,input_h/32)\n        #print('after aspp cat',x.size())\n        x = self.conv1_prune(x) #(b,96,input_h/32,input_h/32)\n        x = self.bn1_prune(x)\n        x = self.relu_prune(x)\n        x = F.interpolate(x, size=(int(math.ceil(input_size[-2]/4)),\n                                int(math.ceil(input_size[-1]/4))), mode='bilinear', align_corners=True) # (b, 96, input_h/4, input_w/4)\n       # ablation=torch.max(low_level_features, 1)[1]\n        #print('after con on aspp output',x.size())\n\n        ##comment to remove low feature\n        #low_level_features = self.conv2(low_level_features)\n        #low_level_features = self.bn2(low_level_features)\n        #low_level_features = self.relu(low_level_features)\n        #print(\"low\",low_level_features.size())\n        \n        x = torch.cat((x, low_level_features), dim=1) #(b, 120, input_h/4, input_w/4)\n        #print('after cat low feature with output of aspp',x.size())\n        x = self.last_conv_prune1(x) #(b, 96, input_h/4, input_w/4)\n        return x\n        \n    def head(self, x, input_size):\n    \n        depth = self.last_conv_prune2(x) #(b, 96, input_h/4, input_w/4)\n        depth = self.deep_prune(depth) #(b, 1, input_h/4, input_w/4)\n        depth = F.interpolate(depth, size=(int(math.ceil(input_size[-2]/2)),\n                                int(math.ceil(input_size[-1]/2))), mode='bilinear', align_corners=True) #(b, 1, input_h/2, input_w/2)\n\n        class_mask = self.last_conv_prune2(x) #(b, 96, input_h/4, input_w/4)\n        class_mask = self.class_prune(class_mask) #(b, 1, input_h/4, input_w/4)\n        class_mask = F.interpolate(class_mask, size=(int(math.ceil(input_size[-2]/2)),\n                                int(math.ceil(input_size[-1]/2))), mode='bilinear', align_corners=True) #(b, n_classes, input_h/2, input_w/2)\n                                \n        return depth, class_mask\n        \n    def forward(self, input):\n        input_size = input.size()\n        \n        x = self.trunk(input, input_size)\n        \n        depth, class_mask = self.head(x, input_size)\n        \n        return {'depth': depth, 'class_mask': class_mask}\n\n    def freeze_bn(self):\n        for m in self.modules():\n            if isinstance(m, nn.BatchNorm2d):\n                m.eval()\n\n    def __init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                torch.nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n                \ndef flops(model,input_size):\n    input = torch.rand(1, 3, input_size, input_size).cuda()\n    flops, params = profile(model, inputs=(input, ))\n    from thop import clever_format\n    flops, params = clever_format([flops, params], \"%.3f\")\n    print(flops, params)\n    #import pdb;pdb.set_trace()\n\ndef FPS(model,input_size):\n    import time\n    model.eval()\n\n    for x in range(0,200):\n        input = torch.randn(1, 3, input_size, input_size).cuda()\n        with torch.no_grad():\n            # import ipdb;ipdb.set_trace()\n            output = model.forward(input)\n        \n    total=0\n    for x in range(0,200):\n        input = torch.randn(1, 3, input_size, input_size).cuda()\n        with torch.no_grad():\n            a = time.perf_counter()\n            output = model.forward(input)\n            torch.cuda.synchronize()\n            b = time.perf_counter()\n            total+=b-a\n    print('FPS:', str(200/total))\n    print('ms:', str(1000*total/200))\n    \nif __name__ == '__main__':\n\n    from thop import profile\n    model = RT(n_classes=59, pretrained=False)\n    model.cuda()\n    input_size = 224\n    # flops(model,input_size)\n    FPS(model,input_size)\n", "fast_mobilenetv1_no_aspp.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Sun Dec 16 11:20:32 2018\n\n@author: Taha Emara  @email: taha@emaraic.com\n\"\"\"\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\nimport math\nimport sys\nimport os\nsys.path.append(os.path.abspath('./'))\n# from models.backbone_networks import MobileNetV2_prune as MobileNetV2\nfrom models.backbone_networks import MobileNetV1\nfrom models import aspp\nfrom models.separableconv import SeparableConv2d \n\n\ndef weights_init(m):\n    # Initialize kernel weights with Gaussian distributions\n    if isinstance(m, nn.Conv2d):\n        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        m.weight.data.normal_(0, math.sqrt(2. / n))\n        if m.bias is not None:\n            m.bias.data.zero_()\n    elif isinstance(m, nn.ConvTranspose2d):\n        n = m.kernel_size[0] * m.kernel_size[1] * m.in_channels\n        m.weight.data.normal_(0, math.sqrt(2. / n))\n        if m.bias is not None:\n            m.bias.data.zero_()\n    elif isinstance(m, nn.BatchNorm2d):\n        m.weight.data.fill_(1)\n        m.bias.data.zero_()\n\n\n\n\nclass RT(nn.Module):\n    \n    def __init__(self, n_classes=19, pretrained=False):\n        \n        super(RT, self).__init__()\n        print(\"PDESNet-MobileNet...\")\n\n        # self.mobile_features=MobileNetV2.MobileNetV2()\n        # self.mobile_features=MobileNetV1.MobileNet_res()\n        self.mobile_features=MobileNetV1.MobileNet()\n        \n        if pretrained:\n            print('Loading pretrained model: model_best.pth.tar...')\n            pretrained_path = os.path.join('pretrained_model', 'model_best.pth.tar')\n            checkpoint = torch.load(pretrained_path)\n            state_dict = checkpoint['state_dict']\n\n            from collections import OrderedDict\n            new_state_dict = OrderedDict()\n            for k, v in state_dict.items():\n                name = k[7:] # remove `module.`\n                new_state_dict[name] = v\n            self.mobile_features.load_state_dict(new_state_dict)\n        else:\n            self.mobile_features.apply(weights_init)\n        \n        \n        # rates = [1, 3, 6, 9]\n        rates = [1, 5, 9]\n\n        x_c = 1024\n        out_c = 96\n        low_level_c = 128\n\n        self.relu_prune = nn.ReLU()\n        self.global_avg_pool_prune = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n                                             nn.Conv2d(x_c, out_c, 1, stride=1, bias=False),\n                                             nn.BatchNorm2d(out_c),\n                                             nn.ReLU())\n        #self.conv1 = nn.Conv2d(480+1280, 96, 1, bias=False)\n        # self.conv1_prune =SeparableConv2d(x_c+out_c*5,out_c,1)\n        self.conv1_prune =SeparableConv2d(x_c,out_c,1)\n        self.bn1_prune = nn.BatchNorm2d(out_c)\n\n        #adopt [1x1, 48] for channel reduction.\n        #self.conv2 = nn.Conv2d(24, 32, 1, bias=False)\n        #self.bn2 = nn.BatchNorm2d(32)\n    \n        self.last_conv_prune1 = nn.Sequential(\n                                       SeparableConv2d(low_level_c+out_c,out_c,3,1,1),\n                                       nn.BatchNorm2d(out_c),\n                                       nn.ReLU())\n                                       \n        self.last_conv_prune2 = nn.Sequential(\n                                       SeparableConv2d(out_c,out_c,3,1,1),\n                                       nn.BatchNorm2d(out_c),\n                                       nn.ReLU())\n        \n        self.deep_prune = nn.Conv2d(out_c, 1, kernel_size=1, stride=1)\n        self.class_prune = nn.Conv2d(out_c, n_classes, kernel_size=1, stride=1)\n        \n    def trunk(self, input, input_size):\n        x, low_level_features = self.mobile_features(input)\n        # x: (b,1024,input_h/32,input_h/32)\n        # low_level_features: (b, 128, input_h/4, input_w/4)\n        # import ipdb; ipdb.set_trace()\n        #print('after aspp cat',x.size())\n        x = self.conv1_prune(x) #(b,96,input_h/32,input_h/32)\n        x = self.bn1_prune(x)\n        x = self.relu_prune(x)\n        x = F.interpolate(x, size=(int(math.ceil(input_size[-2]/4)),\n                                int(math.ceil(input_size[-1]/4))), mode='bilinear', align_corners=True) # (b, 96, input_h/4, input_w/4)\n       # ablation=torch.max(low_level_features, 1)[1]\n        #print('after con on aspp output',x.size())\n\n        ##comment to remove low feature\n        #low_level_features = self.conv2(low_level_features)\n        #low_level_features = self.bn2(low_level_features)\n        #low_level_features = self.relu(low_level_features)\n        #print(\"low\",low_level_features.size())\n        \n        x = torch.cat((x, low_level_features), dim=1) #(b, 120, input_h/4, input_w/4)\n        #print('after cat low feature with output of aspp',x.size())\n        x = self.last_conv_prune1(x) #(b, 96, input_h/4, input_w/4)\n        return x\n        \n    def head(self, x, input_size):\n    \n        depth = self.last_conv_prune2(x) #(b, 96, input_h/4, input_w/4)\n        depth = self.deep_prune(depth) #(b, 1, input_h/4, input_w/4)\n        depth = F.interpolate(depth, size=(int(math.ceil(input_size[-2]/2)),\n                                int(math.ceil(input_size[-1]/2))), mode='bilinear', align_corners=True) #(b, 1, input_h/2, input_w/2)\n\n        class_mask = self.last_conv_prune2(x) #(b, 96, input_h/4, input_w/4)\n        class_mask = self.class_prune(class_mask) #(b, 1, input_h/4, input_w/4)\n        class_mask = F.interpolate(class_mask, size=(int(math.ceil(input_size[-2]/2)),\n                                int(math.ceil(input_size[-1]/2))), mode='bilinear', align_corners=True) #(b, n_classes, input_h/2, input_w/2)\n                                \n        return depth, class_mask\n        \n    def forward(self, input):\n        input_size = input.size()\n        \n        x = self.trunk(input, input_size)\n        \n        depth, class_mask = self.head(x, input_size)\n        \n        return {'depth': depth, 'class_mask': class_mask}\n\n    def freeze_bn(self):\n        for m in self.modules():\n            if isinstance(m, nn.BatchNorm2d):\n                m.eval()\n\n    def __init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                torch.nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n                \ndef flops(model,input_size):\n    input = torch.rand(1, 3, input_size, input_size).cuda()\n    flops, params = profile(model, inputs=(input, ))\n    from thop import clever_format\n    flops, params = clever_format([flops, params], \"%.3f\")\n    print(flops, params)\n    #import pdb;pdb.set_trace()\n\ndef FPS(model,input_size):\n    import time\n    model.eval()\n\n    for x in range(0,200):\n        input = torch.randn(1, 3, input_size, input_size).cuda()\n        with torch.no_grad():\n            # import ipdb;ipdb.set_trace()\n            output = model.forward(input)\n        \n    total=0\n    for x in range(0,200):\n        input = torch.randn(1, 3, input_size, input_size).cuda()\n        with torch.no_grad():\n            a = time.perf_counter()\n            output = model.forward(input)\n            torch.cuda.synchronize()\n            b = time.perf_counter()\n            total+=b-a\n    print('FPS:', str(200/total))\n    print('ms:', str(1000*total/200))\n    \nif __name__ == '__main__':\n\n    from thop import profile\n    model = RT(n_classes=59, pretrained=False)\n    model.cuda()\n    input_size = 224\n    # flops(model,input_size)\n    FPS(model,input_size)\n", "fast_mobilenetv1_aspp_maskinput.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Sun Dec 16 11:20:32 2018\n\n@author: Taha Emara  @email: taha@emaraic.com\n\"\"\"\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\nimport math\nimport sys\nimport os\nsys.path.append(os.path.abspath('./'))\n# from models.backbone_networks import MobileNetV2_prune as MobileNetV2\nfrom models.backbone_networks import MobileNetV1\nfrom models import aspp\nfrom models.separableconv import SeparableConv2d \n\n\ndef weights_init(m):\n    # Initialize kernel weights with Gaussian distributions\n    if isinstance(m, nn.Conv2d):\n        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        m.weight.data.normal_(0, math.sqrt(2. / n))\n        if m.bias is not None:\n            m.bias.data.zero_()\n    elif isinstance(m, nn.ConvTranspose2d):\n        n = m.kernel_size[0] * m.kernel_size[1] * m.in_channels\n        m.weight.data.normal_(0, math.sqrt(2. / n))\n        if m.bias is not None:\n            m.bias.data.zero_()\n    elif isinstance(m, nn.BatchNorm2d):\n        m.weight.data.fill_(1)\n        m.bias.data.zero_()\n\n\n\n\nclass RT(nn.Module):\n    \n    def __init__(self, n_classes=19, pretrained=False):\n        \n        super(RT, self).__init__()\n        print(\"PDESNet-MobileNet...\")\n\n        # self.mobile_features=MobileNetV2.MobileNetV2()\n        # self.mobile_features=MobileNetV1.MobileNet_res()\n        self.mobile_features=MobileNetV1.MobileNet()\n        \n        if pretrained:\n            print('Loading pretrained model: model_best.pth.tar...')\n            pretrained_path = os.path.join('pretrained_model', 'model_best.pth.tar')\n            checkpoint = torch.load(pretrained_path)\n            state_dict = checkpoint['state_dict']\n\n            from collections import OrderedDict\n            new_state_dict = OrderedDict()\n            for k, v in state_dict.items():\n                name = k[7:] # remove `module.`\n                new_state_dict[name] = v\n            self.mobile_features.load_state_dict(new_state_dict)\n        else:\n            self.mobile_features.apply(weights_init)\n        \n        \n        rates = [1, 3, 6, 9]\n        # rates = [1, 5, 9]\n\n        x_c = 1024\n        out_c = 96\n        low_level_c = 128\n        self.aspp1 = aspp.ASPP(x_c, out_c, rate=rates[0])\n        self.aspp2 = aspp.ASPP(x_c, out_c, rate=rates[1])\n        self.aspp3 = aspp.ASPP(x_c, out_c, rate=rates[2])\n        self.aspp4 = aspp.ASPP(x_c, out_c, rate=rates[3])\n\n        self.relu_prune = nn.ReLU()\n        self.global_avg_pool_prune = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n                                             nn.Conv2d(x_c, out_c, 1, stride=1, bias=False),\n                                             nn.BatchNorm2d(out_c),\n                                             nn.ReLU())\n        #self.conv1 = nn.Conv2d(480+1280, 96, 1, bias=False)\n        self.conv1_prune =SeparableConv2d(x_c+out_c*5,out_c,1)\n        self.bn1_prune = nn.BatchNorm2d(out_c)\n\n        #adopt [1x1, 48] for channel reduction.\n        #self.conv2 = nn.Conv2d(24, 32, 1, bias=False)\n        #self.bn2 = nn.BatchNorm2d(32)\n    \n        self.last_conv_prune1 = nn.Sequential(\n                                       SeparableConv2d(low_level_c+out_c,out_c,3,1,1),\n                                       nn.BatchNorm2d(out_c),\n                                       nn.ReLU())\n                                       \n        self.last_conv_prune2 = nn.Sequential(\n                                       SeparableConv2d(out_c,out_c,3,1,1),\n                                       nn.BatchNorm2d(out_c),\n                                       nn.ReLU())\n        \n        self.deep_prune2 = nn.Conv2d(out_c+1, 1, kernel_size=1, stride=1)\n        self.class_prune = nn.Conv2d(out_c, n_classes, kernel_size=1, stride=1)\n        \n    def trunk(self, input, input_size):\n        x, low_level_features = self.mobile_features(input)\n        # x: (b,1024,input_h/32,input_h/32)\n        # low_level_features: (b, 128, input_h/4, input_w/4)\n        # import ipdb; ipdb.set_trace()\n        x1 = self.aspp1(x) #(b,96,input_h/32,input_h/32)\n        x2 = self.aspp2(x) #(b,96,input_h/32,input_h/32)\n        x3 = self.aspp3(x) #(b,96,input_h/32,input_h/32)\n        x4 = self.aspp4(x) #(b,96,input_h/32,input_h/32)\n        x5 = self.global_avg_pool_prune(x) #(b,96,1,1)\n        x5 = F.interpolate(x5, size=x4.size()[2:], mode='bilinear', align_corners=True) #(b,96,input_h/32,input_h/32)\n\n        x = torch.cat((x,x1, x2, x3, x4, x5), dim=1) #(b,1760,input_h/32,input_h/32)\n        # x = torch.cat((x,x1, x2, x3), dim=1) #(b,288,input_h/32,input_h/32)\n        #print('after aspp cat',x.size())\n        x = self.conv1_prune(x) #(b,96,input_h/32,input_h/32)\n        x = self.bn1_prune(x)\n        x = self.relu_prune(x)\n        x = F.interpolate(x, size=(int(math.ceil(input_size[-2]/4)),\n                                int(math.ceil(input_size[-1]/4))), mode='bilinear', align_corners=True) # (b, 96, input_h/4, input_w/4)\n       # ablation=torch.max(low_level_features, 1)[1]\n        #print('after con on aspp output',x.size())\n\n        ##comment to remove low feature\n        #low_level_features = self.conv2(low_level_features)\n        #low_level_features = self.bn2(low_level_features)\n        #low_level_features = self.relu(low_level_features)\n        #print(\"low\",low_level_features.size())\n        \n        x = torch.cat((x, low_level_features), dim=1) #(b, 120, input_h/4, input_w/4)\n        #print('after cat low feature with output of aspp',x.size())\n        x = self.last_conv_prune1(x) #(b, 96, input_h/4, input_w/4)\n        return x\n        \n    def head2(self, x, input_size):\n    \n        class_mask = self.last_conv_prune2(x) #(b, 96, input_h/4, input_w/4)\n        class_mask_final = self.class_prune(class_mask) #(b, n_classes, input_h/4, input_w/4)\n        class_mask_final = F.interpolate(class_mask_final, size=(int(math.ceil(input_size[-2]/2)),\n                                int(math.ceil(input_size[-1]/2))), mode='bilinear', align_corners=True) #(b, n_classes, input_h/2, input_w/2)\n                                \n        depth = self.last_conv_prune2(x) #(b, 96, input_h/4, input_w/4)\n        depth = torch.cat((depth, class_mask), dim=1)\n        depth = self.deep_prune2(depth) #(b, 1, input_h/4, input_w/4)\n        depth = F.interpolate(depth, size=(int(math.ceil(input_size[-2]/2)),\n                                int(math.ceil(input_size[-1]/2))), mode='bilinear', align_corners=True) #(b, 1, input_h/2, input_w/2)\n\n                                \n        return depth, class_mask_final\n        \n    \n    def head(self, x, input_size):\n    \n        class_mask = self.last_conv_prune2(x) #(b, 96, input_h/4, input_w/4)\n        class_mask_final = self.class_prune(class_mask) #(b, n_classes, input_h/4, input_w/4)\n        \n        # import ipdb; ipdb.set_trace()\n        class_mask_bin = class_mask_final.max(1)[1].unsqueeze(1).float() #(b,1,input_h/4, input_w/4)\n        class_mask_bin[class_mask_bin>0] = 1.\n        \n        class_mask_final = F.interpolate(class_mask_final, size=(int(math.ceil(input_size[-2]/2)),\n                                int(math.ceil(input_size[-1]/2))), mode='bilinear', align_corners=True) #(b, n_classes, input_h/2, input_w/2)\n                                \n                                \n        depth = self.last_conv_prune2(x) #(b, 96, input_h/4, input_w/4)\n        depth = torch.cat((depth, class_mask_bin), dim=1)\n        depth = self.deep_prune2(depth) #(b, 1, input_h/4, input_w/4)\n        depth = F.interpolate(depth, size=(int(math.ceil(input_size[-2]/2)),\n                                int(math.ceil(input_size[-1]/2))), mode='bilinear', align_corners=True) #(b, 1, input_h/2, input_w/2)\n\n                                \n        return depth, class_mask_final\n        \n    def forward(self, input):\n        input_size = input.size()\n        \n        x = self.trunk(input, input_size)\n        \n        depth, class_mask = self.head(x, input_size)\n        \n        return {'depth': depth, 'class_mask': class_mask}\n\n    def freeze_bn(self):\n        for m in self.modules():\n            if isinstance(m, nn.BatchNorm2d):\n                m.eval()\n\n    def __init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                torch.nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n                \ndef flops(model,input_size):\n    input = torch.rand(1, 3, input_size, input_size).cuda()\n    flops, params = profile(model, inputs=(input, ))\n    from thop import clever_format\n    flops, params = clever_format([flops, params], \"%.3f\")\n    print(flops, params)\n    #import pdb;pdb.set_trace()\n\ndef FPS(model,input_size):\n    import time\n    model.eval()\n\n    for x in range(0,200):\n        input = torch.randn(1, 3, input_size, input_size).cuda()\n        with torch.no_grad():\n            # import ipdb;ipdb.set_trace()\n            output = model.forward(input)\n        \n    total=0\n    for x in range(0,200):\n        input = torch.randn(1, 3, input_size, input_size).cuda()\n        with torch.no_grad():\n            a = time.perf_counter()\n            output = model.forward(input)\n            torch.cuda.synchronize()\n            b = time.perf_counter()\n            total+=b-a\n    print('FPS:', str(200/total))\n    print('ms:', str(1000*total/200))\n    \nif __name__ == '__main__':\n\n    from thop import profile\n    model = RT(n_classes=1, pretrained=False)\n    model.cuda()\n    input_size = 224\n    # flops(model,input_size)\n    FPS(model,input_size)\n", "fastdepth.py": "import os\nimport torch\nimport torch.nn as nn\nimport torchvision.models\nimport collections\nimport math\nimport torch.nn.functional as F\nimport time\nimport sys\nimport os\nsys.path.append(os.path.abspath('./'))\nfrom models.backbone_networks import MobileNetV1\n\nclass Identity(nn.Module):\n    # a dummy identity module\n    def __init__(self):\n        super(Identity, self).__init__()\n\n    def forward(self, x):\n        return x\n\nclass Unpool(nn.Module):\n    # Unpool: 2*2 unpooling with zero padding\n    def __init__(self, stride=2):\n        super(Unpool, self).__init__()\n\n        self.stride = stride\n\n        # create kernel [1, 0; 0, 0]\n        self.mask = torch.zeros(1, 1, stride, stride)\n        self.mask[:,:,0,0] = 1\n\n    def forward(self, x):\n        assert x.dim() == 4\n        num_channels = x.size(1)\n        return F.conv_transpose2d(x,\n            self.mask.detach().type_as(x).expand(num_channels, 1, -1, -1),\n            stride=self.stride, groups=num_channels)\n\ndef weights_init(m):\n    # Initialize kernel weights with Gaussian distributions\n    if isinstance(m, nn.Conv2d):\n        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        m.weight.data.normal_(0, math.sqrt(2. / n))\n        if m.bias is not None:\n            m.bias.data.zero_()\n    elif isinstance(m, nn.ConvTranspose2d):\n        n = m.kernel_size[0] * m.kernel_size[1] * m.in_channels\n        m.weight.data.normal_(0, math.sqrt(2. / n))\n        if m.bias is not None:\n            m.bias.data.zero_()\n    elif isinstance(m, nn.BatchNorm2d):\n        m.weight.data.fill_(1)\n        m.bias.data.zero_()\n\ndef conv(in_channels, out_channels, kernel_size):\n    padding = (kernel_size-1) // 2\n    assert 2*padding == kernel_size-1, \"parameters incorrect. kernel={}, padding={}\".format(kernel_size, padding)\n    return nn.Sequential(\n          nn.Conv2d(in_channels,out_channels,kernel_size,stride=1,padding=padding,bias=False),\n          nn.BatchNorm2d(out_channels),\n          nn.ReLU(inplace=True),\n        )\n\ndef depthwise(in_channels, kernel_size):\n    padding = (kernel_size-1) // 2\n    assert 2*padding == kernel_size-1, \"parameters incorrect. kernel={}, padding={}\".format(kernel_size, padding)\n    return nn.Sequential(\n          nn.Conv2d(in_channels,in_channels,kernel_size,stride=1,padding=padding,bias=False,groups=in_channels),\n          nn.BatchNorm2d(in_channels),\n          nn.ReLU(inplace=True),\n        )\n\ndef pointwise(in_channels, out_channels):\n    return nn.Sequential(\n          nn.Conv2d(in_channels,out_channels,1,1,0,bias=False),\n          nn.BatchNorm2d(out_channels),\n          nn.ReLU(inplace=True),\n        )\n\ndef convt(in_channels, out_channels, kernel_size):\n    stride = 2\n    padding = (kernel_size - 1) // 2\n    output_padding = kernel_size % 2\n    assert -2 - 2*padding + kernel_size + output_padding == 0, \"deconv parameters incorrect\"\n    return nn.Sequential(\n            nn.ConvTranspose2d(in_channels,out_channels,kernel_size,\n                stride,padding,output_padding,bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        )\n\ndef convt_dw(channels, kernel_size):\n    stride = 2\n    padding = (kernel_size - 1) // 2\n    output_padding = kernel_size % 2\n    assert -2 - 2*padding + kernel_size + output_padding == 0, \"deconv parameters incorrect\"\n    return nn.Sequential(\n            nn.ConvTranspose2d(channels,channels,kernel_size,\n                stride,padding,output_padding,bias=False,groups=channels),\n            nn.BatchNorm2d(channels),\n            nn.ReLU(inplace=True),\n        )\n\ndef upconv(in_channels, out_channels):\n    return nn.Sequential(\n        Unpool(2),\n        nn.Conv2d(in_channels,out_channels,kernel_size=5,stride=1,padding=2,bias=False),\n        nn.BatchNorm2d(out_channels),\n        nn.ReLU(),\n    )\n\nclass upproj(nn.Module):\n    # UpProj module has two branches, with a Unpool at the start and a ReLu at the end\n    #   upper branch: 5*5 conv -> batchnorm -> ReLU -> 3*3 conv -> batchnorm\n    #   bottom branch: 5*5 conv -> batchnorm\n\n    def __init__(self, in_channels, out_channels):\n        super(upproj, self).__init__()\n        self.unpool = Unpool(2)\n        self.branch1 = nn.Sequential(\n            nn.Conv2d(in_channels,out_channels,kernel_size=5,stride=1,padding=2,bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels,out_channels,kernel_size=3,stride=1,padding=1,bias=False),\n            nn.BatchNorm2d(out_channels),\n        )\n        self.branch2 = nn.Sequential(\n            nn.Conv2d(in_channels,out_channels,kernel_size=5,stride=1,padding=2,bias=False),\n            nn.BatchNorm2d(out_channels),\n        )\n\n    def forward(self, x):\n        x = self.unpool(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        return F.relu(x1 + x2)\n\nclass Decoder(nn.Module):\n    names = ['deconv{}{}'.format(i,dw) for i in range(3,10,2) for dw in ['', 'dw']]\n    names.append(\"upconv\")\n    names.append(\"upproj\")\n    for i in range(3,10,2):\n        for dw in ['', 'dw']:\n            names.append(\"nnconv{}{}\".format(i, dw))\n            names.append(\"blconv{}{}\".format(i, dw))\n            names.append(\"shuffle{}{}\".format(i, dw))\n\nclass DeConv(nn.Module):\n\n    def __init__(self, kernel_size, dw):\n        super(DeConv, self).__init__()\n        if dw:\n            self.convt1 = nn.Sequential(\n                convt_dw(1024, kernel_size),\n                pointwise(1024, 512))\n            self.convt2 = nn.Sequential(\n                convt_dw(512, kernel_size),\n                pointwise(512, 256))\n            self.convt3 = nn.Sequential(\n                convt_dw(256, kernel_size),\n                pointwise(256, 128))\n            self.convt4 = nn.Sequential(\n                convt_dw(128, kernel_size),\n                pointwise(128, 64))\n            self.convt5 = nn.Sequential(\n                convt_dw(64, kernel_size),\n                pointwise(64, 32))\n        else:\n            self.convt1 = convt(1024, 512, kernel_size)\n            self.convt2 = convt(512, 256, kernel_size)\n            self.convt3 = convt(256, 128, kernel_size)\n            self.convt4 = convt(128, 64, kernel_size)\n            self.convt5 = convt(64, 32, kernel_size)\n        self.convf = pointwise(32, 1)\n\n    def forward(self, x):\n        x = self.convt1(x)\n        x = self.convt2(x)\n        x = self.convt3(x)\n        x = self.convt4(x)\n        x = self.convt5(x)\n        x = self.convf(x)\n        return x\n\n\nclass UpConv(nn.Module):\n\n    def __init__(self):\n        super(UpConv, self).__init__()\n        self.upconv1 = upconv(1024, 512)\n        self.upconv2 = upconv(512, 256)\n        self.upconv3 = upconv(256, 128)\n        self.upconv4 = upconv(128, 64)\n        self.upconv5 = upconv(64, 32)\n        self.convf = pointwise(32, 1)\n\n    def forward(self, x):\n        x = self.upconv1(x)\n        x = self.upconv2(x)\n        x = self.upconv3(x)\n        x = self.upconv4(x)\n        x = self.upconv5(x)\n        x = self.convf(x)\n        return x\n\nclass UpProj(nn.Module):\n    # UpProj decoder consists of 4 upproj modules with decreasing number of channels and increasing feature map size\n\n    def __init__(self):\n        super(UpProj, self).__init__()\n        self.upproj1 = upproj(1024, 512)\n        self.upproj2 = upproj(512, 256)\n        self.upproj3 = upproj(256, 128)\n        self.upproj4 = upproj(128, 64)\n        self.upproj5 = upproj(64, 32)\n        self.convf = pointwise(32, 1)\n\n    def forward(self, x):\n        x = self.upproj1(x)\n        x = self.upproj2(x)\n        x = self.upproj3(x)\n        x = self.upproj4(x)\n        x = self.upproj5(x)\n        x = self.convf(x)\n        return x\n\nclass NNConv(nn.Module):\n\n    def __init__(self, kernel_size, dw):\n        super(NNConv, self).__init__()\n        if dw:\n            self.conv1 = nn.Sequential(\n                depthwise(1024, kernel_size),\n                pointwise(1024, 512))\n            self.conv2 = nn.Sequential(\n                depthwise(512, kernel_size),\n                pointwise(512, 256))\n            self.conv3 = nn.Sequential(\n                depthwise(256, kernel_size),\n                pointwise(256, 128))\n            self.conv4 = nn.Sequential(\n                depthwise(128, kernel_size),\n                pointwise(128, 64))\n            self.conv5 = nn.Sequential(\n                depthwise(64, kernel_size),\n                pointwise(64, 32))\n            self.conv6 = pointwise(32, 1)\n        else:\n            self.conv1 = conv(1024, 512, kernel_size)\n            self.conv2 = conv(512, 256, kernel_size)\n            self.conv3 = conv(256, 128, kernel_size)\n            self.conv4 = conv(128, 64, kernel_size)\n            self.conv5 = conv(64, 32, kernel_size)\n            self.conv6 = pointwise(32, 1)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.interpolate(x, scale_factor=2, mode='nearest')\n\n        x = self.conv2(x)\n        x = F.interpolate(x, scale_factor=2, mode='nearest')\n\n        x = self.conv3(x)\n        x = F.interpolate(x, scale_factor=2, mode='nearest')\n\n        x = self.conv4(x)\n        x = F.interpolate(x, scale_factor=2, mode='nearest')\n\n        x = self.conv5(x)\n        x = F.interpolate(x, scale_factor=2, mode='nearest')\n\n        x = self.conv6(x)\n        return x\n\nclass BLConv(NNConv):\n\n    def __init__(self, kernel_size, dw):\n        super(BLConv, self).__init__(kernel_size, dw)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n\n        x = self.conv2(x)\n        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n\n        x = self.conv3(x)\n        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n\n        x = self.conv4(x)\n        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n\n        x = self.conv5(x)\n        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n\n        x = self.conv6(x)\n        return x\n\nclass ShuffleConv(nn.Module):\n\n    def __init__(self, kernel_size, dw):\n        super(ShuffleConv, self).__init__()\n        if dw:\n            self.conv1 = nn.Sequential(\n                depthwise(256, kernel_size),\n                pointwise(256, 256))\n            self.conv2 = nn.Sequential(\n                depthwise(64, kernel_size),\n                pointwise(64, 64))\n            self.conv3 = nn.Sequential(\n                depthwise(16, kernel_size),\n                pointwise(16, 16))\n            self.conv4 = nn.Sequential(\n                depthwise(4, kernel_size),\n                pointwise(4, 4))\n        else:\n            self.conv1 = conv(256, 256, kernel_size)\n            self.conv2 = conv(64, 64, kernel_size)\n            self.conv3 = conv(16, 16, kernel_size)\n            self.conv4 = conv(4, 4, kernel_size)\n\n    def forward(self, x):\n        x = F.pixel_shuffle(x, 2)\n        x = self.conv1(x)\n\n        x = F.pixel_shuffle(x, 2)\n        x = self.conv2(x)\n\n        x = F.pixel_shuffle(x, 2)\n        x = self.conv3(x)\n\n        x = F.pixel_shuffle(x, 2)\n        x = self.conv4(x)\n\n        x = F.pixel_shuffle(x, 2)\n        return x\n\ndef choose_decoder(decoder):\n    depthwise = ('dw' in decoder)\n    if decoder[:6] == 'deconv':\n        assert len(decoder)==7 or (len(decoder)==9 and 'dw' in decoder)\n        kernel_size = int(decoder[6])\n        model = DeConv(kernel_size, depthwise)\n    elif decoder == \"upproj\":\n        model = UpProj()\n    elif decoder == \"upconv\":\n        model = UpConv()\n    elif decoder[:7] == 'shuffle':\n        assert len(decoder)==8 or (len(decoder)==10 and 'dw' in decoder)\n        kernel_size = int(decoder[7])\n        model = ShuffleConv(kernel_size, depthwise)\n    elif decoder[:6] == 'nnconv':\n        assert len(decoder)==7 or (len(decoder)==9 and 'dw' in decoder)\n        kernel_size = int(decoder[6])\n        model = NNConv(kernel_size, depthwise)\n    elif decoder[:6] == 'blconv':\n        assert len(decoder)==7 or (len(decoder)==9 and 'dw' in decoder)\n        kernel_size = int(decoder[6])\n        model = BLConv(kernel_size, depthwise)\n    else:\n        assert False, \"invalid option for decoder: {}\".format(decoder)\n    model.apply(weights_init)\n    return model\n\n\nclass ResNet(nn.Module):\n    def __init__(self, layers, decoder, output_size, in_channels=3, pretrained=True):\n\n        if layers not in [18, 34, 50, 101, 152]:\n            raise RuntimeError('Only 18, 34, 50, 101, and 152 layer model are defined for ResNet. Got {}'.format(layers))\n        \n        super(ResNet, self).__init__()\n        self.output_size = output_size\n        pretrained_model = torchvision.models.__dict__['resnet{}'.format(layers)](pretrained=pretrained)\n        if not pretrained:\n            pretrained_model.apply(weights_init)\n        \n        if in_channels == 3:\n            self.conv1 = pretrained_model._modules['conv1']\n            self.bn1 = pretrained_model._modules['bn1']\n        else:\n            self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n            self.bn1 = nn.BatchNorm2d(64)\n            weights_init(self.conv1)\n            weights_init(self.bn1)\n        \n        self.relu = pretrained_model._modules['relu']\n        self.maxpool = pretrained_model._modules['maxpool']\n        self.layer1 = pretrained_model._modules['layer1']\n        self.layer2 = pretrained_model._modules['layer2']\n        self.layer3 = pretrained_model._modules['layer3']\n        self.layer4 = pretrained_model._modules['layer4']\n\n        # clear memory\n        del pretrained_model\n\n        # define number of intermediate channels\n        if layers <= 34:\n            num_channels = 512\n        elif layers >= 50:\n            num_channels = 2048\n        self.conv2 = nn.Conv2d(num_channels, 1024, 1)\n        weights_init(self.conv2)\n        self.decoder = choose_decoder(decoder)\n\n    def forward(self, x):\n        # resnet\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.conv2(x)\n\n        # decoder\n        x = self.decoder(x)\n\n        return x\n\nclass MobileNet(nn.Module):\n    def __init__(self, decoder, output_size, in_channels=3, pretrained=True):\n\n        super(MobileNet, self).__init__()\n        self.output_size = output_size\n        # mobilenet = imagenet.mobilenet.MobileNet()\n        mobilenet = MobileNetV1.MobileNet()\n        if pretrained:\n            print('Loading pretrained model: model_best.pth.tar...')\n            pretrained_path = os.path.join('pretrained_model', 'model_best.pth.tar')\n            checkpoint = torch.load(pretrained_path)\n            state_dict = checkpoint['state_dict']\n\n            from collections import OrderedDict\n            new_state_dict = OrderedDict()\n            for k, v in state_dict.items():\n                name = k[7:] # remove `module.`\n                new_state_dict[name] = v\n            mobilenet.load_state_dict(new_state_dict)\n        else:\n            mobilenet.apply(weights_init)\n\n        if in_channels == 3:\n            self.mobilenet = nn.Sequential(*(mobilenet.model[i] for i in range(14)))\n        else:\n            def conv_bn(inp, oup, stride):\n                return nn.Sequential(\n                    nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                    nn.BatchNorm2d(oup),\n                    nn.ReLU6(inplace=True)\n                )\n\n            self.mobilenet = nn.Sequential(\n                conv_bn(in_channels,  32, 2),\n                *(mobilenet.model[i] for i in range(1,14))\n                )\n\n        self.decoder = choose_decoder(decoder)\n\n    def forward(self, x):\n        x = self.mobilenet(x) #(b,1024,input_h/32,input_w/32)\n        x = self.decoder(x)\n        # print(x.shape)\n        return x\n        \n    def name(self):\n        return 'MobileNet'\n\nclass ResNetSkipAdd(nn.Module):\n    def __init__(self, layers, output_size, in_channels=3, pretrained=True):\n\n        if layers not in [18, 34, 50, 101, 152]:\n            raise RuntimeError('Only 18, 34, 50, 101, and 152 layer model are defined for ResNet. Got {}'.format(layers))\n        \n        super(ResNetSkipAdd, self).__init__()\n        self.output_size = output_size\n        pretrained_model = torchvision.models.__dict__['resnet{}'.format(layers)](pretrained=pretrained)\n        if not pretrained:\n            pretrained_model.apply(weights_init)\n        \n        if in_channels == 3:\n            self.conv1 = pretrained_model._modules['conv1']\n            self.bn1 = pretrained_model._modules['bn1']\n        else:\n            self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n            self.bn1 = nn.BatchNorm2d(64)\n            weights_init(self.conv1)\n            weights_init(self.bn1)\n        \n        self.relu = pretrained_model._modules['relu']\n        self.maxpool = pretrained_model._modules['maxpool']\n        self.layer1 = pretrained_model._modules['layer1']\n        self.layer2 = pretrained_model._modules['layer2']\n        self.layer3 = pretrained_model._modules['layer3']\n        self.layer4 = pretrained_model._modules['layer4']\n\n        # clear memory\n        del pretrained_model\n\n        # define number of intermediate channels\n        if layers <= 34:\n            num_channels = 512\n        elif layers >= 50:\n            num_channels = 2048\n        self.conv2 = nn.Conv2d(num_channels, 1024, 1)\n        weights_init(self.conv2)\n        \n        kernel_size = 5\n        self.decode_conv1 = conv(1024, 512, kernel_size)\n        self.decode_conv2 = conv(512, 256, kernel_size)\n        self.decode_conv3 = conv(256, 128, kernel_size)\n        self.decode_conv4 = conv(128, 64, kernel_size)\n        self.decode_conv5 = conv(64, 32, kernel_size)\n        self.decode_conv6 = pointwise(32, 1)\n        weights_init(self.decode_conv1)\n        weights_init(self.decode_conv2)\n        weights_init(self.decode_conv3)\n        weights_init(self.decode_conv4)\n        weights_init(self.decode_conv5)\n        weights_init(self.decode_conv6)\n\n    def forward(self, x):\n        # resnet\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x1 = self.relu(x)\n        # print(\"x1\", x1.size())\n        x2 = self.maxpool(x1)\n        # print(\"x2\", x2.size())\n        x3 = self.layer1(x2)\n        # print(\"x3\", x3.size())\n        x4 = self.layer2(x3)\n        # print(\"x4\", x4.size())\n        x5 = self.layer3(x4)\n        # print(\"x5\", x5.size())\n        x6 = self.layer4(x5)\n        # print(\"x6\", x6.size())\n        x7 = self.conv2(x6)\n\n        # decoder\n        y10 = self.decode_conv1(x7)\n        # print(\"y10\", y10.size())\n        y9 = F.interpolate(y10 + x6, scale_factor=2, mode='nearest')\n        # print(\"y9\", y9.size())\n        y8 = self.decode_conv2(y9)\n        # print(\"y8\", y8.size())\n        y7 = F.interpolate(y8 + x5, scale_factor=2, mode='nearest')\n        # print(\"y7\", y7.size())\n        y6 = self.decode_conv3(y7)\n        # print(\"y6\", y6.size())\n        y5 = F.interpolate(y6 + x4, scale_factor=2, mode='nearest')\n        # print(\"y5\", y5.size())\n        y4 = self.decode_conv4(y5)\n        # print(\"y4\", y4.size())\n        y3 = F.interpolate(y4 + x3, scale_factor=2, mode='nearest')\n        # print(\"y3\", y3.size())\n        y2 = self.decode_conv5(y3 + x1)\n        # print(\"y2\", y2.size())\n        y1 = F.interpolate(y2, scale_factor=2, mode='nearest')\n        # print(\"y1\", y1.size())\n        y = self.decode_conv6(y1)\n\n        return y\n\nclass ResNetSkipConcat(nn.Module):\n    def __init__(self, layers, output_size, in_channels=3, pretrained=True):\n\n        if layers not in [18, 34, 50, 101, 152]:\n            raise RuntimeError('Only 18, 34, 50, 101, and 152 layer model are defined for ResNet. Got {}'.format(layers))\n        \n        super(ResNetSkipConcat, self).__init__()\n        self.output_size = output_size\n        pretrained_model = torchvision.models.__dict__['resnet{}'.format(layers)](pretrained=pretrained)\n        if not pretrained:\n            pretrained_model.apply(weights_init)\n        \n        if in_channels == 3:\n            self.conv1 = pretrained_model._modules['conv1']\n            self.bn1 = pretrained_model._modules['bn1']\n        else:\n            self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n            self.bn1 = nn.BatchNorm2d(64)\n            weights_init(self.conv1)\n            weights_init(self.bn1)\n        \n        self.relu = pretrained_model._modules['relu']\n        self.maxpool = pretrained_model._modules['maxpool']\n        self.layer1 = pretrained_model._modules['layer1']\n        self.layer2 = pretrained_model._modules['layer2']\n        self.layer3 = pretrained_model._modules['layer3']\n        self.layer4 = pretrained_model._modules['layer4']\n\n        # clear memory\n        del pretrained_model\n\n        # define number of intermediate channels\n        if layers <= 34:\n            num_channels = 512\n        elif layers >= 50:\n            num_channels = 2048\n        self.conv2 = nn.Conv2d(num_channels, 1024, 1)\n        weights_init(self.conv2)\n        \n        kernel_size = 5\n        self.decode_conv1 = conv(1024, 512, kernel_size)\n        self.decode_conv2 = conv(768, 256, kernel_size)\n        self.decode_conv3 = conv(384, 128, kernel_size)\n        self.decode_conv4 = conv(192, 64, kernel_size)\n        self.decode_conv5 = conv(128, 32, kernel_size)\n        self.decode_conv6 = pointwise(32, 1)\n        weights_init(self.decode_conv1)\n        weights_init(self.decode_conv2)\n        weights_init(self.decode_conv3)\n        weights_init(self.decode_conv4)\n        weights_init(self.decode_conv5)\n        weights_init(self.decode_conv6)\n\n    def forward(self, x):\n        # resnet\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x1 = self.relu(x)\n        # print(\"x1\", x1.size())\n        x2 = self.maxpool(x1)\n        # print(\"x2\", x2.size())\n        x3 = self.layer1(x2)\n        # print(\"x3\", x3.size())\n        x4 = self.layer2(x3)\n        # print(\"x4\", x4.size())\n        x5 = self.layer3(x4)\n        # print(\"x5\", x5.size())\n        x6 = self.layer4(x5)\n        # print(\"x6\", x6.size())\n        x7 = self.conv2(x6)\n\n        # decoder\n        y10 = self.decode_conv1(x7)\n        # print(\"y10\", y10.size())\n        y9 = F.interpolate(y10, scale_factor=2, mode='nearest')\n        # print(\"y9\", y9.size())\n        y8 = self.decode_conv2(torch.cat((y9, x5), 1))\n        # print(\"y8\", y8.size())\n        y7 = F.interpolate(y8, scale_factor=2, mode='nearest')\n        # print(\"y7\", y7.size())\n        y6 = self.decode_conv3(torch.cat((y7, x4), 1))\n        # print(\"y6\", y6.size())\n        y5 = F.interpolate(y6, scale_factor=2, mode='nearest')\n        # print(\"y5\", y5.size())\n        y4 = self.decode_conv4(torch.cat((y5, x3), 1))\n        # print(\"y4\", y4.size())\n        y3 = F.interpolate(y4, scale_factor=2, mode='nearest')\n        # print(\"y3\", y3.size())\n        y2 = self.decode_conv5(torch.cat((y3, x1), 1))\n        # print(\"y2\", y2.size())\n        y1 = F.interpolate(y2, scale_factor=2, mode='nearest')\n        # print(\"y1\", y1.size())\n        y = self.decode_conv6(y1)\n\n        return y\n\nclass MobileNetSkipAdd(nn.Module):\n    def __init__(self, output_size, pretrained=True):\n\n        super(MobileNetSkipAdd, self).__init__()\n        self.output_size = output_size\n        # mobilenet = imagenet.mobilenet.MobileNet()\n        mobilenet = MobileNetV1.MobileNet()\n        if pretrained:\n            pretrained_path = os.path.join('pretrained_model', 'model_best.pth.tar')\n            checkpoint = torch.load(pretrained_path)\n            state_dict = checkpoint['state_dict']\n\n            from collections import OrderedDict\n            new_state_dict = OrderedDict()\n            for k, v in state_dict.items():\n                name = k[7:] # remove `module.`\n                new_state_dict[name] = v\n            mobilenet.load_state_dict(new_state_dict)\n        else:\n            mobilenet.apply(weights_init)\n\n        for i in range(14):\n            setattr( self, 'conv{}'.format(i), mobilenet.model[i])\n\n        kernel_size = 5\n        # self.decode_conv1 = conv(1024, 512, kernel_size)\n        # self.decode_conv2 = conv(512, 256, kernel_size)\n        # self.decode_conv3 = conv(256, 128, kernel_size)\n        # self.decode_conv4 = conv(128, 64, kernel_size)\n        # self.decode_conv5 = conv(64, 32, kernel_size)\n        self.decode_conv1 = nn.Sequential(\n            depthwise(1024, kernel_size),\n            pointwise(1024, 512))\n        self.decode_conv2 = nn.Sequential(\n            depthwise(512, kernel_size),\n            pointwise(512, 256))\n        self.decode_conv3 = nn.Sequential(\n            depthwise(256, kernel_size),\n            pointwise(256, 128))\n        self.decode_conv4 = nn.Sequential(\n            depthwise(128, kernel_size),\n            pointwise(128, 64))\n        self.decode_conv5 = nn.Sequential(\n            depthwise(64, kernel_size),\n            pointwise(64, 32))\n        self.decode_conv6 = pointwise(32, 1)\n        weights_init(self.decode_conv1)\n        weights_init(self.decode_conv2)\n        weights_init(self.decode_conv3)\n        weights_init(self.decode_conv4)\n        weights_init(self.decode_conv5)\n        weights_init(self.decode_conv6)\n\n    def forward(self, x):\n        # skip connections: dec4: enc1\n        # dec 3: enc2 or enc3\n        # dec 2: enc4 or enc5\n        # print(x.shape)\n        for i in range(14):\n            layer = getattr(self, 'conv{}'.format(i))\n            x = layer(x)\n            # print(\"{}: {}\".format(i, x.size()))\n            if i==1:\n                x1 = x\n            elif i==3:\n                x2 = x\n            elif i==5:\n                x3 = x\n        for i in range(1,6):\n            layer = getattr(self, 'decode_conv{}'.format(i))\n            x = layer(x)\n            x = F.interpolate(x, scale_factor=2, mode='nearest')\n            # print(\"{}: {}\".format(i, x.size()))\n            if i==4:\n                x = x + x1\n            elif i==3:\n                x = x + x2\n            elif i==2:\n                x = x + x3\n        x = self.decode_conv6(x)\n        # print(x.shape)\n        return x\n        \n    def name(self):\n        return 'MobileNetSkipAdd'\n\nclass MobileNetSkipConcat(nn.Module):\n    def __init__(self, output_size, pretrained=True):\n\n        super(MobileNetSkipConcat, self).__init__()\n        self.output_size = output_size\n        # mobilenet = imagenet.mobilenet.MobileNet()\n        mobilenet = MobileNetV1.MobileNet()\n        if pretrained:\n            pretrained_path = os.path.join('pretrained_model', 'model_best.pth.tar')\n            checkpoint = torch.load(pretrained_path)\n            state_dict = checkpoint['state_dict']\n\n            from collections import OrderedDict\n            new_state_dict = OrderedDict()\n            for k, v in state_dict.items():\n                name = k[7:] # remove `module.`\n                new_state_dict[name] = v\n            mobilenet.load_state_dict(new_state_dict)\n        else:\n            mobilenet.apply(weights_init)\n\n        for i in range(14):\n            setattr( self, 'conv{}'.format(i), mobilenet.model[i])\n\n        kernel_size = 5\n        # self.decode_conv1 = conv(1024, 512, kernel_size)\n        # self.decode_conv2 = conv(512, 256, kernel_size)\n        # self.decode_conv3 = conv(256, 128, kernel_size)\n        # self.decode_conv4 = conv(128, 64, kernel_size)\n        # self.decode_conv5 = conv(64, 32, kernel_size)\n        self.decode_conv1 = nn.Sequential(\n            depthwise(1024, kernel_size),\n            pointwise(1024, 512))\n        self.decode_conv2 = nn.Sequential(\n            depthwise(512, kernel_size),\n            pointwise(512, 256))\n        self.decode_conv3 = nn.Sequential(\n            depthwise(512, kernel_size),\n            pointwise(512, 128))\n        self.decode_conv4 = nn.Sequential(\n            depthwise(256, kernel_size),\n            pointwise(256, 64))\n        self.decode_conv5 = nn.Sequential(\n            depthwise(128, kernel_size),\n            pointwise(128, 32))\n        self.decode_conv6 = pointwise(32, 1)\n        weights_init(self.decode_conv1)\n        weights_init(self.decode_conv2)\n        weights_init(self.decode_conv3)\n        weights_init(self.decode_conv4)\n        weights_init(self.decode_conv5)\n        weights_init(self.decode_conv6)\n\n    def forward(self, x):\n        # skip connections: dec4: enc1\n        # dec 3: enc2 or enc3\n        # dec 2: enc4 or enc5\n        for i in range(14):\n            layer = getattr(self, 'conv{}'.format(i))\n            x = layer(x)\n            # print(\"{}: {}\".format(i, x.size()))\n            if i==1:\n                x1 = x\n            elif i==3:\n                x2 = x\n            elif i==5:\n                x3 = x\n        for i in range(1,6):\n            layer = getattr(self, 'decode_conv{}'.format(i))\n            # print(\"{}a: {}\".format(i, x.size()))\n            x = layer(x)\n            # print(\"{}b: {}\".format(i, x.size()))\n            x = F.interpolate(x, scale_factor=2, mode='nearest')\n            if i==4:\n                x = torch.cat((x, x1), 1)\n            elif i==3:\n                x = torch.cat((x, x2), 1)\n            elif i==2:\n                x = torch.cat((x, x3), 1)\n            # print(\"{}c: {}\".format(i, x.size()))\n        x = self.decode_conv6(x)\n        return x\n    \n    def name(self):\n        return 'MobileNetSkipConcat'\n    \ndef flops(model,input_size):\n    input = torch.rand(1, 3, input_size, input_size).cuda()\n    flops, params = profile(model, inputs=(input, ))\n    from thop import clever_format\n    flops, params = clever_format([flops, params], \"%.3f\")\n    print(flops, params)\n    #import pdb;pdb.set_trace()\n\ndef FPS(model,input_size):\n    model.eval()\n\n    for x in range(0,200):\n        # input = torch.randn(1, 3, input_size, input_size).cuda()\n        input = torch.randn(1, 3, input_size, input_size).cuda()\n        with torch.no_grad():\n            # import ipdb;ipdb.set_trace()\n            output = model.forward(input)\n        \n    total=0\n    for x in range(0,200):\n        input = torch.randn(1, 3, input_size, input_size).cuda()\n        with torch.no_grad():\n            a = time.perf_counter()\n            output = model.forward(input)\n            torch.cuda.synchronize()\n            b = time.perf_counter()\n            total+=b-a\n    print('FPS:', str(200/total))\n    print('ms:', str(1000*total/200))\n    \nif __name__ == '__main__':\n\n    from thop import profile\n    # decoder = 'nnconv5dw'\n    decoder = 'nnconv5'\n    # decoder = 'deconv5'\n    # decoder = 'upproj'\n    # decoder = 'upconv'\n    model = MobileNet(decoder, 224, in_channels=3, pretrained=False)\n    # model = MobileNetSkipAdd(224, pretrained=False)\n    # model = MobileNetSkipConcat(224, pretrained=False)\n    model.cuda()\n    input_size = 224\n    # flops(model,input_size)\n    model_name = model.name()\n    print('==== model name: '+model_name+'   decoder: '+decoder+' =========')\n    FPS(model,input_size)\n", "VNL_loss.py": "import torch\nimport torch.nn as nn\nimport numpy as np\n\n\nclass VNL_Loss(nn.Module):\n    \"\"\"\n    Virtual Normal Loss Function.\n    \"\"\"\n    def __init__(self, input_size, focal_x=519, focal_y=519, \n                 delta_cos=0.867, delta_diff_x=0.01,\n                 delta_diff_y=0.01, delta_diff_z=0.01,\n                 delta_z=0.0001, sample_ratio=0.15):\n        super(VNL_Loss, self).__init__()\n        self.fx = torch.tensor([focal_x], dtype=torch.float32).cuda()  #\u76f8\u673ax\u65b9\u5411\u7126\u8ddd\uff0c\u4e0e\u6570\u636e\u96c6\u6709\u5173\n        self.fy = torch.tensor([focal_y], dtype=torch.float32).cuda()  #\u76f8\u673ay\u65b9\u5411\u7126\u8ddd\n        self.input_size = input_size  #\u8f93\u5165\u53d8\u91cf\u7684\u5927\u5c0f\n        self.u0 = torch.tensor(input_size[1] // 2, dtype=torch.float32).cuda()  #\u76f8\u673a\u5149\u5b66\u4e2d\u5fc3\uff0c\u7528\u8f93\u5165\u53d8\u91cf\u7684\u4e2d\u5fc3\u6765\u8fd1\u4f3c\uff0c\u8c03\u7528\u635f\u5931\u65f6\u8f93\u5165\u56fe\u50cf\u7684\u5927\u5c0f\u51b3\u5b9a\n        self.v0 = torch.tensor(input_size[0] // 2, dtype=torch.float32).cuda()\n        self.init_image_coor()\n        self.delta_cos = delta_cos\n        self.delta_diff_x = delta_diff_x\n        self.delta_diff_y = delta_diff_y\n        self.delta_diff_z = delta_diff_z\n        self.delta_z = delta_z\n        self.sample_ratio = sample_ratio\n\n    def init_image_coor(self):\n        x_row = np.arange(0, self.input_size[1])  #[0,1,2,...,self.input_size[1]-1]\n        x = np.tile(x_row, (self.input_size[0], 1))  #\u628ax_row\u5728\u5217\u4e0a\u590d\u5236self.input_size[0]\u6b21\uff0c\u884c\u4e0a\u590d\u52361\u6b21\n        x = x[np.newaxis, :, :]  #[1,self.input_size[0], self.input_size[1]]\n        x = x.astype(np.float32)\n        x = torch.from_numpy(x.copy()).cuda()\n        self.u_u0 = x #- self.u0  #\u8f93\u5165\u56fe\u50cf\u4e0a\u6bcf\u4e2a\u70b9\u7684\u4e8c\u7ef4\u6a2a\u5750\u6807\u51cf\u53bb\u5149\u5b66\u4e2d\u5fc3\u6a2a\u5750\u6807\n\n        y_col = np.arange(0, self.input_size[0])  # y_col = np.arange(0, height)\n        y = np.tile(y_col, (self.input_size[1], 1)).T  #[1,self.input_size[0], self.input_size[1]]\n        y = y[np.newaxis, :, :]\n        y = y.astype(np.float32)\n        y = torch.from_numpy(y.copy()).cuda()\n        self.v_v0 = y #- self.v0  #\u8f93\u5165\u56fe\u50cf\u4e0a\u6bcf\u4e2a\u70b9\u7684\u800c\u4e14\u7eb5\u5750\u6807\u51cf\u53bb\u5149\u5b66\u4e2d\u5fc3\u7eb5\u5750\u6807\n\n    def transfer_xyz(self, depth):\n        ##\u5229\u7528\u6df1\u5ea6\u56fe\u6784\u9020\u4e09\u7ef4\u70b9\u4e91\u5750\u6807\n        #import ipdb;ipdb.set_trace()\n        x = self.u_u0.repeat((depth.shape[0],1,1,1)) #* torch.abs(depth) / self.fx\n        y = self.v_v0.repeat((depth.shape[0],1,1,1))  #* torch.abs(depth) / self.fy\n        z = depth\n        pw = torch.cat([x, y, z], 1).permute(0, 2, 3, 1) # [b, h, w, c]\uff0cc=3\n        return pw\n\n    def select_index(self):\n        valid_width = self.input_size[1]\n        valid_height = self.input_size[0]\n        num = valid_width * valid_height  #\u8f93\u5165\u56fe\u50cf\u50cf\u7d20\u70b9\u6570\u76ee\n        p1 = np.random.choice(num, int(num * self.sample_ratio), replace=True)  #\u5728num\u4e2d\u968f\u673a\u9009\u62e9num * self.sample_ratio\u4e2a\u6570\u5b57\n        np.random.shuffle(p1)\n        p2 = np.random.choice(num, int(num * self.sample_ratio), replace=True)\n        np.random.shuffle(p2)\n        p3 = np.random.choice(num, int(num * self.sample_ratio), replace=True)\n        np.random.shuffle(p3)\n\n        p1_x = p1 % self.input_size[1]\n        p1_y = (p1 / self.input_size[1]).astype(np.int)  #p1\u968f\u673a\u9009\u62e9\u7684\u50cf\u7d20\u70b9\u7684\u5750\u6807\n\n        p2_x = p2 % self.input_size[1]\n        p2_y = (p2 / self.input_size[1]).astype(np.int)\n\n        p3_x = p3 % self.input_size[1]\n        p3_y = (p3 / self.input_size[1]).astype(np.int)\n        \n        ##\u9009\u62e9\u4e863\u7ec4\uff0c\u6bcf\u7ec4num * self.sample_ratio\u4e2a\u70b9\uff0c\u8fd4\u56de\u6bcf\u4e2a\u70b9\u7684x,y\u5750\u6807\n        p123 = {'p1_x': p1_x, 'p1_y': p1_y, 'p2_x': p2_x, 'p2_y': p2_y, 'p3_x': p3_x, 'p3_y': p3_y}\n        return p123\n\n    def form_pw_groups(self, p123, pw):\n        \"\"\"\n        Form 3D points groups, with 3 points in each grouup.\n        :param p123: points index\n        :param pw: 3D points\n        :return:\n        \"\"\"\n        p1_x = p123['p1_x']\n        p1_y = p123['p1_y']\n        p2_x = p123['p2_x']\n        p2_y = p123['p2_y']\n        p3_x = p123['p3_x']\n        p3_y = p123['p3_y']\n\n        pw1 = pw[:, p1_y, p1_x, :] #\u7b2c\u4e00\u7ec4\u70b9\u5728\u6574\u4e2abatch\u4e0a\u7684\u4e09\u7ef4\u5750\u6807\uff08x,y,z\uff09\n        pw2 = pw[:, p2_y, p2_x, :]\n        pw3 = pw[:, p3_y, p3_x, :]\n        # [B, N, 3(x,y,z), 3(p1,p2,p3)]\n        pw_groups = torch.cat([pw1[:, :, :, np.newaxis], pw2[:, :, :, np.newaxis], pw3[:, :, :, np.newaxis]], 3)\n        return pw_groups\n\n    def filter_mask(self, p123, gt_xyz, delta_cos=0.867,\n                    delta_diff_x=0.005,\n                    delta_diff_y=0.005,\n                    delta_diff_z=0.005):\n        pw = self.form_pw_groups(p123, gt_xyz)\n        #\u75283\u7ec4\u70b9\u6784\u9020\u5411\u91cf\n        pw12 = pw[:, :, :, 1] - pw[:, :, :, 0]  #\u7b2c\u4e00\u7ec4\u5411\u91cf\u7684\uff08x,y,z\uff09,[B, num * self.sample_ratio, 3(x,y,z)]\n        pw13 = pw[:, :, :, 2] - pw[:, :, :, 0]\n        pw23 = pw[:, :, :, 2] - pw[:, :, :, 1]\n        ###ignore linear\n        #\u6bcf\u5f20\u56fe\u4e0a\u9009\u62e9num * self.sample_ratio\u4e2a\u865a\u62df\u6cd5\u7ebf\uff0c\u6bcf\u4e2a\u5411\u91cf\u8868\u793a\u4e3a(x,y,z),\u6bcf\u4e2a\u865a\u62df\u6cd5\u7ebf\u8868\u793a\u4e3a3\u4e2a\u5411\u91cf\uff0c\n        pw_diff = torch.cat([pw12[:, :, :, np.newaxis], pw13[:, :, :, np.newaxis], pw23[:, :, :, np.newaxis]],\n                            3)  # [b, n, 3, 3]\n        m_batchsize, groups, coords, index = pw_diff.shape\n        proj_query = pw_diff.view(m_batchsize * groups, -1, index).permute(0, 2, 1)  # (B* X CX(3)) [bn, 3(p123), 3(xyz)]\n        proj_key = pw_diff.view(m_batchsize * groups, -1, index)  # B X  (3)*C [bn, 3(xyz), 3(p123)]\n        q_norm = proj_query.norm(2, dim=2) #\u6bcf\u4e2a\u5411\u91cf\u76842\u8303\u6570\uff0c[bn,3,1]\n        nm = torch.bmm(q_norm.view(m_batchsize * groups, index, 1), q_norm.view(m_batchsize * groups, 1, index)) #\u77e9\u9635\u4e58\u6cd5\uff0c[bn,3,3]\n        energy = torch.bmm(proj_query, proj_key)  # transpose check [bn, 3(p123), 3(p123)],\u5411\u91cf\u5185\u79ef\n        norm_energy = energy / (nm + 1e-8) #\u4f59\u5f26\uff0c\n        norm_energy = norm_energy.view(m_batchsize * groups, -1) #[bn,9],9\u8868\u793a\u5411\u91cf1\u5206\u522b\u4e0e\u5411\u91cf1,2,3\u7684\u4f59\u5f26\uff0c2\u5206\u522b\u4e0e1,2,3\u7684\u4f59\u5f26\uff0c3\u5206\u522b\u4e0e1,2,3\u7684\u4f59\u5f26\n        mask_cos = torch.sum((norm_energy > delta_cos) + (norm_energy < -delta_cos), 1) > 3  # igonre\uff0c3\u4e3a1\u4e0e1,2\u4e0e2,3\u4e0e3\u7684\u5939\u89d2\u4f59\u5f26\u503c\u7684\u548c\n        mask_cos = mask_cos.view(m_batchsize, groups)  #\u5728\u9009\u62e9\u7684groups\u4e2d\u4e0d\u6ee1\u8db3\u6761\u4ef6\u8981\u88ab\u5ffd\u7565\u7684\u90a3\u4e9b\n        ##ignore padding and invilid depth\n        mask_pad = torch.sum(pw[:, :, 2, :] > self.delta_z, 2) == 3  #\u4e00\u7ec4\u70b9\u7684\u4e09\u4e2a\u5411\u91cf\u7684\u6df1\u5ea6\u503c\u90fd\u5927\u4e8e0.001\n\n        ###ignore near\n        mask_x = torch.sum(torch.abs(pw_diff[:, :, 0, :]) < delta_diff_x, 2) > 0  #\u81f3\u5c11\u67091\u4e2a\u5411\u91cf\u7684x\u5c0f\u4e8e\u9608\u503c\u5373\u5411\u91cf\u7684\u8d77\u59cb\u70b9\u5728x\u4e0a\u79bb\u5f97\u8fd1\n        mask_y = torch.sum(torch.abs(pw_diff[:, :, 1, :]) < delta_diff_y, 2) > 0  #[b,n]\n        mask_z = torch.sum(torch.abs(pw_diff[:, :, 2, :]) < delta_diff_z, 2) > 0\n\n        mask_ignore = (mask_x & mask_y & mask_z) | mask_cos\n        mask_near = ~mask_ignore\n        mask = mask_pad & mask_near\n\n        return mask, pw  #mask\u4e2d1\u4e3a\u6709\u6548\u7684\u7ec4\n\n    def select_points_groups(self, gt_depth, pred_depth):\n        pw_gt = self.transfer_xyz(gt_depth)  #gt\u7684\u4e09\u7ef4\u70b9\u4e91\u5750\u6807\n        pw_pred = self.transfer_xyz(pred_depth)  #pred\u7684\u4e09\u7ef4\u70b9\u4e91\u5750\u6807\uff0c[B,H,W,C],C=(x,y,z)\n        B, C, H, W = gt_depth.shape\n        p123 = self.select_index()  #\u5b57\u5178\uff0c6\u4e2akey\uff0c\u5206\u522b\u4e3a\u7b2c1,2,3\u7ec4\u70b9\u7684x\uff0cy\u5750\u6807\uff0c\u6bcf\u7ec4\u5305\u62ecnum * self.sample_ratio\u4e2a\u70b9\n        # mask:[b, n], pw_groups_gt: [b, n, 3(x,y,z), 3(p1,p2,p3)]\n        ##mask\u4e2d1\u8868\u793a\u6709\u6548\u9009\u62e9\u7684\u8fd9\u7ec4\u70b9\uff0c\u53ef\u7528\u4e8e\u540e\u7eed\u8ba1\u7b97\n        mask, pw_groups_gt = self.filter_mask(p123, pw_gt,\n                                              delta_cos=0.867,\n                                              delta_diff_x=0.005,\n                                              delta_diff_y=0.005,\n                                              delta_diff_z=0.005)\n\n        # pred\u76843\u7ef4\u70b9\u4e91\u5750\u6807[b, n, 3\uff08x,y,z\uff09, 3(p1,p2,p3)]\uff0c\n        pw_groups_pred = self.form_pw_groups(p123, pw_pred)\n        pw_groups_pred[pw_groups_pred[:, :, 2, :] == 0] = 0.0001  #\u9884\u6d4b\u6df1\u5ea6\u4e3a0\u7684\u70b9\u6df1\u5ea6\u8d4b\u503c\u4e3a0.0001\n        mask_broadcast = mask.repeat(1, 9).reshape(B, 3, 3, -1).permute(0, 3, 1, 2) #[b*1,n*9]\u2192[b,3,3,n]\u2192[b,n,3,3]\n        pw_groups_pred_not_ignore = pw_groups_pred[mask_broadcast].reshape(1, -1, 3, 3)\n        pw_groups_gt_not_ignore = pw_groups_gt[mask_broadcast].reshape(1, -1, 3, 3)\n\n        return pw_groups_gt_not_ignore, pw_groups_pred_not_ignore\n\n    def forward(self, gt_depth, pred_depth, select=True):\n        \"\"\"\n        Virtual normal loss.\n        :param pred_depth: predicted depth map, [B,W,H,C]\n        :param data: target label, ground truth depth, [B, W, H, C], padding region [padding_up, padding_down]\n        :return:\n        \"\"\"\n        gt_points, dt_points = self.select_points_groups(gt_depth, pred_depth)  #\u6709\u6548\u53ef\u7528\u4e8e\u8ba1\u7b97\u7684\u70b9\n\n        gt_p12 = gt_points[:, :, :, 1] - gt_points[:, :, :, 0]  #gt\u4e2d\u6bcf\u7ec4\u6709\u6548\u70b9\u6784\u6210\u7684\u5411\u91cf\n        gt_p13 = gt_points[:, :, :, 2] - gt_points[:, :, :, 0]\n        dt_p12 = dt_points[:, :, :, 1] - dt_points[:, :, :, 0]\n        dt_p13 = dt_points[:, :, :, 2] - dt_points[:, :, :, 0]\n        \n        #\u6c42\u6cd5\u7ebf\n        gt_normal = torch.cross(gt_p12, gt_p13, dim=2)\n        dt_normal = torch.cross(dt_p12, dt_p13, dim=2)\n        dt_norm = torch.norm(dt_normal, 2, dim=2, keepdim=True)\n        gt_norm = torch.norm(gt_normal, 2, dim=2, keepdim=True)\n        dt_mask = dt_norm == 0.0\n        gt_mask = gt_norm == 0.0\n        dt_mask = dt_mask.to(torch.float32)\n        gt_mask = gt_mask.to(torch.float32)\n        dt_mask *= 0.01\n        gt_mask *= 0.01\n        gt_norm = gt_norm + gt_mask\n        dt_norm = dt_norm + dt_mask\n        gt_normal = gt_normal / gt_norm\n        dt_normal = dt_normal / dt_norm\n        \n        loss = torch.abs(gt_normal - dt_normal)\n        loss = torch.sum(torch.sum(loss, dim=2), dim=0)\n        if select:\n            loss, indices = torch.sort(loss, dim=0, descending=False)\n            loss = loss[int(loss.size(0) * 0.25):]\n        loss = torch.mean(loss)\n        return loss\n\n\nif __name__ == '__main__':\n    import cv2\n    vnl_loss = VNL_Loss(1.0, 1.0, (480, 640))\n    pred_depth = np.ones([2, 1, 480, 640])\n    gt_depth = np.ones([2, 1, 480, 640])\n    gt_depth = torch.tensor(np.asarray(gt_depth, np.float32)).cuda()\n    pred_depth = torch.tensor(np.asarray(pred_depth, np.float32)).cuda()\n    loss = vnl_loss.cal_VNL_loss(pred_depth, gt_depth)\n    print(loss)\n", "aspp.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Mon Nov 19 18:02:59 2018\n\n@author: Taha Emara  @email: taha@emaraic.com\n\"\"\"\nimport torch.nn as nn\nimport torch\nclass SeparableConv2d(nn.Module):\n    def __init__(self,in_channels,out_channels,kernel_size=1,stride=1,padding=0,dilation=1,bias=False):\n        super(SeparableConv2d,self).__init__()\n\n        self.conv1 = nn.Conv2d(in_channels,in_channels,kernel_size,stride,padding,dilation,groups=in_channels,bias=bias)\n        self.pointwise = nn.Conv2d(in_channels,out_channels,1,1,0,1,1,bias=bias)\n\n    def forward(self,x):\n        x = self.conv1(x)\n        x = self.pointwise(x)\n        return x\nclass ASPP(nn.Module):\n    \n    def __init__(self, inplanes, planes, rate, dw=False):\n        super(ASPP, self).__init__()\n        self.rate=rate\n        if rate == 1:\n            kernel_size = 1\n            padding = 0\n        else:\n            kernel_size = 3\n            padding = rate\n            if dw:\n                self.conv1 =SeparableConv2d(planes,planes,3,1,1)\n            else:\n                self.conv1 = nn.Conv2d(planes, planes, kernel_size=3, bias=False,padding=1)\n            self.bn1 = nn.BatchNorm2d(planes)\n            self.relu1 = nn.ReLU()\n   \n            #self.atrous_convolution = nn.Conv2d(inplanes, planes, kernel_size=kernel_size,\n            #                         stride=1, padding=padding, dilation=rate, bias=False)\n        self.atrous_convolution = SeparableConv2d(inplanes,planes,kernel_size,1,padding,rate)\n        self.bn = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU()\n        \n        \n        self._init_weight()\n\n    def forward(self, x):\n        x = self.atrous_convolution(x)\n        x = self.bn(x)\n        #x = self.relu(x)\n        if self.rate!=1:\n            x=self.conv1(x)\n            x=self.bn1(x)\n            x=self.relu1(x)\n        return x\n\n    def _init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                torch.nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\nclass SAP(nn.Module):\n    \n    def __init__(self, inplanes, planes):\n        super(SAP, self).__init__()\n\n        self.conv1 = SeparableConv2d(inplanes,planes,5,2,2)\n        self.conv2 = SeparableConv2d(inplanes,planes,9,4,4)\n        self.conv3 = SeparableConv2d(inplanes,planes,17,8,8)\n        self.conv4 = SeparableConv2d(inplanes,planes,33,16,16)\n        self.conv5 = SeparableConv2d(inplanes,planes,65,32,32)\n        \n        self.bn = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU()\n\n        #self.atrous_convolution = SeparableConv2d(inplanes,planes,kernel_size,1,padding,rate)\n        \n        \n        self._init_weight()\n\n    def forward(self, x):\n        x1 = self.conv1(x)\n        x1 = self.bn(x1)\n        x1 = self.relu(x1)\n        \n        x2 = self.conv2(x)\n        x2 = self.bn(x2)\n        x2 = self.relu(x2)\n\n        x3 = self.conv3(x)\n        x3 = self.bn(x3)\n        x3 = self.relu(x3)\n        '''\n        x4 = self.conv4(x)\n        x4 = self.bn(x4)\n        x4 = self.relu(x4)\n\n        x5 = self.conv1(x)\n        x5 = self.bn(x5)\n        x5 = self.relu(x5)\n        '''\n        return x1, x2, x3\n\n    def _init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                torch.nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n", "aspp_prune.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Mon Nov 19 18:02:59 2018\n\n@author: Taha Emara  @email: taha@emaraic.com\n\"\"\"\nimport torch.nn as nn\nimport torch\nclass SeparableConv2d(nn.Module):\n    def __init__(self,in_channels,out_channels,kernel_size=1,stride=1,padding=0,dilation=1,bias=False):\n        super(SeparableConv2d,self).__init__()\n\n        self.conv1 = nn.Conv2d(in_channels,in_channels,kernel_size,stride,padding,dilation,groups=in_channels,bias=bias)\n        self.pointwise = nn.Conv2d(in_channels,out_channels,1,1,0,1,1,bias=bias)\n\n    def forward(self,x):\n        x = self.conv1(x)\n        x = self.pointwise(x)\n        return x\nclass ASPP(nn.Module):\n    \n    def __init__(self, inplanes, planes, rate, dw=False):\n        super(ASPP, self).__init__()\n        self.rate=rate\n        if rate == 1:\n            kernel_size = 1\n            padding = 0\n        else:\n            kernel_size = 3\n            padding = rate\n            if dw:\n                self.conv1 =SeparableConv2d(planes,planes,3,1,1)\n            else:\n                self.conv1 = nn.Conv2d(planes, planes, kernel_size=3, bias=False,padding=1)\n            self.bn1 = nn.BatchNorm2d(planes)\n            self.relu1 = nn.ReLU()\n   \n            #self.atrous_convolution = nn.Conv2d(inplanes, planes, kernel_size=kernel_size,\n            #                         stride=1, padding=padding, dilation=rate, bias=False)\n        self.atrous_convolution = SeparableConv2d(inplanes,planes,kernel_size,1,padding,rate)\n        self.bn = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU()\n        \n        \n        self._init_weight()\n\n    def forward(self, x):\n        x = self.atrous_convolution(x)\n        x = self.bn(x)\n        #x = self.relu(x)\n        if self.rate!=1:\n            x=self.conv1(x)\n            x=self.bn1(x)\n            x=self.relu1(x)\n        return x\n\n    def _init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                torch.nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\nclass SAP(nn.Module):\n    \n    def __init__(self, inplanes, planes):\n        super(SAP, self).__init__()\n\n        self.conv1 = SeparableConv2d(inplanes,planes,5,2,2)\n        self.conv2 = SeparableConv2d(inplanes,planes,9,4,4)\n        self.conv3 = SeparableConv2d(inplanes,planes,17,8,8)\n        self.conv4 = SeparableConv2d(inplanes,planes,33,16,16)\n        self.conv5 = SeparableConv2d(inplanes,planes,65,32,32)\n        \n        self.bn = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU()\n\n        #self.atrous_convolution = SeparableConv2d(inplanes,planes,kernel_size,1,padding,rate)\n        \n        \n        self._init_weight()\n\n    def forward(self, x):\n        x1 = self.conv1(x)\n        x1 = self.bn(x1)\n        x1 = self.relu(x1)\n        \n        x2 = self.conv2(x)\n        x2 = self.bn(x2)\n        x2 = self.relu(x2)\n\n        x3 = self.conv3(x)\n        x3 = self.bn(x3)\n        x3 = self.relu(x3)\n        '''\n        x4 = self.conv4(x)\n        x4 = self.bn(x4)\n        x4 = self.relu(x4)\n\n        x5 = self.conv1(x)\n        x5 = self.bn(x5)\n        x5 = self.relu(x5)\n        '''\n        return x1, x2, x3\n\n    def _init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                torch.nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n", "fast_mobilenetv1_aspp_prune.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Sun Dec 16 11:20:32 2018\n\n@author: Taha Emara  @email: taha@emaraic.com\n\"\"\"\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\nimport math\nimport sys\nimport os\nsys.path.append(os.path.abspath('./'))\n# from models.backbone_networks import MobileNetV2_prune as MobileNetV2\nfrom models.backbone_networks import MobileNetV1\nfrom models import aspp\nfrom models.separableconv import SeparableConv2d \n\n\ndef weights_init(m):\n    # Initialize kernel weights with Gaussian distributions\n    if isinstance(m, nn.Conv2d):\n        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        m.weight.data.normal_(0, math.sqrt(2. / n))\n        if m.bias is not None:\n            m.bias.data.zero_()\n    elif isinstance(m, nn.ConvTranspose2d):\n        n = m.kernel_size[0] * m.kernel_size[1] * m.in_channels\n        m.weight.data.normal_(0, math.sqrt(2. / n))\n        if m.bias is not None:\n            m.bias.data.zero_()\n    elif isinstance(m, nn.BatchNorm2d):\n        m.weight.data.fill_(1)\n        m.bias.data.zero_()\n\n\n\n\nclass RT(nn.Module):\n    \n    def __init__(self, n_classes=59, pretrained=False):\n        \n        super(RT, self).__init__()\n        print(\"PDESNet-MobileNet...\")\n\n        # self.mobile_features=MobileNetV2.MobileNetV2()\n        # self.mobile_features=MobileNetV1.MobileNet_res()\n        self.mobile_features=MobileNetV1.MobileNet()\n        \n        if pretrained:\n            print('Loading pretrained model: model_best.pth.tar...')\n            pretrained_path = os.path.join('pretrained_model', 'model_best.pth.tar')\n            checkpoint = torch.load(pretrained_path)\n            state_dict = checkpoint['state_dict']\n\n            from collections import OrderedDict\n            new_state_dict = OrderedDict()\n            for k, v in state_dict.items():\n                name = k[7:] # remove `module.`\n                new_state_dict[name] = v\n            self.mobile_features.load_state_dict(new_state_dict)\n        else:\n            self.mobile_features.apply(weights_init)\n        \n        \n        # rates = [1, 3, 6, 9]\n        rates = [1, 5, 9]\n\n        x_c = 1024\n        out_c = 96\n        low_level_c = 128\n        self.aspp1 = aspp.ASPP(x_c, out_c, rate=rates[0], dw=True)\n        self.aspp2 = aspp.ASPP(x_c, out_c, rate=rates[1], dw=True)\n        self.aspp3 = aspp.ASPP(x_c, out_c, rate=rates[2], dw=True)\n        # self.aspp4 = aspp.ASPP(x_c, out_c, rate=rates[3], dw=True)\n\n        self.relu_prune = nn.ReLU()\n        self.global_avg_pool_prune = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n                                             nn.Conv2d(x_c, out_c, 1, stride=1, bias=False),\n                                             nn.BatchNorm2d(out_c),\n                                             nn.ReLU())\n        #self.conv1 = nn.Conv2d(480+1280, 96, 1, bias=False)\n        # self.conv1_prune =SeparableConv2d(x_c+out_c*5,out_c,1)\n        self.conv1_prune =SeparableConv2d(x_c+out_c*3,out_c,1)\n        self.bn1_prune = nn.BatchNorm2d(out_c)\n\n        #adopt [1x1, 48] for channel reduction.\n        #self.conv2 = nn.Conv2d(24, 32, 1, bias=False)\n        #self.bn2 = nn.BatchNorm2d(32)\n    \n        self.last_conv_prune1 = nn.Sequential(\n                                       SeparableConv2d(low_level_c+out_c,out_c,3,1,1),\n                                       nn.BatchNorm2d(out_c),\n                                       nn.ReLU())\n                                       \n        self.last_conv_prune2 = nn.Sequential(\n                                       SeparableConv2d(out_c,out_c,3,1,1),\n                                       nn.BatchNorm2d(out_c),\n                                       nn.ReLU())\n        \n        self.deep_prune = nn.Conv2d(out_c, 1, kernel_size=1, stride=1)\n        self.class_prune = nn.Conv2d(out_c, n_classes, kernel_size=1, stride=1)\n        \n    def trunk(self, input, input_size):\n        x, low_level_features = self.mobile_features(input)\n        # x: (b,1024,input_h/32,input_h/32)\n        # low_level_features: (b, 128, input_h/4, input_w/4)\n        # import ipdb; ipdb.set_trace()\n        x1 = self.aspp1(x) #(b,96,input_h/32,input_h/32)\n        x2 = self.aspp2(x) #(b,96,input_h/32,input_h/32)\n        x3 = self.aspp3(x) #(b,96,input_h/32,input_h/32)\n        # x4 = self.aspp4(x) #(b,96,input_h/32,input_h/32)\n        # x5 = self.global_avg_pool_prune(x) #(b,96,1,1)\n        # x5 = F.interpolate(x5, size=(int(math.ceil(input_size[-2]/32)),\n                                # int(math.ceil(input_size[-1]/32))), mode='bilinear', align_corners=True) #(b,96,input_h/32,input_h/32)\n\n        # x = torch.cat((x,x1, x2, x3, x4, x5), dim=1) #(b,1760,input_h/32,input_h/32)\n        # x = torch.cat((x,x1, x2, x3, x5), dim=1) #(b,288,input_h/32,input_h/32)\n        x = torch.cat((x,x1, x2, x3), dim=1) #(b,288,input_h/32,input_h/32)\n        #print('after aspp cat',x.size())\n        x = self.conv1_prune(x) #(b,96,input_h/32,input_h/32)\n        x = self.bn1_prune(x)\n        x = self.relu_prune(x)\n        x = F.interpolate(x, size=(int(math.ceil(input_size[-2]/4)),\n                                int(math.ceil(input_size[-1]/4))), mode='bilinear', align_corners=True) # (b, 96, input_h/4, input_w/4)\n       # ablation=torch.max(low_level_features, 1)[1]\n        #print('after con on aspp output',x.size())\n\n        ##comment to remove low feature\n        #low_level_features = self.conv2(low_level_features)\n        #low_level_features = self.bn2(low_level_features)\n        #low_level_features = self.relu(low_level_features)\n        #print(\"low\",low_level_features.size())\n        \n        x = torch.cat((x, low_level_features), dim=1) #(b, 120, input_h/4, input_w/4)\n        #print('after cat low feature with output of aspp',x.size())\n        x = self.last_conv_prune1(x) #(b, 96, input_h/4, input_w/4)\n        return x\n        \n    def head(self, x, input_size):\n    \n        depth = self.last_conv_prune2(x) #(b, 96, input_h/4, input_w/4)\n        depth = self.deep_prune(depth) #(b, 1, input_h/4, input_w/4)\n        depth = F.interpolate(depth, size=(int(math.ceil(input_size[-2]/2)),\n                                int(math.ceil(input_size[-1]/2))), mode='bilinear', align_corners=True) #(b, 1, input_h/2, input_w/2)\n\n        class_mask = self.last_conv_prune2(x) #(b, 96, input_h/4, input_w/4)\n        class_mask = self.class_prune(class_mask) #(b, 1, input_h/4, input_w/4)\n        class_mask = F.interpolate(class_mask, size=(int(math.ceil(input_size[-2]/2)),\n                                int(math.ceil(input_size[-1]/2))), mode='bilinear', align_corners=True) #(b, n_classes, input_h/2, input_w/2)\n                                \n        return depth, class_mask\n        \n    def forward(self, input):\n        #import ipdb;ipdb.set_trace()\n        input_size = input.size()\n        \n        x = self.trunk(input, input_size)\n        \n        depth, class_mask = self.head(x, input_size)\n        \n        return x#{'depth': depth, 'class_mask': class_mask}\n\n    def freeze_bn(self):\n        for m in self.modules():\n            if isinstance(m, nn.BatchNorm2d):\n                m.eval()\n\n    def __init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                torch.nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n                \ndef flops(model,input_size):\n    input = torch.rand(1, 3, input_size, input_size).cuda()\n    flops, params = profile(model, inputs=(input, ))\n    from thop import clever_format\n    flops, params = clever_format([flops, params], \"%.3f\")\n    print(flops, params)\n    #import pdb;pdb.set_trace()\n\ndef FPS(model,input_size):\n    import time\n    model.eval()\n\n    for x in range(0,200):\n        input = torch.randn(1, 3, input_size, input_size).cuda()\n        with torch.no_grad():\n            # import ipdb;ipdb.set_trace()\n            output = model.forward(input)\n        \n    total=0\n    for x in range(0,200):\n        input = torch.randn(1, 3, input_size, input_size).cuda()\n        with torch.no_grad():\n            a = time.perf_counter()\n            output = model.forward(input)\n            torch.cuda.synchronize()\n            b = time.perf_counter()\n            total+=b-a\n    print('FPS:', str(200/total))\n    print('ms:', str(1000*total/200))\n    \nif __name__ == '__main__':\n\n    from thop import profile\n    model = RT(n_classes=59, pretrained=False)\n    model.cuda()\n    input_size = 224\n    # flops(model,input_size)\n    FPS(model,input_size)\n", "Espnet.py": "import torch\nimport torch.nn as nn\n\n__author__ = \"Sachin Mehta\"\n\nclass CBR(nn.Module):\n    '''\n    This class defines the convolution layer with batch normalization and PReLU activation\n    '''\n    def __init__(self, nIn, nOut, kSize, stride=1):\n        '''\n\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param kSize: kernel size\n        :param stride: stride rate for down-sampling. Default is 1\n        '''\n        super().__init__()\n        padding = int((kSize - 1)/2)\n        #self.conv = nn.Conv2d(nIn, nOut, kSize, stride=stride, padding=padding, bias=False)\n        self.conv = nn.Conv2d(nIn, nOut, (kSize, kSize), stride=stride, padding=(padding, padding), bias=False)\n        #self.conv1 = nn.Conv2d(nOut, nOut, (1, kSize), stride=1, padding=(0, padding), bias=False)\n        self.bn = nn.BatchNorm2d(nOut, eps=1e-03)\n        self.act = nn.PReLU(nOut)\n\n    def forward(self, input):\n        '''\n        :param input: input feature map\n        :return: transformed feature map\n        '''\n        output = self.conv(input)\n        #output = self.conv1(output)\n        output = self.bn(output)\n        output = self.act(output)\n        return output\n\n\nclass BR(nn.Module):\n    '''\n        This class groups the batch normalization and PReLU activation\n    '''\n    def __init__(self, nOut):\n        '''\n        :param nOut: output feature maps\n        '''\n        super().__init__()\n        self.bn = nn.BatchNorm2d(nOut, eps=1e-03)\n        self.act = nn.PReLU(nOut)\n\n    def forward(self, input):\n        '''\n        :param input: input feature map\n        :return: normalized and thresholded feature map\n        '''\n        output = self.bn(input)\n        output = self.act(output)\n        return output\n\nclass CB(nn.Module):\n    '''\n       This class groups the convolution and batch normalization\n    '''\n    def __init__(self, nIn, nOut, kSize, stride=1):\n        '''\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param kSize: kernel size\n        :param stride: optinal stide for down-sampling\n        '''\n        super().__init__()\n        padding = int((kSize - 1)/2)\n        self.conv = nn.Conv2d(nIn, nOut, (kSize, kSize), stride=stride, padding=(padding, padding), bias=False)\n        self.bn = nn.BatchNorm2d(nOut, eps=1e-03)\n\n    def forward(self, input):\n        '''\n\n        :param input: input feature map\n        :return: transformed feature map\n        '''\n        output = self.conv(input)\n        output = self.bn(output)\n        return output\n\nclass C(nn.Module):\n    '''\n    This class is for a convolutional layer.\n    '''\n    def __init__(self, nIn, nOut, kSize, stride=1):\n        '''\n\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param kSize: kernel size\n        :param stride: optional stride rate for down-sampling\n        '''\n        super().__init__()\n        padding = int((kSize - 1)/2)\n        self.conv = nn.Conv2d(nIn, nOut, (kSize, kSize), stride=stride, padding=(padding, padding), bias=False)\n\n    def forward(self, input):\n        '''\n        :param input: input feature map\n        :return: transformed feature map\n        '''\n        output = self.conv(input)\n        return output\n\nclass CDilated(nn.Module):\n    '''\n    This class defines the dilated convolution.\n    '''\n    def __init__(self, nIn, nOut, kSize, stride=1, d=1):\n        '''\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param kSize: kernel size\n        :param stride: optional stride rate for down-sampling\n        :param d: optional dilation rate\n        '''\n        super().__init__()\n        padding = int((kSize - 1)/2) * d\n        self.conv = nn.Conv2d(nIn, nOut, (kSize, kSize), stride=stride, padding=(padding, padding), bias=False, dilation=d)\n\n    def forward(self, input):\n        '''\n        :param input: input feature map\n        :return: transformed feature map\n        '''\n        output = self.conv(input)\n        return output\n\nclass DownSamplerB(nn.Module):\n    def __init__(self, nIn, nOut):\n        super().__init__()\n        n = int(nOut/5)\n        n1 = nOut - 4*n\n        self.c1 = C(nIn, n, 3, 2)\n        self.d1 = CDilated(n, n1, 3, 1, 1)\n        self.d2 = CDilated(n, n, 3, 1, 2)\n        self.d4 = CDilated(n, n, 3, 1, 4)\n        self.d8 = CDilated(n, n, 3, 1, 8)\n        self.d16 = CDilated(n, n, 3, 1, 16)\n        self.bn = nn.BatchNorm2d(nOut, eps=1e-3)\n        self.act = nn.PReLU(nOut)\n\n    def forward(self, input):\n        output1 = self.c1(input)\n        d1 = self.d1(output1)\n        d2 = self.d2(output1)\n        d4 = self.d4(output1)\n        d8 = self.d8(output1)\n        d16 = self.d16(output1)\n\n        add1 = d2\n        add2 = add1 + d4\n        add3 = add2 + d8\n        add4 = add3 + d16\n\n        combine = torch.cat([d1, add1, add2, add3, add4],1)\n        #combine_in_out = input + combine\n        output = self.bn(combine)\n        output = self.act(output)\n        return output\n\nclass DilatedParllelResidualBlockB(nn.Module):\n    '''\n    This class defines the ESP block, which is based on the following principle\n        Reduce ---> Split ---> Transform --> Merge\n    '''\n    def __init__(self, nIn, nOut, add=True):\n        '''\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param add: if true, add a residual connection through identity operation. You can use projection too as\n                in ResNet paper, but we avoid to use it if the dimensions are not the same because we do not want to\n                increase the module complexity\n        '''\n        super().__init__()\n        n = int(nOut/5)\n        n1 = nOut - 4*n\n        self.c1 = C(nIn, n, 1, 1)\n        self.d1 = CDilated(n, n1, 3, 1, 1) # dilation rate of 2^0\n        self.d2 = CDilated(n, n, 3, 1, 2) # dilation rate of 2^1\n        self.d4 = CDilated(n, n, 3, 1, 4) # dilation rate of 2^2\n        self.d8 = CDilated(n, n, 3, 1, 8) # dilation rate of 2^3\n        self.d16 = CDilated(n, n, 3, 1, 16) # dilation rate of 2^4\n        self.bn = BR(nOut)\n        self.add = add\n\n    def forward(self, input):\n        '''\n        :param input: input feature map\n        :return: transformed feature map\n        '''\n        # reduce\n        output1 = self.c1(input)\n        # split and transform\n        d1 = self.d1(output1)\n        d2 = self.d2(output1)\n        d4 = self.d4(output1)\n        d8 = self.d8(output1)\n        d16 = self.d16(output1)\n\n        # heirarchical fusion for de-gridding\n        add1 = d2\n        add2 = add1 + d4\n        add3 = add2 + d8\n        add4 = add3 + d16\n\n        #merge\n        combine = torch.cat([d1, add1, add2, add3, add4], 1)\n\n        # if residual version\n        if self.add:\n            combine = input + combine\n        output = self.bn(combine)\n        return output\n\nclass InputProjectionA(nn.Module):\n    '''\n    This class projects the input image to the same spatial dimensions as the feature map.\n    For example, if the input image is 512 x512 x3 and spatial dimensions of feature map size are 56x56xF, then\n    this class will generate an output of 56x56x3\n    '''\n    def __init__(self, samplingTimes):\n        '''\n        :param samplingTimes: The rate at which you want to down-sample the image\n        '''\n        super().__init__()\n        self.pool = nn.ModuleList()\n        for i in range(0, samplingTimes):\n            #pyramid-based approach for down-sampling\n            self.pool.append(nn.AvgPool2d(3, stride=2, padding=1))\n\n    def forward(self, input):\n        '''\n        :param input: Input RGB Image\n        :return: down-sampled image (pyramid-based approach)\n        '''\n        for pool in self.pool:\n            input = pool(input)\n        return input\n\n\nclass ESPNet_Encoder(nn.Module):\n    '''\n    This class defines the ESPNet-C network in the paper\n    '''\n    def __init__(self, classes=59, p=2, q=8):\n        '''\n        :param classes: number of classes in the dataset. Default is 20 for the cityscapes\n        :param p: depth multiplier\n        :param q: depth multiplier\n        '''\n        super().__init__()\n        self.level1 = CBR(3, 16, 3, 2)\n        self.sample1 = InputProjectionA(1)\n        self.sample2 = InputProjectionA(2)\n\n        self.b1 = BR(16 + 3)\n        self.level2_0 = DownSamplerB(16 +3, 64)\n\n        self.level2 = nn.ModuleList()\n        for i in range(0, p):\n            self.level2.append(DilatedParllelResidualBlockB(64 , 64))\n        self.b2 = BR(128 + 3)\n\n        self.level3_0 = DownSamplerB(128 + 3, 128)\n        self.level3 = nn.ModuleList()\n        for i in range(0, q):\n            self.level3.append(DilatedParllelResidualBlockB(128 , 128))\n        self.b3 = BR(256)\n\n        self.up = nn.Sequential(nn.ConvTranspose2d(256, 256, 2, stride=2, padding=0, output_padding=0, bias=False), BR(256))\n        #self.semantic = C(256, classes, 1, 1)\n        #self.depth = C(256, 1, 1, 1)\n        self.semantic = nn.ConvTranspose2d(256, classes, 2, stride=2, padding=0, output_padding=0, bias=False)\n        self.depth = nn.ConvTranspose2d(256, 1, 2, stride=2, padding=0, output_padding=0, bias=False)\n\n    def forward(self, input):\n        '''\n        :param input: Receives the input RGB image\n        :return: the transformed feature map with spatial dimensions 1/8th of the input image\n        '''\n        output0 = self.level1(input)\n        inp1 = self.sample1(input)\n        inp2 = self.sample2(input)\n\n        output0_cat = self.b1(torch.cat([output0, inp1], 1))\n        output1_0 = self.level2_0(output0_cat) # down-sampled\n        \n        for i, layer in enumerate(self.level2):\n            if i==0:\n                output1 = layer(output1_0)\n            else:\n                output1 = layer(output1)\n\n        output1_cat = self.b2(torch.cat([output1,  output1_0, inp2], 1))\n\n        output2_0 = self.level3_0(output1_cat) # down-sampled\n        for i, layer in enumerate(self.level3):\n            if i==0:\n                output2 = layer(output2_0)\n            else:\n                output2 = layer(output2)\n\n        output2_cat = self.b3(torch.cat([output2_0, output2], 1))\n\n        output2_cat = self.up(output2_cat)\n        semantic = self.semantic(output2_cat)\n        depth = self.depth(output2_cat)\n        # print(semantic.shape)\n        # print(depth.shape)\n        return {'depth': depth, 'class_mask': semantic}\n        \nclass ESPNet(nn.Module):\n    '''\n    This class defines the ESPNet network\n    '''\n\n    def __init__(self, classes=20, p=2, q=3, encoderFile=None):\n        '''\n        :param classes: number of classes in the dataset. Default is 20 for the cityscapes\n        :param p: depth multiplier\n        :param q: depth multiplier\n        :param encoderFile: pretrained encoder weights. Recall that we first trained the ESPNet-C and then attached the\n                            RUM-based light weight decoder. See paper for more details.\n        '''\n        super().__init__()\n        self.encoder = ESPNet_Encoder(classes, p, q)\n        if encoderFile != None:\n            self.encoder.load_state_dict(torch.load(encoderFile))\n            print('Encoder loaded!')\n        # load the encoder modules\n        self.modules = []\n        for i, m in enumerate(self.encoder.children()):\n            self.modules.append(m)\n\n        # light-weight decoder\n        self.level3_C = C(128 + 3, classes, 1, 1)\n        self.br = nn.BatchNorm2d(classes, eps=1e-03)\n        self.br_depth = nn.BatchNorm2d(1, eps=1e-03)\n        self.conv = CBR(19 + classes, classes, 3, 1)\n\n        self.up_l3 = nn.Sequential(nn.ConvTranspose2d(classes, classes, 2, stride=2, padding=0, output_padding=0, bias=False))\n        self.up_l3_depth = nn.Sequential(nn.ConvTranspose2d(1, 1, 2, stride=2, padding=0, output_padding=0, bias=False))\n        self.combine_l2_l3 = nn.Sequential(BR(2*classes), DilatedParllelResidualBlockB(2*classes , classes, add=False))\n\n        self.up_l2 = nn.Sequential(nn.ConvTranspose2d(classes, classes, 2, stride=2, padding=0, output_padding=0, bias=False), BR(classes))\n\n        self.semantic = nn.ConvTranspose2d(classes, classes, 2, stride=2, padding=0, output_padding=0, bias=False)\n        self.depth = nn.ConvTranspose2d(classes, classes, 2, stride=2, padding=0, output_padding=0, bias=False)\n        \n    def forward(self, input):\n        '''\n        :param input: RGB image\n        :return: transformed feature map\n        '''\n        output0 = self.modules[0](input)\n        inp1 = self.modules[1](input)\n        inp2 = self.modules[2](input)\n\n        output0_cat = self.modules[3](torch.cat([output0, inp1], 1))\n        output1_0 = self.modules[4](output0_cat)  # down-sampled\n\n        for i, layer in enumerate(self.modules[5]):\n            if i == 0:\n                output1 = layer(output1_0)\n            else:\n                output1 = layer(output1)\n\n        output1_cat = self.modules[6](torch.cat([output1, output1_0, inp2], 1))\n\n        output2_0 = self.modules[7](output1_cat)  # down-sampled (1, 128, h/8, w/8)\n        for i, layer in enumerate(self.modules[8]):\n            if i == 0:\n                output2 = layer(output2_0)\n            else:\n                output2 = layer(output2) #(1, 128, h/8, w/8)\n\n        output2_cat = self.modules[9](torch.cat([output2_0, output2], 1)) # (1, 256, h/8, w/8)\n        # import ipdb; ipdb.set_trace()\n        # self.modules[10](output2_cat): (1, 59, h/8, w/8)\n        # self.br(self.modules[10](output2_cat)): (1, 59, h/8, w/8)\n        output2_c = self.up_l3(self.br(self.modules[10](output2_cat))) # (1, 59, h/4, w/4)\n        output2_depth = self.up_l3_depth(self.br_depth(self.modules[11](output2_cat)))\n\n        output1_C = self.level3_C(output1_cat) # project to C-dimensional space\n        comb_l2_l3 = self.up_l2(self.combine_l2_l3(torch.cat([output1_C, output2_c], 1))) #RUM\n\n        concat_features = self.conv(torch.cat([comb_l2_l3, output0_cat], 1))\n\n        # classifier = self.classifier(concat_features)\n        return concat_features\n\ndef flops(model,input_size):\n    input = torch.rand(1, 3, input_size, input_size).cuda()\n    flops, params = profile(model, inputs=(input, ))\n    from thop import clever_format\n    flops, params = clever_format([flops, params], \"%.3f\")\n    print(flops, params)\n    #import pdb;pdb.set_trace()\n\ndef FPS(model,input_size):\n    import time\n    model.eval()\n\n    for x in range(0,200):\n        input = torch.randn(1, 3, input_size, input_size).cuda()\n        with torch.no_grad():\n            # import ipdb;ipdb.set_trace()\n            output = model.forward(input)\n        \n    total=0\n    for x in range(0,200):\n        input = torch.randn(1, 3, input_size, input_size).cuda()\n        with torch.no_grad():\n            a = time.perf_counter()\n            output = model.forward(input)\n            torch.cuda.synchronize()\n            b = time.perf_counter()\n            total+=b-a\n    print('FPS:', str(200/total))\n    print('ms:', str(1000*total/200))\n    \nif __name__ == '__main__':\n\n    from thop import profile\n    model = ESPNet_Encoder(classes=59)\n    # model = ESPNet(classes=59)\n    model.cuda()\n    input_size = 224\n    # flops(model,input_size)\n    FPS(model,input_size)\n", "pdesnet.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Sun Jan 13 12:00:52 2019\n\n@author: Taha Emara  @email: taha@emaraic.com\n\"\"\"\nimport torch \n\nfrom models import fast_mobilenetv1_aspp, fast_mobilenetv1_aspp_prune, fast_mobilenetv1_no_aspp, fast_mobilenetv1_aspp_maskinput\nfrom models import fast_mobilenetv1_aspp_conv\nfrom models import fastdepth, Espnet, Espnet_yangmei\n\n\n\nclass PDESNet():\n    \n    def build(backbone_network,n_classes,CONFIG=None,is_train=True):\n    \n        if backbone_network == 'fast_mobilenetv1_aspp':\n            net = fast_mobilenetv1_aspp.RT(n_classes=n_classes)\n        elif backbone_network == 'fast_mobilenetv1_aspp_prune':\n            net = fast_mobilenetv1_aspp_prune.RT(n_classes=n_classes)\n        elif backbone_network == 'fast_mobilenetv1_no_aspp':\n            net = fast_mobilenetv1_no_aspp.RT(n_classes=n_classes)\n        elif backbone_network == 'fast_mobilenetv1_aspp_conv':\n            net = fast_mobilenetv1_aspp_conv.RT(n_classes=n_classes)\n        elif backbone_network == 'fast_mobilenetv1_aspp_maskinput':\n            net = fast_mobilenetv1_aspp_maskinput.RT(n_classes=n_classes)\n        elif backbone_network == 'ESPNet_Encoder':\n            net = Espnet.ESPNet_Encoder(classes=n_classes)\n        elif backbone_network == 'ESPNet_Encoder_yangmei':\n            net = Espnet_yangmei.ESPNet_Encoder(classes=n_classes)\n        elif backbone_network == 'fastdepth':\n            decoder = 'nnconv5'\n            net = fastdepth.MobileNet(decoder, 224, in_channels=3)\n        else:\n            raise NotImplementedError\n            \n        #if modelpath is not None:\n            #net.load_state_dict(torch.load(modelpath)['state_dict'])\n            \n        print(\"Using PDESNet with\",backbone_network)\n        return net\n        \n            \n    \n\n", "fast_mobilenetv1_aspp_conv.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Sun Dec 16 11:20:32 2018\n\n@author: Taha Emara  @email: taha@emaraic.com\n\"\"\"\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\nimport math\nimport sys\nimport os\nsys.path.append(os.path.abspath('./'))\n# from models.backbone_networks import MobileNetV2_prune as MobileNetV2\nfrom models.backbone_networks import MobileNetV1\nfrom models import aspp\nfrom models.separableconv import SeparableConv2d \n\n\ndef weights_init(m):\n    # Initialize kernel weights with Gaussian distributions\n    if isinstance(m, nn.Conv2d):\n        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        m.weight.data.normal_(0, math.sqrt(2. / n))\n        if m.bias is not None:\n            m.bias.data.zero_()\n    elif isinstance(m, nn.ConvTranspose2d):\n        n = m.kernel_size[0] * m.kernel_size[1] * m.in_channels\n        m.weight.data.normal_(0, math.sqrt(2. / n))\n        if m.bias is not None:\n            m.bias.data.zero_()\n    elif isinstance(m, nn.BatchNorm2d):\n        m.weight.data.fill_(1)\n        m.bias.data.zero_()\n\n\n\n\nclass RT(nn.Module):\n    \n    def __init__(self, n_classes=19, pretrained=False):\n        \n        super(RT, self).__init__()\n        print(\"PDESNet-MobileNet...\")\n\n        # self.mobile_features=MobileNetV2.MobileNetV2()\n        # self.mobile_features=MobileNetV1.MobileNet_res()\n        self.mobile_features=MobileNetV1.MobileNet()\n        \n        if pretrained:\n            print('Loading pretrained model: model_best.pth.tar...')\n            pretrained_path = os.path.join('pretrained_model', 'model_best.pth.tar')\n            checkpoint = torch.load(pretrained_path)\n            state_dict = checkpoint['state_dict']\n\n            from collections import OrderedDict\n            new_state_dict = OrderedDict()\n            for k, v in state_dict.items():\n                name = k[7:] # remove `module.`\n                new_state_dict[name] = v\n            self.mobile_features.load_state_dict(new_state_dict)\n        else:\n            self.mobile_features.apply(weights_init)\n        \n        \n        rates = [1, 3, 6, 9]\n        # rates = [1, 5, 9]\n\n        x_c = 1024\n        out_c = 96\n        low_level_c = 128\n        # self.aspp1 = aspp.ASPP(x_c, out_c, rate=rates[0], dw=True)\n        # self.aspp2 = aspp.ASPP(x_c, out_c, rate=rates[1], dw=True)\n        # self.aspp3 = aspp.ASPP(x_c, out_c, rate=rates[2], dw=True)\n        # self.aspp4 = aspp.ASPP(x_c, out_c, rate=rates[3], dw=True)\n        \n        self.aspp1 = aspp.ASPP(x_c, out_c, rate=rates[0], dw=False)\n        self.aspp2 = aspp.ASPP(x_c, out_c, rate=rates[1], dw=False)\n        self.aspp3 = aspp.ASPP(x_c, out_c, rate=rates[2], dw=False)\n        self.aspp4 = aspp.ASPP(x_c, out_c, rate=rates[3], dw=False)\n\n        self.relu_prune = nn.ReLU()\n        self.global_avg_pool_prune = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n                                             nn.Conv2d(x_c, out_c, 1, stride=1, bias=False),\n                                             nn.BatchNorm2d(out_c),\n                                             nn.ReLU())\n        self.conv1_prune = nn.Conv2d(x_c+out_c*5, out_c, 1, bias=False)\n        # self.conv1_prune =SeparableConv2d(x_c+out_c*5,out_c,1)\n        self.bn1_prune = nn.BatchNorm2d(out_c)\n\n        #adopt [1x1, 48] for channel reduction.\n        #self.conv2 = nn.Conv2d(24, 32, 1, bias=False)\n        #self.bn2 = nn.BatchNorm2d(32)\n    \n        self.last_conv_prune1 = nn.Sequential(\n                                       nn.Conv2d(low_level_c+out_c, out_c, 3, 1, 1, bias=False),\n                                       # SeparableConv2d(low_level_c+out_c,out_c,3,1,1),\n                                       nn.BatchNorm2d(out_c),\n                                       nn.ReLU())\n                                       \n        self.last_conv_prune2 = nn.Sequential(\n                                       nn.Conv2d(out_c, out_c, 3, 1, 1, bias=False),\n                                       # SeparableConv2d(out_c,out_c,3,1,1),\n                                       nn.BatchNorm2d(out_c),\n                                       nn.ReLU())\n        \n        self.deep_prune = nn.Conv2d(out_c, 1, kernel_size=1, stride=1)\n        self.class_prune = nn.Conv2d(out_c, n_classes, kernel_size=1, stride=1)\n        \n    def trunk(self, input, input_size):\n        x, low_level_features = self.mobile_features(input)\n        # x: (b,1024,input_h/32,input_h/32)\n        # low_level_features: (b, 128, input_h/4, input_w/4)\n        # import ipdb; ipdb.set_trace()\n        x1 = self.aspp1(x) #(b,96,input_h/32,input_h/32)\n        x2 = self.aspp2(x) #(b,96,input_h/32,input_h/32)\n        x3 = self.aspp3(x) #(b,96,input_h/32,input_h/32)\n        x4 = self.aspp4(x) #(b,96,input_h/32,input_h/32)\n        x5 = self.global_avg_pool_prune(x) #(b,96,1,1)\n        x5 = F.interpolate(x5, size=x4.size()[2:], mode='bilinear', align_corners=True) #(b,96,input_h/32,input_h/32)\n\n        x = torch.cat((x,x1, x2, x3, x4, x5), dim=1) #(b,1760,input_h/32,input_h/32)\n        # x = torch.cat((x,x1, x2, x3), dim=1) #(b,288,input_h/32,input_h/32)\n        #print('after aspp cat',x.size())\n        x = self.conv1_prune(x) #(b,96,input_h/32,input_h/32)\n        x = self.bn1_prune(x)\n        x = self.relu_prune(x)\n        x = F.interpolate(x, size=(int(math.ceil(input_size[-2]/4)),\n                                int(math.ceil(input_size[-1]/4))), mode='bilinear', align_corners=True) # (b, 96, input_h/4, input_w/4)\n       # ablation=torch.max(low_level_features, 1)[1]\n        #print('after con on aspp output',x.size())\n\n        ##comment to remove low feature\n        #low_level_features = self.conv2(low_level_features)\n        #low_level_features = self.bn2(low_level_features)\n        #low_level_features = self.relu(low_level_features)\n        #print(\"low\",low_level_features.size())\n        \n        x = torch.cat((x, low_level_features), dim=1) #(b, 120, input_h/4, input_w/4)\n        #print('after cat low feature with output of aspp',x.size())\n        x = self.last_conv_prune1(x) #(b, 96, input_h/4, input_w/4)\n        return x\n        \n    def head(self, x, input_size):\n    \n        depth = self.last_conv_prune2(x) #(b, 96, input_h/4, input_w/4)\n        depth = self.deep_prune(depth) #(b, 1, input_h/4, input_w/4)\n        depth = F.interpolate(depth, size=(int(math.ceil(input_size[-2]/2)),\n                                int(math.ceil(input_size[-1]/2))), mode='bilinear', align_corners=True) #(b, 1, input_h/2, input_w/2)\n\n        class_mask = self.last_conv_prune2(x) #(b, 96, input_h/4, input_w/4)\n        class_mask = self.class_prune(class_mask) #(b, 1, input_h/4, input_w/4)\n        class_mask = F.interpolate(class_mask, size=(int(math.ceil(input_size[-2]/2)),\n                                int(math.ceil(input_size[-1]/2))), mode='bilinear', align_corners=True) #(b, n_classes, input_h/2, input_w/2)\n                                \n        return depth, class_mask\n        \n    def forward(self, input):\n        input_size = input.size()\n        \n        x = self.trunk(input, input_size)\n        \n        depth, class_mask = self.head(x, input_size)\n        \n        return {'depth': depth, 'class_mask': class_mask}\n\n    def freeze_bn(self):\n        for m in self.modules():\n            if isinstance(m, nn.BatchNorm2d):\n                m.eval()\n\n    def __init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                torch.nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n                \ndef flops(model,input_size):\n    input = torch.rand(1, 3, input_size, input_size).cuda()\n    flops, params = profile(model, inputs=(input, ))\n    from thop import clever_format\n    flops, params = clever_format([flops, params], \"%.3f\")\n    print(flops, params)\n    #import pdb;pdb.set_trace()\n\ndef FPS(model,input_size):\n    import time\n    model.eval()\n\n    for x in range(0,200):\n        input = torch.randn(1, 3, input_size, input_size).cuda()\n        with torch.no_grad():\n            # import ipdb;ipdb.set_trace()\n            output = model.forward(input)\n        \n    total=0\n    for x in range(0,200):\n        input = torch.randn(1, 3, input_size, input_size).cuda()\n        with torch.no_grad():\n            a = time.perf_counter()\n            output = model.forward(input)\n            torch.cuda.synchronize()\n            b = time.perf_counter()\n            total+=b-a\n    print('FPS:', str(200/total))\n    print('ms:', str(1000*total/200))\n    \nif __name__ == '__main__':\n\n    from thop import profile\n    model = RT(n_classes=59, pretrained=False)\n    model.cuda()\n    input_size = 224\n    # flops(model,input_size)\n    FPS(model,input_size)\n", "MobileNetV1.py": "import os\nimport shutil\nimport time\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport torch.utils.data\n\nclass MobileNet(nn.Module):\n    def __init__(self, relu6=True):\n        super(MobileNet, self).__init__()\n\n        def relu(relu6):\n            if relu6:\n                return nn.ReLU6(inplace=True)\n            else:\n                return nn.ReLU(inplace=True)\n\n        def conv_bn(inp, oup, stride, relu6):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                relu(relu6),\n            )\n\n        def conv_dw(inp, oup, stride, relu6):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                relu(relu6),\n    \n                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n                relu(relu6),\n            )\n\n        self.model = nn.Sequential(\n            conv_bn(  3,  32, 2, relu6), \n            conv_dw( 32,  64, 1, relu6),\n            conv_dw( 64, 128, 2, relu6),\n            conv_dw(128, 128, 1, relu6),\n            conv_dw(128, 256, 2, relu6),\n            conv_dw(256, 256, 1, relu6),\n            conv_dw(256, 512, 2, relu6),\n            conv_dw(512, 512, 1, relu6),\n            conv_dw(512, 512, 1, relu6),\n            conv_dw(512, 512, 1, relu6),\n            conv_dw(512, 512, 1, relu6),\n            conv_dw(512, 512, 1, relu6),\n            conv_dw(512, 1024, 2, relu6),\n            conv_dw(1024, 1024, 1, relu6),\n            nn.AvgPool2d(7),\n        )\n        self.fc = nn.Linear(1024, 1000)\n\n    def forward(self, x):\n        for i in range(14):\n            x = self.model[i](x)\n            if i == 3:\n                low_level_features = x\n        return x, low_level_features\n        \nclass MobileNet_res(nn.Module):\n    def __init__(self, relu6=True):\n        super(MobileNet_res, self).__init__()\n\n        def relu(relu6):\n            if relu6:\n                return nn.ReLU6(inplace=True)\n            else:\n                return nn.ReLU(inplace=True)\n\n        def conv_bn(inp, oup, stride, relu6):\n            return nn.Sequential(\n                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(oup),\n                relu(relu6),\n            )\n\n        def conv_dw(inp, oup, stride, relu6):\n            return nn.Sequential(\n                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n                nn.BatchNorm2d(inp),\n                relu(relu6),\n    \n                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n                relu(relu6),\n            )\n\n        self.model = nn.Sequential(\n            conv_bn(  3,  32, 2, relu6), \n            conv_dw( 32,  64, 1, relu6),\n            conv_dw( 64, 128, 2, relu6),\n            conv_dw(128, 128, 1, relu6), #(128,56,56)\n            conv_dw(128, 256, 2, relu6),\n            conv_dw(256, 256, 1, relu6),\n            conv_dw(256, 512, 2, relu6),\n            conv_dw(512, 512, 1, relu6),\n            conv_dw(512, 512, 1, relu6),\n            conv_dw(512, 512, 1, relu6),\n            conv_dw(512, 512, 1, relu6),\n            conv_dw(512, 512, 1, relu6),\n            conv_dw(512, 1024, 2, relu6),\n            conv_dw(1024, 1024, 1, relu6), #(1024,7,7)\n            nn.AvgPool2d(7),\n        )\n        self.fc = nn.Linear(1024, 1000)\n\n    def forward(self, x):\n        # res_ind = [3,5,7,8,9,10,11]\n        res_ind = [3,5,7]\n        for i in range(14):\n            out = self.model[i](x)\n            if i in res_ind:\n                x = x + out\n            else:\n                x = out\n            if i == 3:\n                low_level_features = x\n        return x, low_level_features\n\ndef main():\n    import torchvision.models\n    model = MobileNet(relu6=True)\n    model = torch.nn.DataParallel(model).cuda()\n    model_filename = os.path.join('results', 'imagenet.arch=mobilenet.lr=0.1.bs=256', 'model_best.pth.tar')\n    if os.path.isfile(model_filename):\n        print(\"=> loading Imagenet pretrained model '{}'\".format(model_filename))\n        checkpoint = torch.load(model_filename)\n        epoch = checkpoint['epoch']\n        best_prec1 = checkpoint['best_prec1']\n        model.load_state_dict(checkpoint['state_dict'])\n        print(\"=> loaded Imagenet pretrained model '{}' (epoch {}). best_prec1={}\".format(model_filename, epoch, best_prec1))\n\ndef flops(model,input_size):\n    input = torch.rand(1, 3, input_size, input_size).cuda()\n    flops, params = profile(model, inputs=(input, ))\n    from thop import clever_format\n    flops, params = clever_format([flops, params], \"%.3f\")\n    print(flops, params)\n    #import pdb;pdb.set_trace()\n\ndef FPS(model,input_size):\n    model.eval()\n    import time\n    for x in range(0,200):\n        input = torch.randn(1, 3, input_size, input_size).cuda()\n        with torch.no_grad():\n            # import ipdb;ipdb.set_trace()\n            output = model.forward(input)\n        \n    total=0\n    for x in range(0,200):\n        input = torch.randn(1, 3, input_size, input_size).cuda()\n        with torch.no_grad():\n            a = time.perf_counter()\n            output = model.forward(input)\n            torch.cuda.synchronize()\n            b = time.perf_counter()\n            total+=b-a\n    print('FPS:', str(200/total))\n    print('ms:', str(1000*total/200))\n    \nif __name__ == '__main__':\n\n    from thop import profile\n    # model = MobileNet()\n    model = MobileNet_res()\n    model.cuda()\n    input_size = 224\n    # flops(model,input_size)\n    # model_name = model.name()\n    # print('==== model name: '+model_name+'   decoder: '+decoder+' =========')\n    FPS(model,input_size)", "darknet.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Mon Nov 19 21:47:01 2018\n\n@author: Taha Emara  @email: taha@emaraic.com 'based on lightnet library by EAVISE' \n\"\"\"\n\nfrom collections import OrderedDict\nimport torch.nn as nn\nimport lightnet.network as lnn\n\n\n\nclass Darknet19(lnn.module.Darknet):\n    \"\"\" `Darknet19`_ implementation with pytorch.\n\n    Args:\n        weights_file (str, optional): Path to the saved weights; Default **None**\n        input_channels (Number, optional): Number of input channels; Default **3**\n\n    .. _Darknet19: https://github.com/pjreddie/darknet/blob/master/cfg/darknet19.cfg\n    \"\"\"\n\n    def __init__(self, weights_file=None, input_channels=3):\n        \"\"\" Network initialisation \"\"\"\n        super().__init__()\n\n\n        # Network\n        self.layers = nn.Sequential(\n            OrderedDict([\n                ('1_convbatch',     lnn.layer.Conv2dBatchReLU(input_channels, 32, 3, 1, 1)),\n                ('2_max',           nn.MaxPool2d(2, 2)),\n                ('3_convbatch',     lnn.layer.Conv2dBatchReLU(32, 64, 3, 1, 1)),\n                ('4_max',           nn.MaxPool2d(2, 2)),\n                ('5_convbatch',     lnn.layer.Conv2dBatchReLU(64, 128, 3, 1, 1)),\n                ('6_convbatch',     lnn.layer.Conv2dBatchReLU(128, 64, 1, 1, 0)),\n                ('7_convbatch',     lnn.layer.Conv2dBatchReLU(64, 128, 3, 1, 1)),\n                ('8_max',           nn.MaxPool2d(2,2)),\n                ('9_convbatch',     lnn.layer.Conv2dBatchReLU(128, 256, 3, 1, 1)),#2>1\n                ('10_convbatch',    lnn.layer.Conv2dBatchReLU(256, 128, 1, 1, 0)),\n                ('11_convbatch',    lnn.layer.Conv2dBatchReLU(128, 256, 3, 1, 1)),\n                ('12_max',          nn.MaxPool2d(2,2)),\n                ('13_convbatch',    lnn.layer.Conv2dBatchReLU(256, 512, 3, 1, 1,dilation=1)),\n                ('14_convbatch',    lnn.layer.Conv2dBatchReLU(512, 256, 1,1, 1,dilation=1)),\n                ('15_convbatch',    lnn.layer.Conv2dBatchReLU(256, 512, 3, 1, 1,dilation=1)),\n                ('16_convbatch',    lnn.layer.Conv2dBatchReLU(512, 256, 1, 1, 1,dilation=1)),\n                ('17_convbatch',    lnn.layer.Conv2dBatchReLU(256, 512, 3, 1, 1,dilation=1)),\n                ('18_max',          nn.MaxPool2d(1,1)),\n                ('19_convbatch',    lnn.layer.Conv2dBatchReLU(512, 1024, 3, 1, 1,dilation=2)),\n                ('20_convbatch',    lnn.layer.Conv2dBatchReLU(1024, 512, 1, 1, 1,dilation=2)),\n                ('21_convbatch',    lnn.layer.Conv2dBatchReLU(512, 1024, 3, 1, 1,dilation=2)),\n                ('22_convbatch',    lnn.layer.Conv2dBatchReLU(1024, 512, 1, 1, 0,dilation=2)),\n                ('23_convbatch',    lnn.layer.Conv2dBatchReLU(512, 1024, 3, 1, 1,dilation=2))\n            ])\n        )\n\n        if weights_file is not None:\n            self.load(weights_file)\n\n    def _forward(self, x):\n        i=0\n        for i,l in enumerate( self.layers[:7]):\n            #print(str(i),x.size())\n            x = l(x)\n        keep = x\n        for z,l in enumerate(self.layers[7:]):\n            #print(str(z+i),x.size())\n            x = l(x)\n        return x,keep\n\n\"\"\" os =16\n   ('1_convbatch',     lnn.layer.Conv2dBatchReLU(input_channels, 32, 3, 1, 1)),\n                ('2_max',           nn.MaxPool2d(2, 2)),\n                ('3_convbatch',     lnn.layer.Conv2dBatchReLU(32, 64, 3, 1, 1)),\n                ('4_max',           nn.MaxPool2d(2, 2)),\n                ('5_convbatch',     lnn.layer.Conv2dBatchReLU(64, 128, 3, 1, 1)),\n                ('6_convbatch',     lnn.layer.Conv2dBatchReLU(128, 64, 1, 1, 0)),\n                ('7_convbatch',     lnn.layer.Conv2dBatchReLU(64, 128, 3, 1, 1)),\n                ('8_max',           nn.MaxPool2d(2,2)),\n                ('9_convbatch',     lnn.layer.Conv2dBatchReLU(128, 256, 3, 1, 1)),#2>1\n                ('10_convbatch',    lnn.layer.Conv2dBatchReLU(256, 128, 1, 1, 0)),\n                ('11_convbatch',    lnn.layer.Conv2dBatchReLU(128, 256, 3, 1, 1)),\n                ('12_max',          nn.MaxPool2d(2, 2)),\n                ('13_convbatch',    lnn.layer.Conv2dBatchReLU(256, 512, 3, 1, 1,dilation=1)),\n                ('14_convbatch',    lnn.layer.Conv2dBatchReLU(512, 256, 1,1, 1,dilation=1)),\n                ('15_convbatch',    lnn.layer.Conv2dBatchReLU(256, 512, 3, 1, 1,dilation=1)),\n                ('16_convbatch',    lnn.layer.Conv2dBatchReLU(512, 256, 1, 1, 1,dilation=1)),\n                ('17_convbatch',    lnn.layer.Conv2dBatchReLU(256, 512, 3, 1, 1,dilation=1)),\n                ('18_max',          nn.MaxPool2d(1,1)),\n                ('19_convbatch',    lnn.layer.Conv2dBatchReLU(512, 1024, 3, 1, 1,dilation=2)),\n                ('20_convbatch',    lnn.layer.Conv2dBatchReLU(1024, 512, 1, 1, 1,dilation=2)),\n                ('21_convbatch',    lnn.layer.Conv2dBatchReLU(512, 1024, 3, 1, 1,dilation=2)),\n                ('22_convbatch',    lnn.layer.Conv2dBatchReLU(1024, 512, 1, 1, 0,dilation=2)),\n                ('23_convbatch',    lnn.layer.Conv2dBatchReLU(512, 1024, 3, 1, 1,dilation=2))\n                \n\"\"\"\n\"\"\"os =32\n ('1_convbatch',     lnn.layer.Conv2dBatchReLU(input_channels, 32, 3, 1, 1)),\n                ('2_max',           nn.MaxPool2d(2, 2)),\n                ('3_convbatch',     lnn.layer.Conv2dBatchReLU(32, 64, 3, 1, 1)),\n                ('4_max',           nn.MaxPool2d(2, 2)),\n                ('5_convbatch',     lnn.layer.Conv2dBatchReLU(64, 128, 3, 1, 1)),\n                ('6_convbatch',     lnn.layer.Conv2dBatchReLU(128, 64, 1, 1, 0)),\n                ('7_convbatch',     lnn.layer.Conv2dBatchReLU(64, 128, 3, 1, 1)),\n                ('8_max',           nn.MaxPool2d(2,2)),\n                ('9_convbatch',     lnn.layer.Conv2dBatchReLU(128, 256, 3, 1, 1)),#2>1\n                ('10_convbatch',    lnn.layer.Conv2dBatchReLU(256, 128, 1, 1, 0)),\n                ('11_convbatch',    lnn.layer.Conv2dBatchReLU(128, 256, 3, 1, 1)),\n                ('12_max',          nn.MaxPool2d(2, 2)),\n                ('13_convbatch',    lnn.layer.Conv2dBatchReLU(256, 512, 3, 1, 1,dilation=2)),\n                ('14_convbatch',    lnn.layer.Conv2dBatchReLU(512, 256, 1,1, 0,dilation=2)),\n                ('15_convbatch',    lnn.layer.Conv2dBatchReLU(256, 512, 3, 1, 1,dilation=2)),\n                ('16_convbatch',    lnn.layer.Conv2dBatchReLU(512, 256, 1, 1, 0,dilation=2)),\n                ('17_convbatch',    lnn.layer.Conv2dBatchReLU(256, 512, 3, 1, 1,dilation=2)),\n                ('18_max',          nn.MaxPool2d(1,1)),\n                ('19_convbatch',    lnn.layer.Conv2dBatchReLU(512, 1024, 3, 1, 1,dilation=4)),\n                ('20_convbatch',    lnn.layer.Conv2dBatchReLU(1024, 512, 1, 1, 0,dilation=4)),\n                ('21_convbatch',    lnn.layer.Conv2dBatchReLU(512, 1024, 3, 1, 1,dilation=4)),\n                ('22_convbatch',    lnn.layer.Conv2dBatchReLU(1024, 512, 1, 1, 0,dilation=4)),\n                ('23_convbatch',    lnn.layer.Conv2dBatchReLU(512, 1024, 3, 1, 1,dilation=4))\n\"\"\"\n", "ShuffleNet.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Mon Nov 19 21:47:01 2018\n\n@author: https://github.com/ericsun99/ShuffleNet-1g8-Pytorch, edited by:Taha Emara  @email: taha@emaraic.com\n\"\"\"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom collections import OrderedDict\nfrom torch.nn import init\n\n\ndef conv3x3(in_channels, out_channels, stride=1, \n            padding=1, bias=True, groups=1,dilation=1):    \n    \"\"\"3x3 convolution with padding\n    \"\"\"\n    return nn.Conv2d(\n        in_channels, \n        out_channels, \n        kernel_size=3, \n        stride=stride,\n        padding=padding,\n        bias=bias,dilation=dilation,\n        groups=groups)\n\n\ndef conv1x1(in_channels, out_channels, groups=1):\n    \"\"\"1x1 convolution with padding\n    - Normal pointwise convolution When groups == 1\n    - Grouped pointwise convolution when groups > 1\n    \"\"\"\n    return nn.Conv2d(\n        in_channels, \n        out_channels, \n        kernel_size=1, \n        groups=groups,\n        stride=1)\n\n\ndef channel_shuffle(x, groups):\n    batchsize, num_channels, height, width = x.data.size()\n\n    channels_per_group = num_channels // groups\n    \n    # reshape\n    x = x.view(batchsize, groups, \n        channels_per_group, height, width)\n\n    # transpose\n    # - contiguous() required if transpose() is used before view().\n    #   See https://github.com/pytorch/pytorch/issues/764\n    x = torch.transpose(x, 1, 2).contiguous()\n\n    # flatten\n    x = x.view(batchsize, -1, height, width)\n\n    return x\n\n\nclass ShuffleUnit(nn.Module):\n    def __init__(self, in_channels, out_channels, groups=3,\n                 grouped_conv=True, combine='add',dilation=1):\n        \n        super(ShuffleUnit, self).__init__()\n\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.grouped_conv = grouped_conv\n        self.combine = combine\n        self.groups = groups\n        self.dilation=dilation\n        self.bottleneck_channels = self.out_channels // 4\n\n        # define the type of ShuffleUnit\n        if self.combine == 'add':\n            # ShuffleUnit Figure 2b\n            self.depthwise_stride = 1\n            self._combine_func = self._add\n        elif self.combine == 'concat':\n            # ShuffleUnit Figure 2c\n            self.depthwise_stride = 2\n            self._combine_func = self._concat\n            \n            # ensure output of concat has the same channels as \n            # original output channels.\n            self.out_channels -= self.in_channels\n        else:\n            raise ValueError(\"Cannot combine tensors with \\\"{}\\\"\" \\\n                             \"Only \\\"add\\\" and \\\"concat\\\" are\" \\\n                             \"supported\".format(self.combine))\n\n        # Use a 1x1 grouped or non-grouped convolution to reduce input channels\n        # to bottleneck channels, as in a ResNet bottleneck module.\n        # NOTE: Do not use group convolution for the first conv1x1 in Stage 2.\n        self.first_1x1_groups = self.groups if grouped_conv else 1\n\n        self.g_conv_1x1_compress = self._make_grouped_conv1x1(\n            self.in_channels,\n            self.bottleneck_channels,\n            self.first_1x1_groups,\n            batch_norm=True,\n            relu=True\n            )\n\n        # 3x3 depthwise convolution followed by batch normalization\n        self.depthwise_conv3x3 = conv3x3(\n            self.bottleneck_channels, self.bottleneck_channels,\n            stride=self.depthwise_stride, groups=self.bottleneck_channels,dilation=self.dilation)\n        self.bn_after_depthwise = nn.BatchNorm2d(self.bottleneck_channels)\n\n        # Use 1x1 grouped convolution to expand from \n        # bottleneck_channels to out_channels\n        self.g_conv_1x1_expand = self._make_grouped_conv1x1(\n            self.bottleneck_channels,\n            self.out_channels,\n            self.groups,\n            batch_norm=True,\n            relu=False\n            )\n\n\n    @staticmethod\n    def _add(x, out):\n        # residual connection\n        return x + out\n\n\n    @staticmethod\n    def _concat(x, out):\n        # concatenate along channel axis\n        return torch.cat((x, out), 1)\n\n\n    def _make_grouped_conv1x1(self, in_channels, out_channels, groups,\n        batch_norm=True, relu=False):\n\n        modules = OrderedDict()\n\n        conv = conv1x1(in_channels, out_channels, groups=groups)\n        modules['conv1x1'] = conv\n\n        if batch_norm:\n            modules['batch_norm'] = nn.BatchNorm2d(out_channels)\n        if relu:\n            modules['relu'] = nn.ReLU()\n        if len(modules) > 1:\n            return nn.Sequential(modules)\n        else:\n            return conv\n\n\n    def forward(self, x):\n        # save for combining later with output\n        residual = x\n\n        if self.combine == 'concat':\n            residual = F.avg_pool2d(residual, kernel_size=3, \n                stride=2, padding=1)\n\n        out = self.g_conv_1x1_compress(x)\n        out = channel_shuffle(out, self.groups)\n        out = self.depthwise_conv3x3(out)\n        out = self.bn_after_depthwise(out)\n        out = self.g_conv_1x1_expand(out)\n        \n        out = self._combine_func(residual, out)\n        return F.relu(out)\n\n\nclass ShuffleNet(nn.Module):\n    \"\"\"ShuffleNet implementation.\n    \"\"\"\n\n    def __init__(self, groups=8, in_channels=3, num_classes=1000):\n        \"\"\"ShuffleNet constructor.\n\n        Arguments:\n            groups (int, optional): number of groups to be used in grouped \n                1x1 convolutions in each ShuffleUnit. Default is 3 for best\n                performance according to original paper.\n            in_channels (int, optional): number of channels in the input tensor.\n                Default is 3 for RGB image inputs.\n            num_classes (int, optional): number of classes to predict. Default\n                is 1000 for ImageNet.\n\n        \"\"\"\n        super(ShuffleNet, self).__init__()\n\n        self.groups = groups\n        self.stage_repeats = [3, 7, 3]\n        self.in_channels =  in_channels\n        self.num_classes = num_classes\n\n        # index 0 is invalid and should never be called.\n        # only used for indexing convenience.\n        if groups == 1:\n            self.stage_out_channels = [-1, 24, 144, 288, 567]\n        elif groups == 2:\n            self.stage_out_channels = [-1, 24, 200, 400, 800]\n        elif groups == 3:\n            self.stage_out_channels = [-1, 24, 240, 480, 960]\n        elif groups == 4:\n            self.stage_out_channels = [-1, 24, 272, 544, 1088]\n        elif groups == 8:\n            self.stage_out_channels = [-1, 24, 384, 768, 1536]\n        else:\n            raise ValueError(\n                \"\"\"{} groups is not supported for\n                   1x1 Grouped Convolutions\"\"\".format(num_groups))\n        \n        # Stage 1 always has 24 output channels\n        self.conv1 = conv3x3(self.in_channels,\n                             self.stage_out_channels[1], # stage 1\n                             stride=2)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        # Stage 2\n        self.stage2 = self._make_stage(2)\n        # Stage 3\n        self.stage3 = self._make_stage(3)\n        # Stage 4\n        self.stage4 = self._make_stage(4,dilation=2)\n        \n        num_inputs = self.stage_out_channels[-1]\n        self.fc = nn.Linear(num_inputs, self.num_classes)\n\n    \n\n    def _make_stage(self, stage,dilation=1):\n        modules = OrderedDict()\n        stage_name = \"ShuffleUnit_Stage{}\".format(stage)\n        \n        # First ShuffleUnit in the stage\n        # 1. non-grouped 1x1 convolution (i.e. pointwise convolution)\n        #   is used in Stage 2. Group convolutions used everywhere else.\n        grouped_conv = stage > 2\n        \n        # 2. concatenation unit is always used.\n        first_module = ShuffleUnit(\n            self.stage_out_channels[stage-1],\n            self.stage_out_channels[stage],\n            groups=self.groups,\n            grouped_conv=grouped_conv,\n            combine='concat'#,dilation=dilation\n            )\n        modules[stage_name+\"_0\"] = first_module\n\n        # add more ShuffleUnits depending on pre-defined number of repeats\n        for i in range(self.stage_repeats[stage-2]):\n            name = stage_name + \"_{}\".format(i+1)\n            module = ShuffleUnit(\n                self.stage_out_channels[stage],\n                self.stage_out_channels[stage],\n                groups=self.groups,\n                grouped_conv=True,\n                combine='add'#,dilation=dilation\n                )\n            modules[name] = module\n\n        return nn.Sequential(modules)\n\n\n    def forward(self, x):\n        #print('input to network',x.size())\n        x = self.conv1(x)\n        x = self.maxpool(x)\n        #print('Shuffle after conv1',x.size())\n        \n        x = self.stage2(x)\n        #print('after stage 2',x.size())\n\n        low_features=x\n        x = self.stage3(x)\n        #print('after stage 3',x.size())\n\n        x = self.stage4(x)\n        #print('after stage 4',x.size())\n\n\n#       # global average pooling layer\n#        x = F.avg_pool2d(x, x.data.size()[-2:])\n#        \n#        # flatten for input to fully-connected layer\n#        x = x.view(x.size(0), -1)\n#        x = self.fc(x)\n#\n#        return F.log_softmax(x, dim=1)\n        \n        return x,low_features\n\n", "MobileNetV2.py": "\n\"\"\"\nCreated on Mon Nov 19 21:47:01 2018\n\n@author: https://github.com/ericsun99/MobileNet-V2-Pytorch, edited by:Taha Emara  @email: taha@emaraic.com\n\"\"\"\n\nimport torch.nn as nn\nimport math\n\ndef conv_bn(inp, oup, stride):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU6(inplace=True)\n    )\n\n\ndef conv_1x1_bn(inp, oup):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU6(inplace=True)\n    )\n\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, expand_ratio):\n        super(InvertedResidual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n\n        hidden_dim = round(inp * expand_ratio)\n        self.use_res_connect = self.stride == 1 and inp == oup\n\n        if expand_ratio == 1:\n            self.conv = nn.Sequential(\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n        else:\n            self.conv = nn.Sequential(\n                # pw\n                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n\n    def forward(self, x):\n        if self.use_res_connect:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\n\nclass MobileNetV2(nn.Module):\n    def __init__(self, n_class=1000, input_size=224, width_mult=1.):\n        super(MobileNetV2, self).__init__()\n        block = InvertedResidual\n        input_channel = 32\n        last_channel = 1280\n        interverted_residual_setting = [\n            # t, c, n, s\n            [1, 16, 1, 1],\n            [6, 24, 2, 2],\n            [6, 32, 3, 2],\n            [6, 64, 4, 2],\n            [6, 96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1],\n        ]\n\n        # building first layer\n        assert input_size % 32 == 0\n        input_channel = int(input_channel * width_mult)\n        self.last_channel = int(last_channel * width_mult) if width_mult > 1.0 else last_channel\n        self.features = [conv_bn(3, input_channel, 2)]\n        # building inverted residual blocks\n        for t, c, n, s in interverted_residual_setting:\n            output_channel = int(c * width_mult)\n            for i in range(n):\n                if i == 0:\n                    self.features.append(block(input_channel, output_channel, s, expand_ratio=t))\n                else:\n                    self.features.append(block(input_channel, output_channel, 1, expand_ratio=t))\n                input_channel = output_channel\n        # building last several layers\n        self.features.append(conv_1x1_bn(input_channel, self.last_channel))\n        # make it nn.Sequential\n        self.features = nn.Sequential(*self.features)\n\n        # building classifier\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Linear(self.last_channel, n_class),\n        )\n\n        self._initialize_weights()\n\n    def forward(self, x):\n        i=0\n        for i,l in enumerate( self.features[:4]):\n            #print(str(i),x.size())\n            x = l(x)\n        keep = x\n        for z,l in enumerate(self.features[4:]):\n            #print(str(z+i+1),x.size())\n            x = l(x)\n        return x,keep\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                n = m.weight.size(1)\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n"}